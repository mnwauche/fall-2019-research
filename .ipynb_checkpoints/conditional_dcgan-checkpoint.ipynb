{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Conditional DCGAN for MNIST images generations.\n",
    "    Author: Moustafa Alzantot (malzantot@ucla.edu)\n",
    "    All rights reserved.\n",
    "\"\"\"\n",
    "#ref: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "LATENT_DIM = 100\n",
    "GEN_FILTERS = 64\n",
    "DISCRIM_FILTERS = 64\n",
    "CHANNELS = 3\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(LATENT_DIM, GEN_FILTERS * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(GEN_FILTERS * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(GEN_FILTERS * 8, GEN_FILTERS * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(GEN_FILTERS * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(GEN_FILTERS * 4, GEN_FILTERS * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(GEN_FILTERS * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(GEN_FILTERS * 2, GEN_FILTERS, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(GEN_FILTERS),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(GEN_FILTERS, CHANNELS, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "            # state size. (nc) x 32 x 32\n",
    "            \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 32 x 32\n",
    "            nn.Conv2d(in_channels=CHANNELS, out_channels=DISCRIM_FILTERS, \n",
    "                      kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(DISCRIM_FILTERS, DISCRIM_FILTERS * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(DISCRIM_FILTERS * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(DISCRIM_FILTERS * 2, DISCRIM_FILTERS * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(DISCRIM_FILTERS * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(DISCRIM_FILTERS * 4, DISCRIM_FILTERS * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(DISCRIM_FILTERS * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(DISCRIM_FILTERS * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# train_dataset = datasets.CIFAR10(root='../_datasets/cifar-10',\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=transforms.ToTensor())\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='../_datasets/cifar-10', train=True, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(IMAGE_SIZE),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser('Conditional DCGAN')\n",
    "BATCH_SIZE = 128\n",
    "LR = .01\n",
    "EPOCHS = 10                   \n",
    "NOISE = 100 # number of dimensions for input noise\n",
    "CUDA = False\n",
    "SAVE_EVERY = 5 # after how many epochs to save the model\n",
    "PRINT_EVERY = 1 # After how many epochs to print loss and save output samples.\n",
    "SAVE_DIR = 'models' # Path to save the trained models.\n",
    "SAMPLES_DIR = 'samples' # Path to save the output samples\n",
    "\n",
    "NGPU = 0\n",
    "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "NET_G = ''\n",
    "NET_D = ''\n",
    "BETA1 = .5\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "netG = Generator(NGPU).to(device)\n",
    "netG.apply(weights_init)\n",
    "if NET_G != '':\n",
    "    netG.load_state_dict(torch.load(NET_G))\n",
    "print(netG)\n",
    "\n",
    "netD = Discriminator(NGPU).to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "if NET_D != '':\n",
    "    netD.load_state_dict(torch.load(NET_D))\n",
    "print(netD)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(size=(BATCH_SIZE, LATENT_DIM, 1, 1), device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=LR, betas=(BETA1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][0/391] Loss_D: 1.9414 Loss_G: 62.7425 D(x): 0.6502 D(G(z)): 0.7014 / 0.0000\n",
      "[0/10][1/391] Loss_D: 37.7405 Loss_G: 41.9640 D(x): 0.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/10][2/391] Loss_D: 21.4176 Loss_G: 32.7500 D(x): 0.3877 D(G(z)): 0.5682 / 0.0000\n",
      "[0/10][3/391] Loss_D: 9.6515 Loss_G: 9.6678 D(x): 0.5968 D(G(z)): 0.7731 / 0.1239\n",
      "[0/10][4/391] Loss_D: 10.5163 Loss_G: 1.9072 D(x): 0.5044 D(G(z)): 0.2898 / 0.3753\n",
      "[0/10][5/391] Loss_D: 7.2621 Loss_G: 0.1580 D(x): 0.7685 D(G(z)): 0.4327 / 0.9234\n",
      "[0/10][6/391] Loss_D: 9.7982 Loss_G: 2.1033 D(x): 0.9907 D(G(z)): 0.9814 / 0.3039\n",
      "[0/10][7/391] Loss_D: 5.0940 Loss_G: 7.9459 D(x): 0.6752 D(G(z)): 0.3495 / 0.4209\n",
      "[0/10][8/391] Loss_D: 6.1636 Loss_G: 4.6553 D(x): 0.5059 D(G(z)): 0.2504 / 0.1916\n",
      "[0/10][9/391] Loss_D: 7.9230 Loss_G: 27.5097 D(x): 0.8725 D(G(z)): 0.8866 / 0.0135\n",
      "[0/10][10/391] Loss_D: 9.0681 Loss_G: 19.2424 D(x): 0.2919 D(G(z)): 0.0918 / 0.0804\n",
      "[0/10][11/391] Loss_D: 6.4379 Loss_G: 12.4586 D(x): 0.5643 D(G(z)): 0.2548 / 0.0557\n",
      "[0/10][12/391] Loss_D: 9.5871 Loss_G: 11.3715 D(x): 0.5814 D(G(z)): 0.6366 / 0.0432\n",
      "[0/10][13/391] Loss_D: 7.9349 Loss_G: 11.3106 D(x): 0.3119 D(G(z)): 0.3086 / 0.0162\n",
      "[0/10][14/391] Loss_D: 4.2138 Loss_G: 5.9554 D(x): 0.5926 D(G(z)): 0.4159 / 0.1951\n",
      "[0/10][15/391] Loss_D: 6.6576 Loss_G: 10.2756 D(x): 0.7720 D(G(z)): 0.5946 / 0.0914\n",
      "[0/10][16/391] Loss_D: 4.0893 Loss_G: 16.1438 D(x): 0.6595 D(G(z)): 0.5092 / 0.0174\n",
      "[0/10][17/391] Loss_D: 9.0392 Loss_G: 2.8305 D(x): 0.2736 D(G(z)): 0.1459 / 0.3648\n",
      "[0/10][18/391] Loss_D: 9.3834 Loss_G: 11.4974 D(x): 0.5391 D(G(z)): 0.9180 / 0.0682\n",
      "[0/10][19/391] Loss_D: 9.9888 Loss_G: 7.7936 D(x): 0.2156 D(G(z)): 0.3020 / 0.0909\n",
      "[0/10][20/391] Loss_D: 5.4320 Loss_G: 1.5838 D(x): 0.3147 D(G(z)): 0.2826 / 0.4130\n",
      "[0/10][21/391] Loss_D: 2.5824 Loss_G: 6.2099 D(x): 0.7538 D(G(z)): 0.4125 / 0.3957\n",
      "[0/10][22/391] Loss_D: 5.3452 Loss_G: 3.4995 D(x): 0.6928 D(G(z)): 0.5666 / 0.2984\n",
      "[0/10][23/391] Loss_D: 3.4408 Loss_G: 4.6017 D(x): 0.5627 D(G(z)): 0.2299 / 0.2095\n",
      "[0/10][24/391] Loss_D: 2.4911 Loss_G: 1.7702 D(x): 0.6311 D(G(z)): 0.2555 / 0.4304\n",
      "[0/10][25/391] Loss_D: 3.2947 Loss_G: 6.7951 D(x): 0.6820 D(G(z)): 0.4701 / 0.3787\n",
      "[0/10][26/391] Loss_D: 5.4117 Loss_G: 4.0478 D(x): 0.5736 D(G(z)): 0.4185 / 0.1970\n",
      "[0/10][27/391] Loss_D: 2.2050 Loss_G: 3.5388 D(x): 0.5561 D(G(z)): 0.2220 / 0.2278\n",
      "[0/10][28/391] Loss_D: 2.2331 Loss_G: 2.3778 D(x): 0.5985 D(G(z)): 0.2746 / 0.3884\n",
      "[0/10][29/391] Loss_D: 5.1560 Loss_G: 3.5662 D(x): 0.7489 D(G(z)): 0.4761 / 0.2698\n",
      "[0/10][30/391] Loss_D: 1.4922 Loss_G: 3.4781 D(x): 0.7252 D(G(z)): 0.2695 / 0.1410\n",
      "[0/10][31/391] Loss_D: 1.1450 Loss_G: 3.6128 D(x): 0.7679 D(G(z)): 0.2124 / 0.0942\n",
      "[0/10][32/391] Loss_D: 0.8783 Loss_G: 2.5109 D(x): 0.7245 D(G(z)): 0.1551 / 0.3629\n",
      "[0/10][33/391] Loss_D: 2.5813 Loss_G: 6.3737 D(x): 0.7842 D(G(z)): 0.3869 / 0.4442\n",
      "[0/10][34/391] Loss_D: 5.6048 Loss_G: 3.4186 D(x): 0.5721 D(G(z)): 0.4347 / 0.2688\n",
      "[0/10][35/391] Loss_D: 2.4399 Loss_G: 4.8592 D(x): 0.5287 D(G(z)): 0.2757 / 0.0532\n",
      "[0/10][36/391] Loss_D: 1.1188 Loss_G: 2.2156 D(x): 0.6167 D(G(z)): 0.0932 / 0.2720\n",
      "[0/10][37/391] Loss_D: 1.0061 Loss_G: 6.7544 D(x): 0.8985 D(G(z)): 0.2721 / 0.2800\n",
      "[0/10][38/391] Loss_D: 2.9859 Loss_G: 6.2694 D(x): 0.6738 D(G(z)): 0.3150 / 0.0850\n",
      "[0/10][39/391] Loss_D: 1.2083 Loss_G: 4.3410 D(x): 0.8257 D(G(z)): 0.1496 / 0.1401\n",
      "[0/10][40/391] Loss_D: 0.8929 Loss_G: 3.7981 D(x): 0.7795 D(G(z)): 0.1796 / 0.0459\n",
      "[0/10][41/391] Loss_D: 0.2874 Loss_G: 3.6849 D(x): 0.8832 D(G(z)): 0.0695 / 0.0865\n",
      "[0/10][42/391] Loss_D: 0.2318 Loss_G: 4.5554 D(x): 0.9375 D(G(z)): 0.1115 / 0.0331\n",
      "[0/10][43/391] Loss_D: 0.1159 Loss_G: 5.2312 D(x): 0.9498 D(G(z)): 0.0483 / 0.0143\n",
      "[0/10][44/391] Loss_D: 0.1480 Loss_G: 5.6795 D(x): 0.9632 D(G(z)): 0.0937 / 0.0073\n",
      "[0/10][45/391] Loss_D: 0.1266 Loss_G: 6.6338 D(x): 0.9409 D(G(z)): 0.0545 / 0.0031\n",
      "[0/10][46/391] Loss_D: 0.9271 Loss_G: 8.4759 D(x): 0.9117 D(G(z)): 0.4587 / 0.0034\n",
      "[0/10][47/391] Loss_D: 6.3499 Loss_G: 0.7207 D(x): 0.0621 D(G(z)): 0.0094 / 0.6405\n",
      "[0/10][48/391] Loss_D: 6.7548 Loss_G: 0.1210 D(x): 0.8417 D(G(z)): 0.5715 / 0.8907\n",
      "[0/10][49/391] Loss_D: 5.6738 Loss_G: 2.6272 D(x): 0.9909 D(G(z)): 0.9029 / 0.3920\n",
      "[0/10][50/391] Loss_D: 2.3958 Loss_G: 4.3417 D(x): 0.7885 D(G(z)): 0.4673 / 0.0649\n",
      "[0/10][51/391] Loss_D: 1.5797 Loss_G: 2.5442 D(x): 0.4490 D(G(z)): 0.1182 / 0.1844\n",
      "[0/10][52/391] Loss_D: 0.7536 Loss_G: 3.1720 D(x): 0.8639 D(G(z)): 0.2272 / 0.1948\n",
      "[0/10][53/391] Loss_D: 0.6300 Loss_G: 2.4817 D(x): 0.8609 D(G(z)): 0.2483 / 0.2330\n",
      "[0/10][54/391] Loss_D: 1.2288 Loss_G: 7.2587 D(x): 0.8995 D(G(z)): 0.3073 / 0.1890\n",
      "[0/10][55/391] Loss_D: 2.1935 Loss_G: 4.3407 D(x): 0.6348 D(G(z)): 0.2278 / 0.0417\n",
      "[0/10][56/391] Loss_D: 0.3788 Loss_G: 4.4411 D(x): 0.8934 D(G(z)): 0.1140 / 0.0285\n",
      "[0/10][57/391] Loss_D: 0.1971 Loss_G: 6.9533 D(x): 0.9562 D(G(z)): 0.0933 / 0.0243\n",
      "[0/10][58/391] Loss_D: 0.1088 Loss_G: 6.8704 D(x): 0.9536 D(G(z)): 0.0365 / 0.0298\n",
      "[0/10][59/391] Loss_D: 0.0783 Loss_G: 6.5870 D(x): 0.9737 D(G(z)): 0.0433 / 0.0205\n",
      "[0/10][60/391] Loss_D: 0.1225 Loss_G: 6.1497 D(x): 0.9602 D(G(z)): 0.0320 / 0.0115\n",
      "[0/10][61/391] Loss_D: 0.0562 Loss_G: 5.9987 D(x): 0.9785 D(G(z)): 0.0299 / 0.0098\n",
      "[0/10][62/391] Loss_D: 0.1082 Loss_G: 6.4204 D(x): 0.9887 D(G(z)): 0.0647 / 0.0064\n",
      "[0/10][63/391] Loss_D: 0.0612 Loss_G: 6.5957 D(x): 0.9898 D(G(z)): 0.0486 / 0.0072\n",
      "[0/10][64/391] Loss_D: 0.3205 Loss_G: 7.0715 D(x): 0.9804 D(G(z)): 0.2340 / 0.0044\n",
      "[0/10][65/391] Loss_D: 0.2513 Loss_G: 5.6835 D(x): 0.8498 D(G(z)): 0.0092 / 0.0155\n",
      "[0/10][66/391] Loss_D: 0.1194 Loss_G: 5.9128 D(x): 0.9613 D(G(z)): 0.0562 / 0.0410\n",
      "[0/10][67/391] Loss_D: 1.5479 Loss_G: 12.8135 D(x): 0.9651 D(G(z)): 0.6405 / 0.0015\n",
      "[0/10][68/391] Loss_D: 4.4206 Loss_G: 3.3591 D(x): 0.1139 D(G(z)): 0.0046 / 0.2970\n",
      "[0/10][69/391] Loss_D: 4.0978 Loss_G: 6.4428 D(x): 0.8791 D(G(z)): 0.5998 / 0.1105\n",
      "[0/10][70/391] Loss_D: 1.2567 Loss_G: 5.2118 D(x): 0.7418 D(G(z)): 0.2839 / 0.0739\n",
      "[0/10][71/391] Loss_D: 1.7443 Loss_G: 5.4642 D(x): 0.6425 D(G(z)): 0.2058 / 0.1510\n",
      "[0/10][72/391] Loss_D: 1.5287 Loss_G: 2.6588 D(x): 0.6578 D(G(z)): 0.2951 / 0.1834\n",
      "[0/10][73/391] Loss_D: 1.4717 Loss_G: 10.2980 D(x): 0.8281 D(G(z)): 0.5157 / 0.0086\n",
      "[0/10][74/391] Loss_D: 2.5392 Loss_G: 5.5977 D(x): 0.5364 D(G(z)): 0.2359 / 0.0347\n",
      "[0/10][75/391] Loss_D: 6.0912 Loss_G: 18.7868 D(x): 0.6078 D(G(z)): 0.8639 / 0.0120\n",
      "[0/10][76/391] Loss_D: 6.6029 Loss_G: 4.6008 D(x): 0.2116 D(G(z)): 0.0783 / 0.2154\n",
      "[0/10][77/391] Loss_D: 2.6951 Loss_G: 3.9258 D(x): 0.8242 D(G(z)): 0.4385 / 0.1766\n",
      "[0/10][78/391] Loss_D: 0.4478 Loss_G: 8.3531 D(x): 0.9643 D(G(z)): 0.2265 / 0.0270\n",
      "[0/10][79/391] Loss_D: 0.2817 Loss_G: 7.3437 D(x): 0.8877 D(G(z)): 0.0481 / 0.0242\n",
      "[0/10][80/391] Loss_D: 0.2017 Loss_G: 5.7426 D(x): 0.9533 D(G(z)): 0.0861 / 0.0265\n",
      "[0/10][81/391] Loss_D: 0.0671 Loss_G: 5.8569 D(x): 0.9874 D(G(z)): 0.0458 / 0.0202\n",
      "[0/10][82/391] Loss_D: 0.5525 Loss_G: 7.9156 D(x): 0.9836 D(G(z)): 0.2637 / 0.0109\n",
      "[0/10][83/391] Loss_D: 0.7870 Loss_G: 6.2601 D(x): 0.7490 D(G(z)): 0.1768 / 0.0283\n",
      "[0/10][84/391] Loss_D: 3.1625 Loss_G: 18.0607 D(x): 0.9825 D(G(z)): 0.7698 / 0.0000\n",
      "[0/10][85/391] Loss_D: 10.0199 Loss_G: 7.1911 D(x): 0.0031 D(G(z)): 0.0000 / 0.0390\n",
      "[0/10][86/391] Loss_D: 1.8353 Loss_G: 3.0579 D(x): 0.6306 D(G(z)): 0.1887 / 0.3194\n",
      "[0/10][87/391] Loss_D: 1.4131 Loss_G: 5.1728 D(x): 0.9036 D(G(z)): 0.4902 / 0.0680\n",
      "[0/10][88/391] Loss_D: 2.7378 Loss_G: 11.7754 D(x): 0.8174 D(G(z)): 0.7772 / 0.0006\n",
      "[0/10][89/391] Loss_D: 4.0971 Loss_G: 7.0056 D(x): 0.2381 D(G(z)): 0.0040 / 0.0241\n",
      "[0/10][90/391] Loss_D: 1.8880 Loss_G: 1.6352 D(x): 0.4058 D(G(z)): 0.1354 / 0.4901\n",
      "[0/10][91/391] Loss_D: 3.1612 Loss_G: 2.7460 D(x): 0.7013 D(G(z)): 0.5853 / 0.3109\n",
      "[0/10][92/391] Loss_D: 1.3425 Loss_G: 7.1682 D(x): 0.7420 D(G(z)): 0.3289 / 0.0129\n",
      "[0/10][93/391] Loss_D: 1.2690 Loss_G: 4.0269 D(x): 0.5565 D(G(z)): 0.1473 / 0.2011\n",
      "[0/10][94/391] Loss_D: 2.4325 Loss_G: 7.2789 D(x): 0.6461 D(G(z)): 0.4830 / 0.2836\n",
      "[0/10][95/391] Loss_D: 4.3439 Loss_G: 5.7942 D(x): 0.5282 D(G(z)): 0.3155 / 0.0214\n",
      "[0/10][96/391] Loss_D: 0.9097 Loss_G: 4.6659 D(x): 0.6711 D(G(z)): 0.1862 / 0.2442\n",
      "[0/10][97/391] Loss_D: 1.7055 Loss_G: 6.9757 D(x): 0.6953 D(G(z)): 0.2062 / 0.0118\n",
      "[0/10][98/391] Loss_D: 0.1544 Loss_G: 5.9794 D(x): 0.9423 D(G(z)): 0.0695 / 0.0728\n",
      "[0/10][99/391] Loss_D: 0.4675 Loss_G: 7.1602 D(x): 0.9298 D(G(z)): 0.2303 / 0.0135\n",
      "[0/10][100/391] Loss_D: 0.2469 Loss_G: 6.9222 D(x): 0.9313 D(G(z)): 0.1144 / 0.0156\n",
      "[0/10][101/391] Loss_D: 0.2017 Loss_G: 6.0140 D(x): 0.9436 D(G(z)): 0.1160 / 0.0246\n",
      "[0/10][102/391] Loss_D: 0.4363 Loss_G: 7.4646 D(x): 0.9706 D(G(z)): 0.2695 / 0.0037\n",
      "[0/10][103/391] Loss_D: 0.6488 Loss_G: 3.7659 D(x): 0.6653 D(G(z)): 0.0615 / 0.0428\n",
      "[0/10][104/391] Loss_D: 0.8116 Loss_G: 10.2874 D(x): 0.9932 D(G(z)): 0.4944 / 0.0003\n",
      "[0/10][105/391] Loss_D: 0.1372 Loss_G: 8.1945 D(x): 0.9241 D(G(z)): 0.0095 / 0.0083\n",
      "[0/10][106/391] Loss_D: 0.3217 Loss_G: 10.6663 D(x): 0.9711 D(G(z)): 0.1870 / 0.0006\n",
      "[0/10][107/391] Loss_D: 0.1646 Loss_G: 9.6163 D(x): 0.8761 D(G(z)): 0.0055 / 0.0053\n",
      "[0/10][108/391] Loss_D: 0.0634 Loss_G: 8.5505 D(x): 0.9582 D(G(z)): 0.0133 / 0.0127\n",
      "[0/10][109/391] Loss_D: 0.0419 Loss_G: 7.3902 D(x): 0.9948 D(G(z)): 0.0354 / 0.0234\n",
      "[0/10][110/391] Loss_D: 0.0581 Loss_G: 6.8168 D(x): 0.9971 D(G(z)): 0.0528 / 0.0259\n",
      "[0/10][111/391] Loss_D: 0.1253 Loss_G: 6.0097 D(x): 0.9992 D(G(z)): 0.1148 / 0.0330\n",
      "[0/10][112/391] Loss_D: 0.1770 Loss_G: 5.8352 D(x): 0.9980 D(G(z)): 0.1572 / 0.0211\n",
      "[0/10][113/391] Loss_D: 0.0409 Loss_G: 6.2650 D(x): 0.9985 D(G(z)): 0.0384 / 0.0106\n",
      "[0/10][114/391] Loss_D: 0.0431 Loss_G: 5.7081 D(x): 0.9953 D(G(z)): 0.0374 / 0.0133\n",
      "[0/10][115/391] Loss_D: 0.0521 Loss_G: 5.3325 D(x): 0.9977 D(G(z)): 0.0482 / 0.0189\n",
      "[0/10][116/391] Loss_D: 0.0907 Loss_G: 5.8795 D(x): 0.9967 D(G(z)): 0.0797 / 0.0067\n",
      "[0/10][117/391] Loss_D: 0.0303 Loss_G: 5.7869 D(x): 0.9949 D(G(z)): 0.0247 / 0.0086\n",
      "[0/10][118/391] Loss_D: 0.0234 Loss_G: 7.0454 D(x): 0.9911 D(G(z)): 0.0139 / 0.0018\n",
      "[0/10][119/391] Loss_D: 0.2944 Loss_G: 16.9723 D(x): 0.9886 D(G(z)): 0.1556 / 0.0000\n",
      "[0/10][120/391] Loss_D: 16.9107 Loss_G: 6.1932 D(x): 0.0000 D(G(z)): 0.0000 / 0.1556\n",
      "[0/10][121/391] Loss_D: 3.8160 Loss_G: 1.4033 D(x): 0.5489 D(G(z)): 0.1970 / 0.3349\n",
      "[0/10][122/391] Loss_D: 5.9758 Loss_G: 8.9432 D(x): 0.9983 D(G(z)): 0.9700 / 0.0570\n",
      "[0/10][123/391] Loss_D: 1.5231 Loss_G: 8.7441 D(x): 0.7536 D(G(z)): 0.1461 / 0.0073\n",
      "[0/10][124/391] Loss_D: 1.0325 Loss_G: 6.9024 D(x): 0.6901 D(G(z)): 0.0586 / 0.0351\n",
      "[0/10][125/391] Loss_D: 0.4918 Loss_G: 5.0383 D(x): 0.8794 D(G(z)): 0.1546 / 0.0241\n",
      "[0/10][126/391] Loss_D: 0.3729 Loss_G: 4.8109 D(x): 0.9959 D(G(z)): 0.2467 / 0.0351\n",
      "[0/10][127/391] Loss_D: 0.3249 Loss_G: 6.2628 D(x): 0.9841 D(G(z)): 0.2192 / 0.0078\n",
      "[0/10][128/391] Loss_D: 0.0751 Loss_G: 6.5360 D(x): 0.9829 D(G(z)): 0.0508 / 0.0049\n",
      "[0/10][129/391] Loss_D: 0.0402 Loss_G: 5.9834 D(x): 0.9913 D(G(z)): 0.0292 / 0.0077\n",
      "[0/10][130/391] Loss_D: 0.0652 Loss_G: 5.3028 D(x): 0.9829 D(G(z)): 0.0378 / 0.0112\n",
      "[0/10][131/391] Loss_D: 0.0873 Loss_G: 4.5424 D(x): 0.9829 D(G(z)): 0.0624 / 0.0230\n",
      "[0/10][132/391] Loss_D: 0.2648 Loss_G: 7.7474 D(x): 0.9880 D(G(z)): 0.2045 / 0.0022\n",
      "[0/10][133/391] Loss_D: 0.1677 Loss_G: 9.6660 D(x): 0.9690 D(G(z)): 0.1050 / 0.0003\n",
      "[0/10][134/391] Loss_D: 0.0792 Loss_G: 8.3539 D(x): 0.9506 D(G(z)): 0.0119 / 0.0029\n",
      "[0/10][135/391] Loss_D: 0.1051 Loss_G: 6.5120 D(x): 0.9432 D(G(z)): 0.0284 / 0.0112\n",
      "[0/10][136/391] Loss_D: 0.0701 Loss_G: 6.2817 D(x): 0.9514 D(G(z)): 0.0123 / 0.0095\n",
      "[0/10][137/391] Loss_D: 0.2280 Loss_G: 6.1007 D(x): 0.9684 D(G(z)): 0.1470 / 0.0048\n",
      "[0/10][138/391] Loss_D: 0.0930 Loss_G: 6.1125 D(x): 0.9843 D(G(z)): 0.0686 / 0.0070\n",
      "[0/10][139/391] Loss_D: 2.7607 Loss_G: 20.0630 D(x): 0.9705 D(G(z)): 0.8529 / 0.0000\n",
      "[0/10][140/391] Loss_D: 14.8949 Loss_G: 11.6155 D(x): 0.0001 D(G(z)): 0.0002 / 0.0094\n",
      "[0/10][141/391] Loss_D: 10.5849 Loss_G: 5.7806 D(x): 0.0250 D(G(z)): 0.0294 / 0.1989\n",
      "[0/10][142/391] Loss_D: 5.0218 Loss_G: 2.1796 D(x): 0.2790 D(G(z)): 0.2182 / 0.3391\n",
      "[0/10][143/391] Loss_D: 2.2829 Loss_G: 3.0856 D(x): 0.5449 D(G(z)): 0.3671 / 0.3066\n",
      "[0/10][144/391] Loss_D: 1.0724 Loss_G: 3.2730 D(x): 0.7822 D(G(z)): 0.3900 / 0.3189\n",
      "[0/10][145/391] Loss_D: 0.9396 Loss_G: 2.1002 D(x): 0.8587 D(G(z)): 0.4102 / 0.2705\n",
      "[0/10][146/391] Loss_D: 1.4799 Loss_G: 2.8391 D(x): 0.8280 D(G(z)): 0.4002 / 0.2807\n",
      "[0/10][147/391] Loss_D: 1.1028 Loss_G: 2.5310 D(x): 0.7998 D(G(z)): 0.4031 / 0.2070\n",
      "[0/10][148/391] Loss_D: 0.9140 Loss_G: 2.1906 D(x): 0.7374 D(G(z)): 0.2857 / 0.1814\n",
      "[0/10][149/391] Loss_D: 0.9132 Loss_G: 2.8766 D(x): 0.7407 D(G(z)): 0.2979 / 0.1912\n",
      "[0/10][150/391] Loss_D: 0.7267 Loss_G: 2.6248 D(x): 0.7632 D(G(z)): 0.2296 / 0.1491\n",
      "[0/10][151/391] Loss_D: 0.9439 Loss_G: 1.8952 D(x): 0.7069 D(G(z)): 0.2596 / 0.2630\n",
      "[0/10][152/391] Loss_D: 1.5574 Loss_G: 4.8835 D(x): 0.7256 D(G(z)): 0.4346 / 0.3361\n",
      "[0/10][153/391] Loss_D: 2.7318 Loss_G: 3.2228 D(x): 0.5783 D(G(z)): 0.4022 / 0.1146\n",
      "[0/10][154/391] Loss_D: 1.3880 Loss_G: 2.0740 D(x): 0.5992 D(G(z)): 0.2550 / 0.2475\n",
      "[0/10][155/391] Loss_D: 1.4916 Loss_G: 3.3964 D(x): 0.7384 D(G(z)): 0.4420 / 0.2535\n",
      "[0/10][156/391] Loss_D: 2.3402 Loss_G: 3.3641 D(x): 0.6372 D(G(z)): 0.5115 / 0.1718\n",
      "[0/10][157/391] Loss_D: 1.6649 Loss_G: 2.8804 D(x): 0.5389 D(G(z)): 0.2852 / 0.1704\n",
      "[0/10][158/391] Loss_D: 1.5226 Loss_G: 3.2565 D(x): 0.5886 D(G(z)): 0.3658 / 0.3634\n",
      "[0/10][159/391] Loss_D: 2.3391 Loss_G: 3.8876 D(x): 0.5408 D(G(z)): 0.4650 / 0.2953\n",
      "[0/10][160/391] Loss_D: 2.6562 Loss_G: 2.4370 D(x): 0.5084 D(G(z)): 0.3473 / 0.4745\n",
      "[0/10][161/391] Loss_D: 3.3767 Loss_G: 6.3673 D(x): 0.7113 D(G(z)): 0.6748 / 0.0076\n",
      "[0/10][162/391] Loss_D: 2.8785 Loss_G: 2.3043 D(x): 0.1655 D(G(z)): 0.0299 / 0.2427\n",
      "[0/10][163/391] Loss_D: 2.1152 Loss_G: 2.7480 D(x): 0.5329 D(G(z)): 0.4095 / 0.1545\n",
      "[0/10][164/391] Loss_D: 1.3266 Loss_G: 3.6239 D(x): 0.7100 D(G(z)): 0.4641 / 0.0601\n",
      "[0/10][165/391] Loss_D: 1.3496 Loss_G: 2.5480 D(x): 0.5462 D(G(z)): 0.2121 / 0.1900\n",
      "[0/10][166/391] Loss_D: 1.4716 Loss_G: 1.9710 D(x): 0.6084 D(G(z)): 0.3673 / 0.3377\n",
      "[0/10][167/391] Loss_D: 1.1666 Loss_G: 2.7783 D(x): 0.7225 D(G(z)): 0.3968 / 0.1242\n",
      "[0/10][168/391] Loss_D: 0.7377 Loss_G: 3.0916 D(x): 0.8121 D(G(z)): 0.2992 / 0.0933\n",
      "[0/10][169/391] Loss_D: 0.6699 Loss_G: 2.1005 D(x): 0.7413 D(G(z)): 0.2039 / 0.1861\n",
      "[0/10][170/391] Loss_D: 1.1453 Loss_G: 4.3474 D(x): 0.8151 D(G(z)): 0.5136 / 0.0421\n",
      "[0/10][171/391] Loss_D: 2.5259 Loss_G: 1.9919 D(x): 0.1810 D(G(z)): 0.0685 / 0.1799\n",
      "[0/10][172/391] Loss_D: 0.7737 Loss_G: 1.1389 D(x): 0.6758 D(G(z)): 0.2314 / 0.4388\n",
      "[0/10][173/391] Loss_D: 1.1554 Loss_G: 1.9666 D(x): 0.9173 D(G(z)): 0.5572 / 0.2164\n",
      "[0/10][174/391] Loss_D: 0.9910 Loss_G: 2.7752 D(x): 0.8470 D(G(z)): 0.4597 / 0.1101\n",
      "[0/10][175/391] Loss_D: 1.0467 Loss_G: 1.5978 D(x): 0.5674 D(G(z)): 0.2129 / 0.2528\n",
      "[0/10][176/391] Loss_D: 0.5469 Loss_G: 3.7840 D(x): 0.8872 D(G(z)): 0.2777 / 0.0457\n",
      "[0/10][177/391] Loss_D: 0.3390 Loss_G: 3.4856 D(x): 0.7871 D(G(z)): 0.0665 / 0.0585\n",
      "[0/10][178/391] Loss_D: 0.2317 Loss_G: 3.2363 D(x): 0.9376 D(G(z)): 0.1392 / 0.0555\n",
      "[0/10][179/391] Loss_D: 0.3989 Loss_G: 3.1187 D(x): 0.9439 D(G(z)): 0.2668 / 0.0812\n",
      "[0/10][180/391] Loss_D: 0.2271 Loss_G: 3.5727 D(x): 0.9064 D(G(z)): 0.1019 / 0.0560\n",
      "[0/10][181/391] Loss_D: 0.3151 Loss_G: 3.3278 D(x): 0.9629 D(G(z)): 0.2100 / 0.0642\n",
      "[0/10][182/391] Loss_D: 0.3268 Loss_G: 4.9953 D(x): 0.9008 D(G(z)): 0.1679 / 0.0154\n",
      "[0/10][183/391] Loss_D: 0.5868 Loss_G: 3.2513 D(x): 0.6262 D(G(z)): 0.0633 / 0.0580\n",
      "[0/10][184/391] Loss_D: 0.6609 Loss_G: 1.0251 D(x): 0.6242 D(G(z)): 0.1175 / 0.4633\n",
      "[0/10][185/391] Loss_D: 3.9554 Loss_G: 6.7131 D(x): 0.9943 D(G(z)): 0.9548 / 0.0048\n",
      "[0/10][186/391] Loss_D: 1.4706 Loss_G: 4.6446 D(x): 0.3428 D(G(z)): 0.0164 / 0.0298\n",
      "[0/10][187/391] Loss_D: 1.3444 Loss_G: 1.6267 D(x): 0.3541 D(G(z)): 0.0562 / 0.2811\n",
      "[0/10][188/391] Loss_D: 0.8542 Loss_G: 1.4040 D(x): 0.7623 D(G(z)): 0.3563 / 0.3743\n",
      "[0/10][189/391] Loss_D: 1.5042 Loss_G: 2.8726 D(x): 0.9008 D(G(z)): 0.6383 / 0.1521\n",
      "[0/10][190/391] Loss_D: 1.8746 Loss_G: 0.8370 D(x): 0.4263 D(G(z)): 0.2347 / 0.5191\n",
      "[0/10][191/391] Loss_D: 2.4581 Loss_G: 2.6083 D(x): 0.9653 D(G(z)): 0.7614 / 0.1129\n",
      "[0/10][192/391] Loss_D: 1.4502 Loss_G: 2.8550 D(x): 0.5581 D(G(z)): 0.3489 / 0.1028\n",
      "[0/10][193/391] Loss_D: 0.7994 Loss_G: 2.0394 D(x): 0.6127 D(G(z)): 0.1352 / 0.1964\n",
      "[0/10][194/391] Loss_D: 0.7900 Loss_G: 1.9536 D(x): 0.7876 D(G(z)): 0.3258 / 0.2013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][195/391] Loss_D: 0.6686 Loss_G: 2.7718 D(x): 0.8233 D(G(z)): 0.3066 / 0.0870\n",
      "[0/10][196/391] Loss_D: 0.8563 Loss_G: 2.2083 D(x): 0.7218 D(G(z)): 0.3330 / 0.1638\n",
      "[0/10][197/391] Loss_D: 0.4757 Loss_G: 2.6853 D(x): 0.7462 D(G(z)): 0.1038 / 0.0948\n",
      "[0/10][198/391] Loss_D: 0.2727 Loss_G: 2.6466 D(x): 0.9238 D(G(z)): 0.1579 / 0.0955\n",
      "[0/10][199/391] Loss_D: 0.6150 Loss_G: 4.3788 D(x): 0.9553 D(G(z)): 0.3951 / 0.0276\n",
      "[0/10][200/391] Loss_D: 0.7142 Loss_G: 2.0679 D(x): 0.6359 D(G(z)): 0.1102 / 0.1899\n",
      "[0/10][201/391] Loss_D: 0.3221 Loss_G: 3.2781 D(x): 0.8979 D(G(z)): 0.1725 / 0.0502\n",
      "[0/10][202/391] Loss_D: 1.2701 Loss_G: 6.1909 D(x): 0.8169 D(G(z)): 0.5947 / 0.0038\n",
      "[0/10][203/391] Loss_D: 2.8594 Loss_G: 1.4244 D(x): 0.1109 D(G(z)): 0.0058 / 0.3578\n",
      "[0/10][204/391] Loss_D: 1.0573 Loss_G: 0.6493 D(x): 0.5453 D(G(z)): 0.0865 / 0.5726\n",
      "[0/10][205/391] Loss_D: 1.5341 Loss_G: 3.9994 D(x): 0.9920 D(G(z)): 0.6857 / 0.0435\n",
      "[0/10][206/391] Loss_D: 0.3292 Loss_G: 4.1742 D(x): 0.8391 D(G(z)): 0.0992 / 0.0319\n",
      "[0/10][207/391] Loss_D: 0.5432 Loss_G: 2.0669 D(x): 0.7067 D(G(z)): 0.1052 / 0.1834\n",
      "[0/10][208/391] Loss_D: 1.9660 Loss_G: 4.8112 D(x): 0.9779 D(G(z)): 0.7724 / 0.0280\n",
      "[0/10][209/391] Loss_D: 3.6448 Loss_G: 2.6422 D(x): 0.0726 D(G(z)): 0.0408 / 0.1086\n",
      "[0/10][210/391] Loss_D: 1.1908 Loss_G: 1.2849 D(x): 0.4510 D(G(z)): 0.1409 / 0.3738\n",
      "[0/10][211/391] Loss_D: 1.3127 Loss_G: 1.4729 D(x): 0.7871 D(G(z)): 0.5057 / 0.3392\n",
      "[0/10][212/391] Loss_D: 1.0415 Loss_G: 1.4552 D(x): 0.7526 D(G(z)): 0.4038 / 0.2880\n",
      "[0/10][213/391] Loss_D: 0.7857 Loss_G: 1.9043 D(x): 0.7893 D(G(z)): 0.3566 / 0.2115\n",
      "[0/10][214/391] Loss_D: 0.8778 Loss_G: 1.7398 D(x): 0.6616 D(G(z)): 0.2969 / 0.2204\n",
      "[0/10][215/391] Loss_D: 0.7089 Loss_G: 2.9598 D(x): 0.7236 D(G(z)): 0.2743 / 0.0798\n",
      "[0/10][216/391] Loss_D: 0.7604 Loss_G: 2.0298 D(x): 0.7090 D(G(z)): 0.2897 / 0.1795\n",
      "[0/10][217/391] Loss_D: 0.9142 Loss_G: 2.0609 D(x): 0.6984 D(G(z)): 0.3364 / 0.2115\n",
      "[0/10][218/391] Loss_D: 1.0321 Loss_G: 2.9961 D(x): 0.6689 D(G(z)): 0.3480 / 0.1734\n",
      "[0/10][219/391] Loss_D: 1.2455 Loss_G: 2.1278 D(x): 0.6083 D(G(z)): 0.2212 / 0.1747\n",
      "[0/10][220/391] Loss_D: 0.7861 Loss_G: 2.4776 D(x): 0.7988 D(G(z)): 0.3683 / 0.1049\n",
      "[0/10][221/391] Loss_D: 0.7268 Loss_G: 2.6942 D(x): 0.7109 D(G(z)): 0.2813 / 0.0892\n",
      "[0/10][222/391] Loss_D: 1.1574 Loss_G: 1.8322 D(x): 0.5866 D(G(z)): 0.3853 / 0.1998\n",
      "[0/10][223/391] Loss_D: 0.6896 Loss_G: 1.8501 D(x): 0.6076 D(G(z)): 0.1196 / 0.2142\n",
      "[0/10][224/391] Loss_D: 1.3257 Loss_G: 5.5850 D(x): 0.8578 D(G(z)): 0.6041 / 0.0124\n",
      "[0/10][225/391] Loss_D: 3.3485 Loss_G: 1.9023 D(x): 0.0989 D(G(z)): 0.0583 / 0.2527\n",
      "[0/10][226/391] Loss_D: 1.1067 Loss_G: 1.3750 D(x): 0.6226 D(G(z)): 0.3215 / 0.4325\n",
      "[0/10][227/391] Loss_D: 1.2655 Loss_G: 2.3664 D(x): 0.8347 D(G(z)): 0.5401 / 0.2036\n",
      "[0/10][228/391] Loss_D: 1.3617 Loss_G: 2.6496 D(x): 0.7068 D(G(z)): 0.5065 / 0.1699\n",
      "[0/10][229/391] Loss_D: 1.3171 Loss_G: 2.0714 D(x): 0.4942 D(G(z)): 0.2385 / 0.1793\n",
      "[0/10][230/391] Loss_D: 1.5163 Loss_G: 2.5074 D(x): 0.5796 D(G(z)): 0.4851 / 0.1617\n",
      "[0/10][231/391] Loss_D: 0.8316 Loss_G: 1.9648 D(x): 0.7082 D(G(z)): 0.2802 / 0.2022\n",
      "[0/10][232/391] Loss_D: 0.6731 Loss_G: 2.2628 D(x): 0.8031 D(G(z)): 0.3012 / 0.1390\n",
      "[0/10][233/391] Loss_D: 0.5975 Loss_G: 1.6998 D(x): 0.8058 D(G(z)): 0.2739 / 0.2236\n",
      "[0/10][234/391] Loss_D: 0.5413 Loss_G: 3.0260 D(x): 0.9067 D(G(z)): 0.3253 / 0.0834\n",
      "[0/10][235/391] Loss_D: 0.4356 Loss_G: 2.5405 D(x): 0.8055 D(G(z)): 0.1574 / 0.1026\n",
      "[0/10][236/391] Loss_D: 0.2640 Loss_G: 3.3337 D(x): 0.8804 D(G(z)): 0.1182 / 0.0526\n",
      "[0/10][237/391] Loss_D: 0.7225 Loss_G: 3.1495 D(x): 0.7056 D(G(z)): 0.2452 / 0.0692\n",
      "[0/10][238/391] Loss_D: 1.9644 Loss_G: 0.6815 D(x): 0.2026 D(G(z)): 0.1052 / 0.5568\n",
      "[0/10][239/391] Loss_D: 1.2914 Loss_G: 2.1106 D(x): 0.8968 D(G(z)): 0.5704 / 0.3106\n",
      "[0/10][240/391] Loss_D: 1.3778 Loss_G: 1.7908 D(x): 0.7133 D(G(z)): 0.4326 / 0.2117\n",
      "[0/10][241/391] Loss_D: 1.2827 Loss_G: 1.5500 D(x): 0.5989 D(G(z)): 0.4182 / 0.2710\n",
      "[0/10][242/391] Loss_D: 1.6328 Loss_G: 1.4869 D(x): 0.5835 D(G(z)): 0.5828 / 0.2758\n",
      "[0/10][243/391] Loss_D: 1.4619 Loss_G: 1.7669 D(x): 0.4539 D(G(z)): 0.3826 / 0.2207\n",
      "[0/10][244/391] Loss_D: 1.3199 Loss_G: 1.5296 D(x): 0.5133 D(G(z)): 0.3818 / 0.2650\n",
      "[0/10][245/391] Loss_D: 1.2486 Loss_G: 1.2213 D(x): 0.5293 D(G(z)): 0.3790 / 0.3485\n",
      "[0/10][246/391] Loss_D: 1.3012 Loss_G: 2.8351 D(x): 0.6778 D(G(z)): 0.5465 / 0.0959\n",
      "[0/10][247/391] Loss_D: 1.6403 Loss_G: 0.8633 D(x): 0.3388 D(G(z)): 0.1795 / 0.5276\n",
      "[0/10][248/391] Loss_D: 1.5489 Loss_G: 2.1526 D(x): 0.6696 D(G(z)): 0.5417 / 0.2742\n",
      "[0/10][249/391] Loss_D: 1.6994 Loss_G: 1.7366 D(x): 0.5629 D(G(z)): 0.4078 / 0.2610\n",
      "[0/10][250/391] Loss_D: 1.2798 Loss_G: 1.9573 D(x): 0.5629 D(G(z)): 0.3265 / 0.2322\n",
      "[0/10][251/391] Loss_D: 1.0893 Loss_G: 1.6550 D(x): 0.6414 D(G(z)): 0.3353 / 0.2810\n",
      "[0/10][252/391] Loss_D: 1.0344 Loss_G: 1.8223 D(x): 0.6748 D(G(z)): 0.3686 / 0.2709\n",
      "[0/10][253/391] Loss_D: 1.4258 Loss_G: 3.5023 D(x): 0.7675 D(G(z)): 0.5332 / 0.1363\n",
      "[0/10][254/391] Loss_D: 1.8247 Loss_G: 1.2744 D(x): 0.4155 D(G(z)): 0.2608 / 0.3490\n",
      "[0/10][255/391] Loss_D: 1.5667 Loss_G: 2.1077 D(x): 0.7291 D(G(z)): 0.6207 / 0.3241\n",
      "[0/10][256/391] Loss_D: 1.6476 Loss_G: 1.4816 D(x): 0.5733 D(G(z)): 0.3877 / 0.3123\n",
      "[0/10][257/391] Loss_D: 1.1472 Loss_G: 1.9556 D(x): 0.6348 D(G(z)): 0.3147 / 0.2792\n",
      "[0/10][258/391] Loss_D: 1.0250 Loss_G: 1.2587 D(x): 0.7221 D(G(z)): 0.3838 / 0.3318\n",
      "[0/10][259/391] Loss_D: 0.8442 Loss_G: 2.4362 D(x): 0.8078 D(G(z)): 0.4099 / 0.1543\n",
      "[0/10][260/391] Loss_D: 0.9146 Loss_G: 1.2302 D(x): 0.6037 D(G(z)): 0.2092 / 0.3508\n",
      "[0/10][261/391] Loss_D: 0.9112 Loss_G: 2.8028 D(x): 0.8728 D(G(z)): 0.4784 / 0.1090\n",
      "[0/10][262/391] Loss_D: 1.3113 Loss_G: 0.7647 D(x): 0.4252 D(G(z)): 0.1623 / 0.5364\n",
      "[0/10][263/391] Loss_D: 1.6535 Loss_G: 2.8136 D(x): 0.8843 D(G(z)): 0.6705 / 0.1012\n",
      "[0/10][264/391] Loss_D: 1.8272 Loss_G: 1.1331 D(x): 0.2677 D(G(z)): 0.1330 / 0.3873\n",
      "[0/10][265/391] Loss_D: 1.2157 Loss_G: 0.8445 D(x): 0.6308 D(G(z)): 0.4439 / 0.4616\n",
      "[0/10][266/391] Loss_D: 1.1404 Loss_G: 1.8525 D(x): 0.7096 D(G(z)): 0.4889 / 0.2033\n",
      "[0/10][267/391] Loss_D: 1.4659 Loss_G: 0.6200 D(x): 0.3995 D(G(z)): 0.2788 / 0.6026\n",
      "[0/10][268/391] Loss_D: 1.8621 Loss_G: 2.5525 D(x): 0.8210 D(G(z)): 0.7309 / 0.1034\n",
      "[0/10][269/391] Loss_D: 1.7732 Loss_G: 1.1996 D(x): 0.2320 D(G(z)): 0.1220 / 0.3467\n",
      "[0/10][270/391] Loss_D: 1.3311 Loss_G: 1.1484 D(x): 0.5306 D(G(z)): 0.3866 / 0.5027\n",
      "[0/10][271/391] Loss_D: 1.8297 Loss_G: 1.7831 D(x): 0.6673 D(G(z)): 0.5696 / 0.2718\n",
      "[0/10][272/391] Loss_D: 1.3068 Loss_G: 1.9984 D(x): 0.5615 D(G(z)): 0.3491 / 0.1887\n",
      "[0/10][273/391] Loss_D: 1.3739 Loss_G: 1.2121 D(x): 0.4802 D(G(z)): 0.2544 / 0.4145\n",
      "[0/10][274/391] Loss_D: 1.5890 Loss_G: 1.2982 D(x): 0.6860 D(G(z)): 0.4707 / 0.3145\n",
      "[0/10][275/391] Loss_D: 1.0400 Loss_G: 1.3863 D(x): 0.6278 D(G(z)): 0.3756 / 0.2966\n",
      "[0/10][276/391] Loss_D: 0.9914 Loss_G: 1.3539 D(x): 0.6687 D(G(z)): 0.3753 / 0.2843\n",
      "[0/10][277/391] Loss_D: 1.0576 Loss_G: 1.3214 D(x): 0.5870 D(G(z)): 0.3523 / 0.3178\n",
      "[0/10][278/391] Loss_D: 0.9051 Loss_G: 1.3473 D(x): 0.6740 D(G(z)): 0.3566 / 0.2931\n",
      "[0/10][279/391] Loss_D: 1.0358 Loss_G: 1.7319 D(x): 0.6845 D(G(z)): 0.4188 / 0.2233\n",
      "[0/10][280/391] Loss_D: 1.0438 Loss_G: 0.7745 D(x): 0.5222 D(G(z)): 0.2285 / 0.5159\n",
      "[0/10][281/391] Loss_D: 1.3048 Loss_G: 2.8409 D(x): 0.8912 D(G(z)): 0.6335 / 0.1136\n",
      "[0/10][282/391] Loss_D: 1.4191 Loss_G: 1.5149 D(x): 0.4248 D(G(z)): 0.1544 / 0.2783\n",
      "[0/10][283/391] Loss_D: 1.0760 Loss_G: 1.2657 D(x): 0.5972 D(G(z)): 0.3102 / 0.3848\n",
      "[0/10][284/391] Loss_D: 1.0943 Loss_G: 1.2137 D(x): 0.7488 D(G(z)): 0.4239 / 0.3400\n",
      "[0/10][285/391] Loss_D: 0.8440 Loss_G: 2.1751 D(x): 0.7966 D(G(z)): 0.4127 / 0.1503\n",
      "[0/10][286/391] Loss_D: 0.7245 Loss_G: 1.6248 D(x): 0.6398 D(G(z)): 0.1972 / 0.2268\n",
      "[0/10][287/391] Loss_D: 0.7506 Loss_G: 1.6057 D(x): 0.7251 D(G(z)): 0.3084 / 0.2414\n",
      "[0/10][288/391] Loss_D: 0.6553 Loss_G: 2.6658 D(x): 0.8105 D(G(z)): 0.3195 / 0.1030\n",
      "[0/10][289/391] Loss_D: 0.8730 Loss_G: 2.7767 D(x): 0.6592 D(G(z)): 0.3094 / 0.0936\n",
      "[0/10][290/391] Loss_D: 0.6567 Loss_G: 1.7084 D(x): 0.6455 D(G(z)): 0.1399 / 0.2321\n",
      "[0/10][291/391] Loss_D: 1.6011 Loss_G: 5.5890 D(x): 0.8661 D(G(z)): 0.7185 / 0.0129\n",
      "[0/10][292/391] Loss_D: 3.6283 Loss_G: 0.9704 D(x): 0.0722 D(G(z)): 0.0286 / 0.4443\n",
      "[0/10][293/391] Loss_D: 1.7974 Loss_G: 1.1230 D(x): 0.5603 D(G(z)): 0.5157 / 0.5358\n",
      "[0/10][294/391] Loss_D: 1.7576 Loss_G: 1.0752 D(x): 0.6772 D(G(z)): 0.5957 / 0.3830\n",
      "[0/10][295/391] Loss_D: 1.2262 Loss_G: 1.9461 D(x): 0.6075 D(G(z)): 0.4387 / 0.1965\n",
      "[0/10][296/391] Loss_D: 1.1373 Loss_G: 1.4322 D(x): 0.5154 D(G(z)): 0.2853 / 0.2725\n",
      "[0/10][297/391] Loss_D: 1.1528 Loss_G: 1.1443 D(x): 0.5712 D(G(z)): 0.3920 / 0.3720\n",
      "[0/10][298/391] Loss_D: 1.2451 Loss_G: 1.4743 D(x): 0.6455 D(G(z)): 0.4958 / 0.3021\n",
      "[0/10][299/391] Loss_D: 1.3229 Loss_G: 1.2773 D(x): 0.5284 D(G(z)): 0.3876 / 0.3174\n",
      "[0/10][300/391] Loss_D: 0.9832 Loss_G: 1.4516 D(x): 0.6627 D(G(z)): 0.3860 / 0.2894\n",
      "[0/10][301/391] Loss_D: 0.9655 Loss_G: 1.7053 D(x): 0.6636 D(G(z)): 0.3598 / 0.2234\n",
      "[0/10][302/391] Loss_D: 0.8921 Loss_G: 1.7821 D(x): 0.7337 D(G(z)): 0.3983 / 0.1981\n",
      "[0/10][303/391] Loss_D: 0.9603 Loss_G: 1.7801 D(x): 0.6688 D(G(z)): 0.3830 / 0.2065\n",
      "[0/10][304/391] Loss_D: 0.9131 Loss_G: 1.8813 D(x): 0.6282 D(G(z)): 0.3045 / 0.1830\n",
      "[0/10][305/391] Loss_D: 0.7960 Loss_G: 1.7114 D(x): 0.7112 D(G(z)): 0.3310 / 0.2147\n",
      "[0/10][306/391] Loss_D: 0.7338 Loss_G: 2.7408 D(x): 0.7344 D(G(z)): 0.2989 / 0.1373\n",
      "[0/10][307/391] Loss_D: 0.9586 Loss_G: 1.9543 D(x): 0.6515 D(G(z)): 0.2698 / 0.2979\n",
      "[0/10][308/391] Loss_D: 1.5674 Loss_G: 4.8150 D(x): 0.7284 D(G(z)): 0.4781 / 0.0292\n",
      "[0/10][309/391] Loss_D: 1.8235 Loss_G: 0.9375 D(x): 0.3085 D(G(z)): 0.0794 / 0.4686\n",
      "[0/10][310/391] Loss_D: 1.4894 Loss_G: 2.8104 D(x): 0.8738 D(G(z)): 0.6575 / 0.0943\n",
      "[0/10][311/391] Loss_D: 1.4468 Loss_G: 1.4861 D(x): 0.4101 D(G(z)): 0.2134 / 0.3233\n",
      "[0/10][312/391] Loss_D: 1.2654 Loss_G: 1.8557 D(x): 0.6549 D(G(z)): 0.4089 / 0.2685\n",
      "[0/10][313/391] Loss_D: 1.3338 Loss_G: 1.4092 D(x): 0.5986 D(G(z)): 0.3921 / 0.3098\n",
      "[0/10][314/391] Loss_D: 1.2076 Loss_G: 1.8121 D(x): 0.6655 D(G(z)): 0.4432 / 0.2123\n",
      "[0/10][315/391] Loss_D: 1.3131 Loss_G: 1.8446 D(x): 0.5258 D(G(z)): 0.3884 / 0.2136\n",
      "[0/10][316/391] Loss_D: 1.1996 Loss_G: 1.4567 D(x): 0.5199 D(G(z)): 0.3395 / 0.2770\n",
      "[0/10][317/391] Loss_D: 1.3313 Loss_G: 3.3589 D(x): 0.6842 D(G(z)): 0.5386 / 0.0453\n",
      "[0/10][318/391] Loss_D: 1.0971 Loss_G: 1.4121 D(x): 0.4328 D(G(z)): 0.0978 / 0.2886\n",
      "[0/10][319/391] Loss_D: 0.8505 Loss_G: 2.5577 D(x): 0.8309 D(G(z)): 0.4467 / 0.1236\n",
      "[0/10][320/391] Loss_D: 0.7688 Loss_G: 2.8492 D(x): 0.7887 D(G(z)): 0.3539 / 0.0778\n",
      "[0/10][321/391] Loss_D: 0.9624 Loss_G: 1.8132 D(x): 0.5632 D(G(z)): 0.2497 / 0.2101\n",
      "[0/10][322/391] Loss_D: 1.2158 Loss_G: 1.5908 D(x): 0.6603 D(G(z)): 0.4778 / 0.2790\n",
      "[0/10][323/391] Loss_D: 1.6059 Loss_G: 2.5018 D(x): 0.7024 D(G(z)): 0.6489 / 0.1370\n",
      "[0/10][324/391] Loss_D: 1.5320 Loss_G: 1.8478 D(x): 0.4024 D(G(z)): 0.2849 / 0.3290\n",
      "[0/10][325/391] Loss_D: 1.3380 Loss_G: 1.0975 D(x): 0.6444 D(G(z)): 0.4209 / 0.3874\n",
      "[0/10][326/391] Loss_D: 1.0495 Loss_G: 2.8672 D(x): 0.8279 D(G(z)): 0.4725 / 0.1264\n",
      "[0/10][327/391] Loss_D: 1.1779 Loss_G: 1.6636 D(x): 0.5433 D(G(z)): 0.2680 / 0.2455\n",
      "[0/10][328/391] Loss_D: 0.8238 Loss_G: 2.4533 D(x): 0.7994 D(G(z)): 0.3951 / 0.1452\n",
      "[0/10][329/391] Loss_D: 1.0578 Loss_G: 1.8105 D(x): 0.6364 D(G(z)): 0.3445 / 0.2064\n",
      "[0/10][330/391] Loss_D: 1.0963 Loss_G: 2.1849 D(x): 0.6558 D(G(z)): 0.4200 / 0.1533\n",
      "[0/10][331/391] Loss_D: 1.3193 Loss_G: 3.4143 D(x): 0.6291 D(G(z)): 0.4821 / 0.0508\n",
      "[0/10][332/391] Loss_D: 1.4190 Loss_G: 1.5661 D(x): 0.3596 D(G(z)): 0.1527 / 0.2992\n",
      "[0/10][333/391] Loss_D: 0.1493 Loss_G: 2.5825 D(x): 0.9543 D(G(z)): 0.0909 / 0.1028\n",
      "[0/10][334/391] Loss_D: 0.1899 Loss_G: 3.0550 D(x): 0.9872 D(G(z)): 0.1565 / 0.0637\n",
      "[0/10][335/391] Loss_D: 0.2679 Loss_G: 3.3340 D(x): 0.9849 D(G(z)): 0.2091 / 0.0519\n",
      "[0/10][336/391] Loss_D: 0.3080 Loss_G: 3.9104 D(x): 0.9902 D(G(z)): 0.2396 / 0.0316\n",
      "[0/10][337/391] Loss_D: 0.4156 Loss_G: 2.3127 D(x): 0.7871 D(G(z)): 0.1035 / 0.1503\n",
      "[0/10][338/391] Loss_D: 1.8008 Loss_G: 6.7628 D(x): 0.7214 D(G(z)): 0.6563 / 0.0059\n",
      "[0/10][339/391] Loss_D: 3.8094 Loss_G: 2.1234 D(x): 0.0579 D(G(z)): 0.0095 / 0.1866\n",
      "[0/10][340/391] Loss_D: 0.8457 Loss_G: 1.6831 D(x): 0.9171 D(G(z)): 0.4259 / 0.2940\n",
      "[0/10][341/391] Loss_D: 1.2665 Loss_G: 3.6295 D(x): 0.9441 D(G(z)): 0.6289 / 0.0525\n",
      "[0/10][342/391] Loss_D: 1.4016 Loss_G: 2.7997 D(x): 0.5284 D(G(z)): 0.3024 / 0.1240\n",
      "[0/10][343/391] Loss_D: 1.4700 Loss_G: 2.3449 D(x): 0.4808 D(G(z)): 0.2799 / 0.1582\n",
      "[0/10][344/391] Loss_D: 1.1819 Loss_G: 2.4147 D(x): 0.6475 D(G(z)): 0.3908 / 0.1461\n",
      "[0/10][345/391] Loss_D: 1.4550 Loss_G: 3.8159 D(x): 0.6356 D(G(z)): 0.5109 / 0.0416\n",
      "[0/10][346/391] Loss_D: 1.3126 Loss_G: 2.3149 D(x): 0.3642 D(G(z)): 0.0957 / 0.1512\n",
      "[0/10][347/391] Loss_D: 1.1049 Loss_G: 4.1727 D(x): 0.8675 D(G(z)): 0.5269 / 0.0290\n",
      "[0/10][348/391] Loss_D: 1.1189 Loss_G: 2.3831 D(x): 0.4908 D(G(z)): 0.1714 / 0.1315\n",
      "[0/10][349/391] Loss_D: 1.1744 Loss_G: 4.1012 D(x): 0.8608 D(G(z)): 0.5718 / 0.0353\n",
      "[0/10][350/391] Loss_D: 0.8435 Loss_G: 2.6003 D(x): 0.5328 D(G(z)): 0.0745 / 0.1245\n",
      "[0/10][351/391] Loss_D: 0.9570 Loss_G: 1.9302 D(x): 0.6028 D(G(z)): 0.2343 / 0.2741\n",
      "[0/10][352/391] Loss_D: 0.8175 Loss_G: 2.4643 D(x): 0.8964 D(G(z)): 0.4673 / 0.1995\n",
      "[0/10][353/391] Loss_D: 0.9959 Loss_G: 1.8546 D(x): 0.6840 D(G(z)): 0.3939 / 0.1919\n",
      "[0/10][354/391] Loss_D: 1.0948 Loss_G: 4.9145 D(x): 0.5552 D(G(z)): 0.3287 / 0.0109\n",
      "[0/10][355/391] Loss_D: 1.9613 Loss_G: 1.3800 D(x): 0.2418 D(G(z)): 0.1683 / 0.3250\n",
      "[0/10][356/391] Loss_D: 2.0586 Loss_G: 1.0651 D(x): 0.3530 D(G(z)): 0.4531 / 0.3944\n",
      "[0/10][357/391] Loss_D: 1.6842 Loss_G: 0.7649 D(x): 0.5090 D(G(z)): 0.5755 / 0.5036\n",
      "[0/10][358/391] Loss_D: 1.4624 Loss_G: 1.6084 D(x): 0.5869 D(G(z)): 0.5301 / 0.3463\n",
      "[0/10][359/391] Loss_D: 1.4723 Loss_G: 0.8630 D(x): 0.4899 D(G(z)): 0.4011 / 0.4664\n",
      "[0/10][360/391] Loss_D: 1.2355 Loss_G: 1.5468 D(x): 0.6757 D(G(z)): 0.4854 / 0.3203\n",
      "[0/10][361/391] Loss_D: 1.1155 Loss_G: 1.7306 D(x): 0.6580 D(G(z)): 0.3607 / 0.2684\n",
      "[0/10][362/391] Loss_D: 1.0450 Loss_G: 1.7790 D(x): 0.7272 D(G(z)): 0.3558 / 0.2705\n",
      "[0/10][363/391] Loss_D: 2.0105 Loss_G: 3.8572 D(x): 0.8294 D(G(z)): 0.6525 / 0.0542\n",
      "[0/10][364/391] Loss_D: 2.3499 Loss_G: 1.9318 D(x): 0.2067 D(G(z)): 0.1352 / 0.3006\n",
      "[0/10][365/391] Loss_D: 1.4291 Loss_G: 1.2428 D(x): 0.5747 D(G(z)): 0.3852 / 0.3572\n",
      "[0/10][366/391] Loss_D: 1.0878 Loss_G: 1.8799 D(x): 0.7146 D(G(z)): 0.4887 / 0.2759\n",
      "[0/10][367/391] Loss_D: 1.2250 Loss_G: 1.2802 D(x): 0.5802 D(G(z)): 0.3827 / 0.3619\n",
      "[0/10][368/391] Loss_D: 1.1243 Loss_G: 1.8887 D(x): 0.7237 D(G(z)): 0.4999 / 0.2076\n",
      "[0/10][369/391] Loss_D: 1.2644 Loss_G: 0.6811 D(x): 0.4615 D(G(z)): 0.2819 / 0.5496\n",
      "[0/10][370/391] Loss_D: 1.4696 Loss_G: 2.4336 D(x): 0.8505 D(G(z)): 0.6612 / 0.1577\n",
      "[0/10][371/391] Loss_D: 1.7060 Loss_G: 1.0547 D(x): 0.3105 D(G(z)): 0.2516 / 0.4171\n",
      "[0/10][372/391] Loss_D: 1.3863 Loss_G: 0.9014 D(x): 0.6493 D(G(z)): 0.5555 / 0.4377\n",
      "[0/10][373/391] Loss_D: 1.3862 Loss_G: 1.7939 D(x): 0.6516 D(G(z)): 0.5530 / 0.2056\n",
      "[0/10][374/391] Loss_D: 1.3487 Loss_G: 0.8304 D(x): 0.3815 D(G(z)): 0.2350 / 0.4744\n",
      "[0/10][375/391] Loss_D: 1.3155 Loss_G: 1.8424 D(x): 0.6888 D(G(z)): 0.5512 / 0.2515\n",
      "[0/10][376/391] Loss_D: 1.4360 Loss_G: 1.0182 D(x): 0.5036 D(G(z)): 0.3612 / 0.4277\n",
      "[0/10][377/391] Loss_D: 1.2569 Loss_G: 1.7329 D(x): 0.6740 D(G(z)): 0.4694 / 0.2428\n",
      "[0/10][378/391] Loss_D: 1.3512 Loss_G: 0.9943 D(x): 0.5072 D(G(z)): 0.3491 / 0.3976\n",
      "[0/10][379/391] Loss_D: 1.2421 Loss_G: 2.1163 D(x): 0.7364 D(G(z)): 0.5751 / 0.1484\n",
      "[0/10][380/391] Loss_D: 1.6033 Loss_G: 0.7119 D(x): 0.3025 D(G(z)): 0.2086 / 0.5365\n",
      "[0/10][381/391] Loss_D: 1.5075 Loss_G: 1.4349 D(x): 0.6879 D(G(z)): 0.6041 / 0.2757\n",
      "[0/10][382/391] Loss_D: 1.1965 Loss_G: 1.5318 D(x): 0.5469 D(G(z)): 0.3856 / 0.2808\n",
      "[0/10][383/391] Loss_D: 1.2713 Loss_G: 1.6656 D(x): 0.5729 D(G(z)): 0.4160 / 0.2182\n",
      "[0/10][384/391] Loss_D: 1.2066 Loss_G: 1.0487 D(x): 0.5103 D(G(z)): 0.3275 / 0.3805\n",
      "[0/10][385/391] Loss_D: 1.3174 Loss_G: 2.7153 D(x): 0.7080 D(G(z)): 0.5708 / 0.0931\n",
      "[0/10][386/391] Loss_D: 1.3542 Loss_G: 0.9201 D(x): 0.3843 D(G(z)): 0.2114 / 0.4440\n",
      "[0/10][387/391] Loss_D: 1.4014 Loss_G: 1.4413 D(x): 0.7160 D(G(z)): 0.5899 / 0.2955\n",
      "[0/10][388/391] Loss_D: 1.2740 Loss_G: 1.0269 D(x): 0.5196 D(G(z)): 0.3611 / 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][389/391] Loss_D: 1.2190 Loss_G: 1.6406 D(x): 0.6756 D(G(z)): 0.4927 / 0.2433\n",
      "[0/10][390/391] Loss_D: 1.2873 Loss_G: 1.1439 D(x): 0.5186 D(G(z)): 0.3761 / 0.3628\n",
      "[1/10][0/391] Loss_D: 1.1133 Loss_G: 1.6914 D(x): 0.6833 D(G(z)): 0.4632 / 0.2303\n",
      "[1/10][1/391] Loss_D: 1.1344 Loss_G: 1.6966 D(x): 0.5962 D(G(z)): 0.3838 / 0.2134\n",
      "[1/10][2/391] Loss_D: 1.0969 Loss_G: 1.1692 D(x): 0.5554 D(G(z)): 0.3332 / 0.3433\n",
      "[1/10][3/391] Loss_D: 1.3003 Loss_G: 2.6287 D(x): 0.7028 D(G(z)): 0.5547 / 0.1346\n",
      "[1/10][4/391] Loss_D: 1.7554 Loss_G: 0.7518 D(x): 0.3549 D(G(z)): 0.2497 / 0.5167\n",
      "[1/10][5/391] Loss_D: 1.9027 Loss_G: 2.3631 D(x): 0.6991 D(G(z)): 0.6706 / 0.1326\n",
      "[1/10][6/391] Loss_D: 1.6717 Loss_G: 0.8491 D(x): 0.3385 D(G(z)): 0.2866 / 0.4647\n",
      "[1/10][7/391] Loss_D: 1.3788 Loss_G: 1.5572 D(x): 0.6719 D(G(z)): 0.5546 / 0.2384\n",
      "[1/10][8/391] Loss_D: 1.1732 Loss_G: 1.2859 D(x): 0.5027 D(G(z)): 0.3342 / 0.2965\n",
      "[1/10][9/391] Loss_D: 0.9980 Loss_G: 1.3827 D(x): 0.6181 D(G(z)): 0.3688 / 0.2730\n",
      "[1/10][10/391] Loss_D: 0.8919 Loss_G: 2.1072 D(x): 0.7106 D(G(z)): 0.3921 / 0.1342\n",
      "[1/10][11/391] Loss_D: 0.8048 Loss_G: 1.2296 D(x): 0.5924 D(G(z)): 0.1928 / 0.3526\n",
      "[1/10][12/391] Loss_D: 1.0645 Loss_G: 3.3126 D(x): 0.8547 D(G(z)): 0.5377 / 0.0494\n",
      "[1/10][13/391] Loss_D: 1.2484 Loss_G: 1.1439 D(x): 0.4109 D(G(z)): 0.1639 / 0.3803\n",
      "[1/10][14/391] Loss_D: 1.4046 Loss_G: 2.8964 D(x): 0.7958 D(G(z)): 0.6066 / 0.0842\n",
      "[1/10][15/391] Loss_D: 1.5900 Loss_G: 1.0229 D(x): 0.3297 D(G(z)): 0.2169 / 0.4172\n",
      "[1/10][16/391] Loss_D: 1.8981 Loss_G: 2.8589 D(x): 0.7237 D(G(z)): 0.7413 / 0.0742\n",
      "[1/10][17/391] Loss_D: 2.0136 Loss_G: 0.7196 D(x): 0.1977 D(G(z)): 0.1906 / 0.5102\n",
      "[1/10][18/391] Loss_D: 1.4493 Loss_G: 1.4103 D(x): 0.6150 D(G(z)): 0.5678 / 0.2817\n",
      "[1/10][19/391] Loss_D: 1.2692 Loss_G: 1.3446 D(x): 0.5350 D(G(z)): 0.4124 / 0.2859\n",
      "[1/10][20/391] Loss_D: 1.0392 Loss_G: 1.4537 D(x): 0.6138 D(G(z)): 0.3697 / 0.2730\n",
      "[1/10][21/391] Loss_D: 1.1768 Loss_G: 1.2348 D(x): 0.5805 D(G(z)): 0.4154 / 0.3254\n",
      "[1/10][22/391] Loss_D: 1.0554 Loss_G: 1.6109 D(x): 0.6810 D(G(z)): 0.4451 / 0.2309\n",
      "[1/10][23/391] Loss_D: 0.8044 Loss_G: 1.8468 D(x): 0.7139 D(G(z)): 0.3326 / 0.1808\n",
      "[1/10][24/391] Loss_D: 0.7582 Loss_G: 1.9562 D(x): 0.7119 D(G(z)): 0.3087 / 0.1709\n",
      "[1/10][25/391] Loss_D: 0.8302 Loss_G: 2.6143 D(x): 0.8003 D(G(z)): 0.4094 / 0.1090\n",
      "[1/10][26/391] Loss_D: 1.2825 Loss_G: 1.4529 D(x): 0.5682 D(G(z)): 0.3889 / 0.2922\n",
      "[1/10][27/391] Loss_D: 1.3264 Loss_G: 3.9259 D(x): 0.7984 D(G(z)): 0.5907 / 0.0297\n",
      "[1/10][28/391] Loss_D: 1.5173 Loss_G: 0.7749 D(x): 0.3130 D(G(z)): 0.1268 / 0.5262\n",
      "[1/10][29/391] Loss_D: 1.4583 Loss_G: 3.1850 D(x): 0.8396 D(G(z)): 0.6323 / 0.0639\n",
      "[1/10][30/391] Loss_D: 1.1822 Loss_G: 1.9751 D(x): 0.4997 D(G(z)): 0.1908 / 0.2147\n",
      "[1/10][31/391] Loss_D: 1.1976 Loss_G: 2.3424 D(x): 0.6956 D(G(z)): 0.4474 / 0.1277\n",
      "[1/10][32/391] Loss_D: 1.0220 Loss_G: 2.3706 D(x): 0.6372 D(G(z)): 0.3641 / 0.1205\n",
      "[1/10][33/391] Loss_D: 0.8246 Loss_G: 1.9048 D(x): 0.6400 D(G(z)): 0.2482 / 0.1855\n",
      "[1/10][34/391] Loss_D: 1.3656 Loss_G: 4.7132 D(x): 0.7573 D(G(z)): 0.5943 / 0.0139\n",
      "[1/10][35/391] Loss_D: 3.3937 Loss_G: 0.5331 D(x): 0.0513 D(G(z)): 0.0459 / 0.6170\n",
      "[1/10][36/391] Loss_D: 1.5532 Loss_G: 0.7613 D(x): 0.7761 D(G(z)): 0.6620 / 0.5068\n",
      "[1/10][37/391] Loss_D: 1.1211 Loss_G: 1.5298 D(x): 0.7579 D(G(z)): 0.5377 / 0.2621\n",
      "[1/10][38/391] Loss_D: 1.0033 Loss_G: 1.3907 D(x): 0.6226 D(G(z)): 0.3524 / 0.2863\n",
      "[1/10][39/391] Loss_D: 0.9868 Loss_G: 1.5974 D(x): 0.6869 D(G(z)): 0.4014 / 0.2343\n",
      "[1/10][40/391] Loss_D: 0.8475 Loss_G: 1.5211 D(x): 0.6815 D(G(z)): 0.3084 / 0.2464\n",
      "[1/10][41/391] Loss_D: 1.0255 Loss_G: 1.4072 D(x): 0.6642 D(G(z)): 0.3996 / 0.2660\n",
      "[1/10][42/391] Loss_D: 1.0573 Loss_G: 1.3877 D(x): 0.5920 D(G(z)): 0.3636 / 0.2680\n",
      "[1/10][43/391] Loss_D: 1.0978 Loss_G: 1.8526 D(x): 0.6397 D(G(z)): 0.4391 / 0.1839\n",
      "[1/10][44/391] Loss_D: 1.3041 Loss_G: 0.9095 D(x): 0.4592 D(G(z)): 0.3248 / 0.4371\n",
      "[1/10][45/391] Loss_D: 1.9108 Loss_G: 2.6473 D(x): 0.5856 D(G(z)): 0.6606 / 0.1120\n",
      "[1/10][46/391] Loss_D: 2.2208 Loss_G: 0.6740 D(x): 0.2166 D(G(z)): 0.2446 / 0.5449\n",
      "[1/10][47/391] Loss_D: 2.0188 Loss_G: 1.1566 D(x): 0.5871 D(G(z)): 0.7066 / 0.3454\n",
      "[1/10][48/391] Loss_D: 1.5825 Loss_G: 1.4400 D(x): 0.4549 D(G(z)): 0.4690 / 0.2714\n",
      "[1/10][49/391] Loss_D: 1.3501 Loss_G: 1.0138 D(x): 0.4504 D(G(z)): 0.3578 / 0.3944\n",
      "[1/10][50/391] Loss_D: 1.0334 Loss_G: 1.4935 D(x): 0.6900 D(G(z)): 0.4503 / 0.2636\n",
      "[1/10][51/391] Loss_D: 0.8943 Loss_G: 1.5220 D(x): 0.6688 D(G(z)): 0.3374 / 0.2594\n",
      "[1/10][52/391] Loss_D: 0.7985 Loss_G: 1.9158 D(x): 0.7325 D(G(z)): 0.3318 / 0.1781\n",
      "[1/10][53/391] Loss_D: 0.6570 Loss_G: 1.5908 D(x): 0.7064 D(G(z)): 0.2302 / 0.2345\n",
      "[1/10][54/391] Loss_D: 0.9539 Loss_G: 2.0520 D(x): 0.7914 D(G(z)): 0.4658 / 0.1600\n",
      "[1/10][55/391] Loss_D: 0.9116 Loss_G: 1.6340 D(x): 0.6103 D(G(z)): 0.2789 / 0.2244\n",
      "[1/10][56/391] Loss_D: 0.8597 Loss_G: 1.9515 D(x): 0.7101 D(G(z)): 0.3627 / 0.1850\n",
      "[1/10][57/391] Loss_D: 1.0380 Loss_G: 1.2166 D(x): 0.6015 D(G(z)): 0.3174 / 0.3313\n",
      "[1/10][58/391] Loss_D: 0.9510 Loss_G: 3.1144 D(x): 0.8144 D(G(z)): 0.4899 / 0.0603\n",
      "[1/10][59/391] Loss_D: 1.2494 Loss_G: 0.6879 D(x): 0.3873 D(G(z)): 0.1218 / 0.5453\n",
      "[1/10][60/391] Loss_D: 1.4795 Loss_G: 3.0680 D(x): 0.8597 D(G(z)): 0.6653 / 0.0605\n",
      "[1/10][61/391] Loss_D: 1.3477 Loss_G: 1.0761 D(x): 0.3302 D(G(z)): 0.0865 / 0.3851\n",
      "[1/10][62/391] Loss_D: 1.2387 Loss_G: 2.1331 D(x): 0.7979 D(G(z)): 0.5873 / 0.1551\n",
      "[1/10][63/391] Loss_D: 0.8530 Loss_G: 1.8544 D(x): 0.6195 D(G(z)): 0.2524 / 0.1828\n",
      "[1/10][64/391] Loss_D: 1.6265 Loss_G: 3.0159 D(x): 0.6253 D(G(z)): 0.6185 / 0.0733\n",
      "[1/10][65/391] Loss_D: 1.8347 Loss_G: 1.2860 D(x): 0.3449 D(G(z)): 0.2992 / 0.3214\n",
      "[1/10][66/391] Loss_D: 1.5197 Loss_G: 1.7226 D(x): 0.5404 D(G(z)): 0.5137 / 0.2081\n",
      "[1/10][67/391] Loss_D: 1.1870 Loss_G: 1.8668 D(x): 0.5717 D(G(z)): 0.3874 / 0.1815\n",
      "[1/10][68/391] Loss_D: 1.2506 Loss_G: 1.7188 D(x): 0.5840 D(G(z)): 0.4204 / 0.2029\n",
      "[1/10][69/391] Loss_D: 1.3956 Loss_G: 1.5268 D(x): 0.5776 D(G(z)): 0.4928 / 0.2644\n",
      "[1/10][70/391] Loss_D: 1.3847 Loss_G: 1.7026 D(x): 0.5513 D(G(z)): 0.4805 / 0.2365\n",
      "[1/10][71/391] Loss_D: 1.1230 Loss_G: 1.3196 D(x): 0.5543 D(G(z)): 0.3445 / 0.3118\n",
      "[1/10][72/391] Loss_D: 1.2236 Loss_G: 1.8157 D(x): 0.6594 D(G(z)): 0.4907 / 0.2139\n",
      "[1/10][73/391] Loss_D: 1.0789 Loss_G: 1.0584 D(x): 0.5737 D(G(z)): 0.3261 / 0.3831\n",
      "[1/10][74/391] Loss_D: 1.3149 Loss_G: 2.6960 D(x): 0.7928 D(G(z)): 0.5907 / 0.1242\n",
      "[1/10][75/391] Loss_D: 1.4486 Loss_G: 1.2264 D(x): 0.4198 D(G(z)): 0.1998 / 0.3442\n",
      "[1/10][76/391] Loss_D: 0.9464 Loss_G: 1.3918 D(x): 0.7449 D(G(z)): 0.4084 / 0.2869\n",
      "[1/10][77/391] Loss_D: 0.8130 Loss_G: 2.3078 D(x): 0.8280 D(G(z)): 0.4162 / 0.1260\n",
      "[1/10][78/391] Loss_D: 0.7311 Loss_G: 1.6251 D(x): 0.6426 D(G(z)): 0.1701 / 0.2166\n",
      "[1/10][79/391] Loss_D: 0.6262 Loss_G: 1.8844 D(x): 0.8258 D(G(z)): 0.3246 / 0.1847\n",
      "[1/10][80/391] Loss_D: 0.7357 Loss_G: 2.7170 D(x): 0.7877 D(G(z)): 0.3470 / 0.0867\n",
      "[1/10][81/391] Loss_D: 1.0211 Loss_G: 1.3805 D(x): 0.5674 D(G(z)): 0.2396 / 0.3627\n",
      "[1/10][82/391] Loss_D: 1.0983 Loss_G: 3.8694 D(x): 0.7351 D(G(z)): 0.4256 / 0.0335\n",
      "[1/10][83/391] Loss_D: 0.8573 Loss_G: 3.3245 D(x): 0.6690 D(G(z)): 0.2925 / 0.0586\n",
      "[1/10][84/391] Loss_D: 0.9429 Loss_G: 1.7798 D(x): 0.5324 D(G(z)): 0.1274 / 0.2592\n",
      "[1/10][85/391] Loss_D: 0.8035 Loss_G: 2.9961 D(x): 0.9129 D(G(z)): 0.4115 / 0.0954\n",
      "[1/10][86/391] Loss_D: 0.7413 Loss_G: 1.8291 D(x): 0.7244 D(G(z)): 0.2298 / 0.2028\n",
      "[1/10][87/391] Loss_D: 0.8716 Loss_G: 1.9628 D(x): 0.7645 D(G(z)): 0.3900 / 0.1982\n",
      "[1/10][88/391] Loss_D: 0.8786 Loss_G: 2.8442 D(x): 0.7573 D(G(z)): 0.3925 / 0.0763\n",
      "[1/10][89/391] Loss_D: 1.4536 Loss_G: 0.3693 D(x): 0.3162 D(G(z)): 0.1295 / 0.7084\n",
      "[1/10][90/391] Loss_D: 1.6269 Loss_G: 4.2660 D(x): 0.9843 D(G(z)): 0.7533 / 0.0536\n",
      "[1/10][91/391] Loss_D: 3.2345 Loss_G: 1.6397 D(x): 0.1324 D(G(z)): 0.0571 / 0.2373\n",
      "[1/10][92/391] Loss_D: 1.1104 Loss_G: 0.4688 D(x): 0.4951 D(G(z)): 0.2564 / 0.6618\n",
      "[1/10][93/391] Loss_D: 2.0185 Loss_G: 0.9362 D(x): 0.8568 D(G(z)): 0.6747 / 0.4279\n",
      "[1/10][94/391] Loss_D: 1.1321 Loss_G: 1.6698 D(x): 0.7085 D(G(z)): 0.5115 / 0.2404\n",
      "[1/10][95/391] Loss_D: 1.4427 Loss_G: 0.9745 D(x): 0.3838 D(G(z)): 0.2839 / 0.4351\n",
      "[1/10][96/391] Loss_D: 1.4377 Loss_G: 1.2234 D(x): 0.6819 D(G(z)): 0.5296 / 0.3400\n",
      "[1/10][97/391] Loss_D: 1.2098 Loss_G: 1.1063 D(x): 0.5491 D(G(z)): 0.3906 / 0.3603\n",
      "[1/10][98/391] Loss_D: 1.4755 Loss_G: 0.8143 D(x): 0.4976 D(G(z)): 0.4916 / 0.4794\n",
      "[1/10][99/391] Loss_D: 1.5432 Loss_G: 1.4142 D(x): 0.5858 D(G(z)): 0.5682 / 0.2821\n",
      "[1/10][100/391] Loss_D: 1.5935 Loss_G: 0.9538 D(x): 0.3639 D(G(z)): 0.3669 / 0.4158\n",
      "[1/10][101/391] Loss_D: 1.5172 Loss_G: 0.9404 D(x): 0.4740 D(G(z)): 0.4821 / 0.4122\n",
      "[1/10][102/391] Loss_D: 1.3782 Loss_G: 1.4283 D(x): 0.5431 D(G(z)): 0.5048 / 0.2749\n",
      "[1/10][103/391] Loss_D: 1.2324 Loss_G: 1.3356 D(x): 0.4940 D(G(z)): 0.3377 / 0.2886\n",
      "[1/10][104/391] Loss_D: 1.1048 Loss_G: 1.2496 D(x): 0.5555 D(G(z)): 0.3658 / 0.3175\n",
      "[1/10][105/391] Loss_D: 0.9750 Loss_G: 1.7333 D(x): 0.6715 D(G(z)): 0.4057 / 0.2143\n",
      "[1/10][106/391] Loss_D: 0.9699 Loss_G: 1.4743 D(x): 0.6013 D(G(z)): 0.3133 / 0.2668\n",
      "[1/10][107/391] Loss_D: 1.0028 Loss_G: 1.6872 D(x): 0.6476 D(G(z)): 0.3889 / 0.2135\n",
      "[1/10][108/391] Loss_D: 1.2235 Loss_G: 0.8494 D(x): 0.5197 D(G(z)): 0.3726 / 0.4526\n",
      "[1/10][109/391] Loss_D: 1.0905 Loss_G: 2.3649 D(x): 0.7742 D(G(z)): 0.5376 / 0.1299\n",
      "[1/10][110/391] Loss_D: 1.5051 Loss_G: 0.4638 D(x): 0.3347 D(G(z)): 0.2105 / 0.6486\n",
      "[1/10][111/391] Loss_D: 1.4473 Loss_G: 1.6891 D(x): 0.8509 D(G(z)): 0.6887 / 0.2666\n",
      "[1/10][112/391] Loss_D: 1.1205 Loss_G: 1.3242 D(x): 0.5656 D(G(z)): 0.2854 / 0.3083\n",
      "[1/10][113/391] Loss_D: 0.9298 Loss_G: 1.5002 D(x): 0.6718 D(G(z)): 0.3466 / 0.2509\n",
      "[1/10][114/391] Loss_D: 0.7204 Loss_G: 1.6350 D(x): 0.7515 D(G(z)): 0.3149 / 0.2215\n",
      "[1/10][115/391] Loss_D: 0.6144 Loss_G: 1.8658 D(x): 0.7721 D(G(z)): 0.2556 / 0.1774\n",
      "[1/10][116/391] Loss_D: 0.5840 Loss_G: 2.0244 D(x): 0.7598 D(G(z)): 0.2422 / 0.1624\n",
      "[1/10][117/391] Loss_D: 0.5815 Loss_G: 2.7531 D(x): 0.8298 D(G(z)): 0.2961 / 0.0847\n",
      "[1/10][118/391] Loss_D: 0.6522 Loss_G: 1.4066 D(x): 0.6562 D(G(z)): 0.1561 / 0.2818\n",
      "[1/10][119/391] Loss_D: 1.1486 Loss_G: 4.8956 D(x): 0.8657 D(G(z)): 0.5703 / 0.0173\n",
      "[1/10][120/391] Loss_D: 2.0586 Loss_G: 0.6368 D(x): 0.2105 D(G(z)): 0.0276 / 0.5633\n",
      "[1/10][121/391] Loss_D: 1.5145 Loss_G: 2.5609 D(x): 0.9130 D(G(z)): 0.7021 / 0.1215\n",
      "[1/10][122/391] Loss_D: 0.7873 Loss_G: 2.4029 D(x): 0.6108 D(G(z)): 0.1842 / 0.1294\n",
      "[1/10][123/391] Loss_D: 0.6967 Loss_G: 2.0752 D(x): 0.7491 D(G(z)): 0.2924 / 0.1679\n",
      "[1/10][124/391] Loss_D: 0.9812 Loss_G: 2.6224 D(x): 0.8062 D(G(z)): 0.4864 / 0.0910\n",
      "[1/10][125/391] Loss_D: 1.6017 Loss_G: 1.6225 D(x): 0.4346 D(G(z)): 0.4448 / 0.2551\n",
      "[1/10][126/391] Loss_D: 2.2021 Loss_G: 2.1323 D(x): 0.4664 D(G(z)): 0.6614 / 0.1524\n",
      "[1/10][127/391] Loss_D: 2.0214 Loss_G: 1.0236 D(x): 0.2588 D(G(z)): 0.3172 / 0.4368\n",
      "[1/10][128/391] Loss_D: 1.5851 Loss_G: 2.3614 D(x): 0.6313 D(G(z)): 0.5759 / 0.1769\n",
      "[1/10][129/391] Loss_D: 1.4061 Loss_G: 1.8339 D(x): 0.4651 D(G(z)): 0.2784 / 0.1946\n",
      "[1/10][130/391] Loss_D: 0.9450 Loss_G: 1.9143 D(x): 0.6614 D(G(z)): 0.3461 / 0.1913\n",
      "[1/10][131/391] Loss_D: 0.9705 Loss_G: 2.8730 D(x): 0.7094 D(G(z)): 0.3968 / 0.1024\n",
      "[1/10][132/391] Loss_D: 1.2528 Loss_G: 1.3315 D(x): 0.5190 D(G(z)): 0.2871 / 0.3494\n",
      "[1/10][133/391] Loss_D: 1.6818 Loss_G: 2.1389 D(x): 0.6770 D(G(z)): 0.5875 / 0.1942\n",
      "[1/10][134/391] Loss_D: 1.3816 Loss_G: 1.7064 D(x): 0.4891 D(G(z)): 0.3653 / 0.2287\n",
      "[1/10][135/391] Loss_D: 1.3929 Loss_G: 1.6955 D(x): 0.5459 D(G(z)): 0.4520 / 0.2605\n",
      "[1/10][136/391] Loss_D: 1.1461 Loss_G: 2.3167 D(x): 0.7170 D(G(z)): 0.4522 / 0.1540\n",
      "[1/10][137/391] Loss_D: 1.2388 Loss_G: 1.8165 D(x): 0.5871 D(G(z)): 0.3483 / 0.2500\n",
      "[1/10][138/391] Loss_D: 1.5109 Loss_G: 1.9713 D(x): 0.6536 D(G(z)): 0.5202 / 0.1993\n",
      "[1/10][139/391] Loss_D: 1.7585 Loss_G: 1.6189 D(x): 0.4954 D(G(z)): 0.5241 / 0.2422\n",
      "[1/10][140/391] Loss_D: 1.9600 Loss_G: 1.6865 D(x): 0.4010 D(G(z)): 0.5281 / 0.2179\n",
      "[1/10][141/391] Loss_D: 1.7221 Loss_G: 1.7696 D(x): 0.3634 D(G(z)): 0.3985 / 0.1997\n",
      "[1/10][142/391] Loss_D: 1.2996 Loss_G: 2.2437 D(x): 0.5063 D(G(z)): 0.4016 / 0.1366\n",
      "[1/10][143/391] Loss_D: 1.0918 Loss_G: 1.5913 D(x): 0.5427 D(G(z)): 0.3003 / 0.2409\n",
      "[1/10][144/391] Loss_D: 1.1362 Loss_G: 1.6639 D(x): 0.6542 D(G(z)): 0.4467 / 0.2295\n",
      "[1/10][145/391] Loss_D: 1.2881 Loss_G: 1.7556 D(x): 0.6288 D(G(z)): 0.5032 / 0.2413\n",
      "[1/10][146/391] Loss_D: 1.2231 Loss_G: 1.0592 D(x): 0.5367 D(G(z)): 0.3406 / 0.3728\n",
      "[1/10][147/391] Loss_D: 1.1839 Loss_G: 1.6050 D(x): 0.6490 D(G(z)): 0.4858 / 0.2367\n",
      "[1/10][148/391] Loss_D: 0.8838 Loss_G: 1.5062 D(x): 0.6504 D(G(z)): 0.2965 / 0.2482\n",
      "[1/10][149/391] Loss_D: 0.6875 Loss_G: 2.1153 D(x): 0.7995 D(G(z)): 0.3455 / 0.1409\n",
      "[1/10][150/391] Loss_D: 0.6322 Loss_G: 1.9328 D(x): 0.7125 D(G(z)): 0.2158 / 0.1694\n",
      "[1/10][151/391] Loss_D: 0.6509 Loss_G: 1.5984 D(x): 0.7242 D(G(z)): 0.2326 / 0.2280\n",
      "[1/10][152/391] Loss_D: 0.7387 Loss_G: 2.8722 D(x): 0.8152 D(G(z)): 0.3687 / 0.0829\n",
      "[1/10][153/391] Loss_D: 0.6513 Loss_G: 1.6795 D(x): 0.6830 D(G(z)): 0.1942 / 0.2086\n",
      "[1/10][154/391] Loss_D: 0.9017 Loss_G: 2.3238 D(x): 0.7503 D(G(z)): 0.4137 / 0.1240\n",
      "[1/10][155/391] Loss_D: 1.0138 Loss_G: 0.4998 D(x): 0.5113 D(G(z)): 0.2039 / 0.6427\n",
      "[1/10][156/391] Loss_D: 2.6191 Loss_G: 5.9226 D(x): 0.9564 D(G(z)): 0.8832 / 0.0049\n",
      "[1/10][157/391] Loss_D: 3.8661 Loss_G: 1.1172 D(x): 0.0375 D(G(z)): 0.0117 / 0.3678\n",
      "[1/10][158/391] Loss_D: 1.1799 Loss_G: 0.7061 D(x): 0.5988 D(G(z)): 0.4286 / 0.5358\n",
      "[1/10][159/391] Loss_D: 1.2948 Loss_G: 2.0103 D(x): 0.8111 D(G(z)): 0.6228 / 0.1548\n",
      "[1/10][160/391] Loss_D: 0.9689 Loss_G: 1.7151 D(x): 0.5072 D(G(z)): 0.2079 / 0.2025\n",
      "[1/10][161/391] Loss_D: 1.0136 Loss_G: 1.4525 D(x): 0.6296 D(G(z)): 0.3870 / 0.2732\n",
      "[1/10][162/391] Loss_D: 1.0000 Loss_G: 2.5947 D(x): 0.7264 D(G(z)): 0.4581 / 0.0927\n",
      "[1/10][163/391] Loss_D: 1.4056 Loss_G: 0.8915 D(x): 0.3292 D(G(z)): 0.1722 / 0.4318\n",
      "[1/10][164/391] Loss_D: 1.6611 Loss_G: 1.9307 D(x): 0.7505 D(G(z)): 0.7139 / 0.2155\n",
      "[1/10][165/391] Loss_D: 1.5072 Loss_G: 1.3303 D(x): 0.4655 D(G(z)): 0.4085 / 0.2815\n",
      "[1/10][166/391] Loss_D: 1.4803 Loss_G: 1.2052 D(x): 0.4795 D(G(z)): 0.4698 / 0.3210\n",
      "[1/10][167/391] Loss_D: 1.4794 Loss_G: 1.5391 D(x): 0.5140 D(G(z)): 0.5065 / 0.2599\n",
      "[1/10][168/391] Loss_D: 1.4196 Loss_G: 0.9933 D(x): 0.4597 D(G(z)): 0.3878 / 0.4078\n",
      "[1/10][169/391] Loss_D: 1.4000 Loss_G: 1.8268 D(x): 0.6649 D(G(z)): 0.5281 / 0.2082\n",
      "[1/10][170/391] Loss_D: 1.3285 Loss_G: 1.1404 D(x): 0.4789 D(G(z)): 0.3336 / 0.3483\n",
      "[1/10][171/391] Loss_D: 1.1172 Loss_G: 1.2228 D(x): 0.6427 D(G(z)): 0.4503 / 0.3165\n",
      "[1/10][172/391] Loss_D: 1.1143 Loss_G: 1.5895 D(x): 0.6676 D(G(z)): 0.4801 / 0.2355\n",
      "[1/10][173/391] Loss_D: 1.0802 Loss_G: 1.1206 D(x): 0.5281 D(G(z)): 0.3025 / 0.3648\n",
      "[1/10][174/391] Loss_D: 0.9426 Loss_G: 1.7065 D(x): 0.7406 D(G(z)): 0.4399 / 0.2077\n",
      "[1/10][175/391] Loss_D: 1.0388 Loss_G: 1.1069 D(x): 0.5508 D(G(z)): 0.2840 / 0.3571\n",
      "[1/10][176/391] Loss_D: 0.8580 Loss_G: 1.9649 D(x): 0.8218 D(G(z)): 0.4377 / 0.1822\n",
      "[1/10][177/391] Loss_D: 0.9432 Loss_G: 1.4178 D(x): 0.6544 D(G(z)): 0.3633 / 0.2689\n",
      "[1/10][178/391] Loss_D: 0.8219 Loss_G: 1.7762 D(x): 0.7668 D(G(z)): 0.3904 / 0.2010\n",
      "[1/10][179/391] Loss_D: 0.9409 Loss_G: 1.2622 D(x): 0.5922 D(G(z)): 0.2802 / 0.3036\n",
      "[1/10][180/391] Loss_D: 1.2037 Loss_G: 1.9612 D(x): 0.6722 D(G(z)): 0.5151 / 0.1659\n",
      "[1/10][181/391] Loss_D: 1.1872 Loss_G: 1.2426 D(x): 0.5407 D(G(z)): 0.3627 / 0.3202\n",
      "[1/10][182/391] Loss_D: 1.6733 Loss_G: 3.6927 D(x): 0.6181 D(G(z)): 0.6415 / 0.0330\n",
      "[1/10][183/391] Loss_D: 1.9317 Loss_G: 0.5498 D(x): 0.1884 D(G(z)): 0.0760 / 0.6179\n",
      "[1/10][184/391] Loss_D: 1.9161 Loss_G: 2.3473 D(x): 0.8261 D(G(z)): 0.7717 / 0.1246\n",
      "[1/10][185/391] Loss_D: 1.4222 Loss_G: 1.7263 D(x): 0.3956 D(G(z)): 0.2696 / 0.2165\n",
      "[1/10][186/391] Loss_D: 1.2791 Loss_G: 1.3349 D(x): 0.5026 D(G(z)): 0.3538 / 0.2988\n",
      "[1/10][187/391] Loss_D: 1.2090 Loss_G: 1.8767 D(x): 0.6244 D(G(z)): 0.4687 / 0.1820\n",
      "[1/10][188/391] Loss_D: 1.1189 Loss_G: 1.9526 D(x): 0.5646 D(G(z)): 0.3577 / 0.1746\n",
      "[1/10][189/391] Loss_D: 1.0331 Loss_G: 1.6411 D(x): 0.5588 D(G(z)): 0.3052 / 0.2273\n",
      "[1/10][190/391] Loss_D: 1.0768 Loss_G: 2.0746 D(x): 0.7208 D(G(z)): 0.4926 / 0.1459\n",
      "[1/10][191/391] Loss_D: 1.2799 Loss_G: 1.3589 D(x): 0.4862 D(G(z)): 0.3500 / 0.2992\n",
      "[1/10][192/391] Loss_D: 1.1398 Loss_G: 2.1616 D(x): 0.6894 D(G(z)): 0.4856 / 0.1698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][193/391] Loss_D: 1.0502 Loss_G: 1.4160 D(x): 0.5719 D(G(z)): 0.2938 / 0.2766\n",
      "[1/10][194/391] Loss_D: 1.0954 Loss_G: 2.2249 D(x): 0.7799 D(G(z)): 0.5318 / 0.1552\n",
      "[1/10][195/391] Loss_D: 0.9798 Loss_G: 1.5046 D(x): 0.5940 D(G(z)): 0.2758 / 0.2550\n",
      "[1/10][196/391] Loss_D: 0.7457 Loss_G: 2.3912 D(x): 0.8256 D(G(z)): 0.3888 / 0.1356\n",
      "[1/10][197/391] Loss_D: 0.6587 Loss_G: 2.1733 D(x): 0.7556 D(G(z)): 0.2576 / 0.1328\n",
      "[1/10][198/391] Loss_D: 0.9343 Loss_G: 2.6485 D(x): 0.7672 D(G(z)): 0.4457 / 0.0931\n",
      "[1/10][199/391] Loss_D: 0.7090 Loss_G: 2.2988 D(x): 0.6605 D(G(z)): 0.1952 / 0.1247\n",
      "[1/10][200/391] Loss_D: 0.5078 Loss_G: 2.4673 D(x): 0.8087 D(G(z)): 0.2178 / 0.1112\n",
      "[1/10][201/391] Loss_D: 0.6167 Loss_G: 4.5864 D(x): 0.8929 D(G(z)): 0.3617 / 0.0161\n",
      "[1/10][202/391] Loss_D: 0.8269 Loss_G: 1.9370 D(x): 0.5344 D(G(z)): 0.0306 / 0.2018\n",
      "[1/10][203/391] Loss_D: 1.2313 Loss_G: 4.8563 D(x): 0.9343 D(G(z)): 0.6353 / 0.0112\n",
      "[1/10][204/391] Loss_D: 1.8945 Loss_G: 0.6802 D(x): 0.2498 D(G(z)): 0.0501 / 0.5481\n",
      "[1/10][205/391] Loss_D: 2.0184 Loss_G: 3.0854 D(x): 0.8885 D(G(z)): 0.8014 / 0.0732\n",
      "[1/10][206/391] Loss_D: 1.4574 Loss_G: 1.5514 D(x): 0.3724 D(G(z)): 0.2253 / 0.2418\n",
      "[1/10][207/391] Loss_D: 1.0001 Loss_G: 1.5507 D(x): 0.6532 D(G(z)): 0.3822 / 0.2406\n",
      "[1/10][208/391] Loss_D: 1.2957 Loss_G: 2.2546 D(x): 0.6135 D(G(z)): 0.4700 / 0.1868\n",
      "[1/10][209/391] Loss_D: 1.4093 Loss_G: 1.5967 D(x): 0.4943 D(G(z)): 0.3057 / 0.2660\n",
      "[1/10][210/391] Loss_D: 1.4426 Loss_G: 2.7449 D(x): 0.6475 D(G(z)): 0.5300 / 0.1154\n",
      "[1/10][211/391] Loss_D: 1.1885 Loss_G: 2.3291 D(x): 0.4845 D(G(z)): 0.2319 / 0.1217\n",
      "[1/10][212/391] Loss_D: 1.3850 Loss_G: 3.1833 D(x): 0.7016 D(G(z)): 0.5753 / 0.0643\n",
      "[1/10][213/391] Loss_D: 1.4221 Loss_G: 0.6077 D(x): 0.3480 D(G(z)): 0.1543 / 0.5672\n",
      "[1/10][214/391] Loss_D: 1.8294 Loss_G: 4.4320 D(x): 0.8441 D(G(z)): 0.7854 / 0.0151\n",
      "[1/10][215/391] Loss_D: 2.1722 Loss_G: 0.8524 D(x): 0.1469 D(G(z)): 0.0490 / 0.4530\n",
      "[1/10][216/391] Loss_D: 1.2290 Loss_G: 1.2116 D(x): 0.7575 D(G(z)): 0.5746 / 0.3689\n",
      "[1/10][217/391] Loss_D: 1.2202 Loss_G: 1.2937 D(x): 0.6503 D(G(z)): 0.4852 / 0.3012\n",
      "[1/10][218/391] Loss_D: 1.0765 Loss_G: 1.1626 D(x): 0.5654 D(G(z)): 0.3492 / 0.3299\n",
      "[1/10][219/391] Loss_D: 1.0239 Loss_G: 1.3777 D(x): 0.6451 D(G(z)): 0.4033 / 0.3234\n",
      "[1/10][220/391] Loss_D: 0.9791 Loss_G: 1.0094 D(x): 0.6408 D(G(z)): 0.3334 / 0.4029\n",
      "[1/10][221/391] Loss_D: 0.9810 Loss_G: 1.9505 D(x): 0.7880 D(G(z)): 0.4570 / 0.1971\n",
      "[1/10][222/391] Loss_D: 0.9852 Loss_G: 1.1073 D(x): 0.5613 D(G(z)): 0.2367 / 0.3592\n",
      "[1/10][223/391] Loss_D: 1.0053 Loss_G: 1.9378 D(x): 0.7960 D(G(z)): 0.4944 / 0.1996\n",
      "[1/10][224/391] Loss_D: 1.0650 Loss_G: 1.0998 D(x): 0.5816 D(G(z)): 0.3121 / 0.3669\n",
      "[1/10][225/391] Loss_D: 0.9768 Loss_G: 2.1004 D(x): 0.7733 D(G(z)): 0.4722 / 0.1431\n",
      "[1/10][226/391] Loss_D: 1.0330 Loss_G: 1.0321 D(x): 0.4877 D(G(z)): 0.2060 / 0.3891\n",
      "[1/10][227/391] Loss_D: 1.1251 Loss_G: 3.6929 D(x): 0.8550 D(G(z)): 0.5933 / 0.0322\n",
      "[1/10][228/391] Loss_D: 1.4708 Loss_G: 0.9689 D(x): 0.2993 D(G(z)): 0.0637 / 0.4214\n",
      "[1/10][229/391] Loss_D: 1.2218 Loss_G: 2.3128 D(x): 0.7853 D(G(z)): 0.5800 / 0.1328\n",
      "[1/10][230/391] Loss_D: 1.1083 Loss_G: 1.3364 D(x): 0.5178 D(G(z)): 0.2670 / 0.3056\n",
      "[1/10][231/391] Loss_D: 1.1445 Loss_G: 2.0195 D(x): 0.6547 D(G(z)): 0.4495 / 0.1805\n",
      "[1/10][232/391] Loss_D: 1.3598 Loss_G: 1.3014 D(x): 0.5099 D(G(z)): 0.3833 / 0.3006\n",
      "[1/10][233/391] Loss_D: 1.2221 Loss_G: 2.7122 D(x): 0.6366 D(G(z)): 0.4722 / 0.0873\n",
      "[1/10][234/391] Loss_D: 1.0554 Loss_G: 1.0808 D(x): 0.4452 D(G(z)): 0.1495 / 0.3830\n",
      "[1/10][235/391] Loss_D: 1.7294 Loss_G: 5.0350 D(x): 0.8623 D(G(z)): 0.7593 / 0.0211\n",
      "[1/10][236/391] Loss_D: 2.3536 Loss_G: 0.8754 D(x): 0.1742 D(G(z)): 0.1226 / 0.4722\n",
      "[1/10][237/391] Loss_D: 1.2725 Loss_G: 1.6453 D(x): 0.7071 D(G(z)): 0.5412 / 0.2371\n",
      "[1/10][238/391] Loss_D: 1.1997 Loss_G: 1.8309 D(x): 0.6222 D(G(z)): 0.4515 / 0.2059\n",
      "[1/10][239/391] Loss_D: 1.4573 Loss_G: 1.6002 D(x): 0.5384 D(G(z)): 0.4976 / 0.2348\n",
      "[1/10][240/391] Loss_D: 1.3736 Loss_G: 1.5448 D(x): 0.5323 D(G(z)): 0.4631 / 0.2494\n",
      "[1/10][241/391] Loss_D: 1.4463 Loss_G: 1.1350 D(x): 0.4787 D(G(z)): 0.4433 / 0.3551\n",
      "[1/10][242/391] Loss_D: 1.3229 Loss_G: 1.8705 D(x): 0.6668 D(G(z)): 0.5517 / 0.1806\n",
      "[1/10][243/391] Loss_D: 1.2497 Loss_G: 1.6691 D(x): 0.5732 D(G(z)): 0.4158 / 0.2337\n",
      "[1/10][244/391] Loss_D: 1.2685 Loss_G: 1.7092 D(x): 0.5730 D(G(z)): 0.4096 / 0.2190\n",
      "[1/10][245/391] Loss_D: 1.0935 Loss_G: 1.7445 D(x): 0.5794 D(G(z)): 0.3493 / 0.2059\n",
      "[1/10][246/391] Loss_D: 1.0884 Loss_G: 1.7193 D(x): 0.6362 D(G(z)): 0.4313 / 0.2158\n",
      "[1/10][247/391] Loss_D: 1.0778 Loss_G: 1.8803 D(x): 0.6396 D(G(z)): 0.4054 / 0.2038\n",
      "[1/10][248/391] Loss_D: 0.9699 Loss_G: 1.4886 D(x): 0.6445 D(G(z)): 0.3226 / 0.2769\n",
      "[1/10][249/391] Loss_D: 1.3235 Loss_G: 2.7322 D(x): 0.6893 D(G(z)): 0.5456 / 0.1131\n",
      "[1/10][250/391] Loss_D: 1.7173 Loss_G: 0.6362 D(x): 0.3245 D(G(z)): 0.2468 / 0.5542\n",
      "[1/10][251/391] Loss_D: 1.8055 Loss_G: 2.5757 D(x): 0.7918 D(G(z)): 0.7593 / 0.1052\n",
      "[1/10][252/391] Loss_D: 2.1090 Loss_G: 0.9628 D(x): 0.1850 D(G(z)): 0.1728 / 0.4375\n",
      "[1/10][253/391] Loss_D: 1.3943 Loss_G: 1.7298 D(x): 0.7146 D(G(z)): 0.5660 / 0.2186\n",
      "[1/10][254/391] Loss_D: 1.1729 Loss_G: 2.0738 D(x): 0.5998 D(G(z)): 0.4072 / 0.1561\n",
      "[1/10][255/391] Loss_D: 1.1553 Loss_G: 1.1846 D(x): 0.4780 D(G(z)): 0.2416 / 0.3494\n",
      "[1/10][256/391] Loss_D: 1.1267 Loss_G: 2.2274 D(x): 0.7544 D(G(z)): 0.5318 / 0.1251\n",
      "[1/10][257/391] Loss_D: 1.0386 Loss_G: 1.4017 D(x): 0.5104 D(G(z)): 0.2554 / 0.2703\n",
      "[1/10][258/391] Loss_D: 1.1436 Loss_G: 1.4677 D(x): 0.6407 D(G(z)): 0.4658 / 0.2508\n",
      "[1/10][259/391] Loss_D: 1.0276 Loss_G: 1.4799 D(x): 0.6049 D(G(z)): 0.3641 / 0.2420\n",
      "[1/10][260/391] Loss_D: 1.3333 Loss_G: 2.6079 D(x): 0.7284 D(G(z)): 0.5912 / 0.1171\n",
      "[1/10][261/391] Loss_D: 1.9268 Loss_G: 0.8701 D(x): 0.2570 D(G(z)): 0.2032 / 0.4400\n",
      "[1/10][262/391] Loss_D: 0.9804 Loss_G: 1.6348 D(x): 0.8319 D(G(z)): 0.5118 / 0.2209\n",
      "[1/10][263/391] Loss_D: 0.9466 Loss_G: 2.1106 D(x): 0.7356 D(G(z)): 0.4367 / 0.1464\n",
      "[1/10][264/391] Loss_D: 1.2294 Loss_G: 1.3909 D(x): 0.5696 D(G(z)): 0.4180 / 0.2748\n",
      "[1/10][265/391] Loss_D: 1.2176 Loss_G: 2.9145 D(x): 0.7643 D(G(z)): 0.5175 / 0.0699\n",
      "[1/10][266/391] Loss_D: 1.7101 Loss_G: 0.9125 D(x): 0.2573 D(G(z)): 0.1369 / 0.4411\n",
      "[1/10][267/391] Loss_D: 1.7988 Loss_G: 3.0937 D(x): 0.8917 D(G(z)): 0.7695 / 0.0564\n",
      "[1/10][268/391] Loss_D: 2.0835 Loss_G: 1.3867 D(x): 0.1738 D(G(z)): 0.0825 / 0.2996\n",
      "[1/10][269/391] Loss_D: 0.8064 Loss_G: 1.5531 D(x): 0.7714 D(G(z)): 0.3561 / 0.2633\n",
      "[1/10][270/391] Loss_D: 1.1412 Loss_G: 2.5534 D(x): 0.8387 D(G(z)): 0.5632 / 0.1133\n",
      "[1/10][271/391] Loss_D: 1.0190 Loss_G: 1.6601 D(x): 0.4723 D(G(z)): 0.1473 / 0.2300\n",
      "[1/10][272/391] Loss_D: 0.5520 Loss_G: 1.9875 D(x): 0.8280 D(G(z)): 0.2774 / 0.1562\n",
      "[1/10][273/391] Loss_D: 0.5133 Loss_G: 2.8753 D(x): 0.8899 D(G(z)): 0.3151 / 0.0707\n",
      "[1/10][274/391] Loss_D: 0.4791 Loss_G: 2.8744 D(x): 0.7791 D(G(z)): 0.1857 / 0.0715\n",
      "[1/10][275/391] Loss_D: 0.3172 Loss_G: 2.7838 D(x): 0.8086 D(G(z)): 0.0855 / 0.0889\n",
      "[1/10][276/391] Loss_D: 0.7453 Loss_G: 3.6993 D(x): 0.8941 D(G(z)): 0.4170 / 0.0368\n",
      "[1/10][277/391] Loss_D: 1.0619 Loss_G: 3.0083 D(x): 0.4174 D(G(z)): 0.0917 / 0.0691\n",
      "[1/10][278/391] Loss_D: 1.7443 Loss_G: 0.2977 D(x): 0.2452 D(G(z)): 0.1061 / 0.7619\n",
      "[1/10][279/391] Loss_D: 2.8449 Loss_G: 1.4731 D(x): 0.9232 D(G(z)): 0.8986 / 0.2901\n",
      "[1/10][280/391] Loss_D: 1.3815 Loss_G: 1.8633 D(x): 0.4968 D(G(z)): 0.3942 / 0.1975\n",
      "[1/10][281/391] Loss_D: 1.4109 Loss_G: 0.9330 D(x): 0.3694 D(G(z)): 0.2485 / 0.4140\n",
      "[1/10][282/391] Loss_D: 1.2480 Loss_G: 0.8142 D(x): 0.6210 D(G(z)): 0.5061 / 0.4656\n",
      "[1/10][283/391] Loss_D: 1.4120 Loss_G: 1.4679 D(x): 0.6484 D(G(z)): 0.5866 / 0.2575\n",
      "[1/10][284/391] Loss_D: 1.2724 Loss_G: 1.4142 D(x): 0.4834 D(G(z)): 0.3675 / 0.2745\n",
      "[1/10][285/391] Loss_D: 1.3069 Loss_G: 1.0165 D(x): 0.4752 D(G(z)): 0.3811 / 0.3866\n",
      "[1/10][286/391] Loss_D: 1.2962 Loss_G: 1.1310 D(x): 0.5816 D(G(z)): 0.4916 / 0.3499\n",
      "[1/10][287/391] Loss_D: 1.3007 Loss_G: 1.1155 D(x): 0.5875 D(G(z)): 0.5011 / 0.3620\n",
      "[1/10][288/391] Loss_D: 1.2883 Loss_G: 1.1927 D(x): 0.5905 D(G(z)): 0.4860 / 0.3294\n",
      "[1/10][289/391] Loss_D: 1.2246 Loss_G: 1.0506 D(x): 0.5511 D(G(z)): 0.4249 / 0.3706\n",
      "[1/10][290/391] Loss_D: 1.0984 Loss_G: 1.3009 D(x): 0.6151 D(G(z)): 0.4234 / 0.3334\n",
      "[1/10][291/391] Loss_D: 1.0585 Loss_G: 1.0464 D(x): 0.5999 D(G(z)): 0.3766 / 0.3663\n",
      "[1/10][292/391] Loss_D: 1.1396 Loss_G: 1.3145 D(x): 0.6637 D(G(z)): 0.4682 / 0.2946\n",
      "[1/10][293/391] Loss_D: 1.3451 Loss_G: 1.3559 D(x): 0.5334 D(G(z)): 0.4594 / 0.2918\n",
      "[1/10][294/391] Loss_D: 1.3494 Loss_G: 1.6511 D(x): 0.5691 D(G(z)): 0.4915 / 0.2150\n",
      "[1/10][295/391] Loss_D: 1.4865 Loss_G: 1.8411 D(x): 0.4761 D(G(z)): 0.4630 / 0.1959\n",
      "[1/10][296/391] Loss_D: 1.3696 Loss_G: 1.6634 D(x): 0.4766 D(G(z)): 0.3640 / 0.2204\n",
      "[1/10][297/391] Loss_D: 1.1660 Loss_G: 1.6248 D(x): 0.5549 D(G(z)): 0.3765 / 0.2196\n",
      "[1/10][298/391] Loss_D: 1.3248 Loss_G: 1.4483 D(x): 0.6395 D(G(z)): 0.5334 / 0.2631\n",
      "[1/10][299/391] Loss_D: 1.3200 Loss_G: 1.2370 D(x): 0.5041 D(G(z)): 0.4001 / 0.3247\n",
      "[1/10][300/391] Loss_D: 1.1590 Loss_G: 1.3986 D(x): 0.5915 D(G(z)): 0.3979 / 0.2826\n",
      "[1/10][301/391] Loss_D: 1.1903 Loss_G: 1.0428 D(x): 0.5889 D(G(z)): 0.4081 / 0.3902\n",
      "[1/10][302/391] Loss_D: 1.4480 Loss_G: 1.6108 D(x): 0.5840 D(G(z)): 0.4784 / 0.2915\n",
      "[1/10][303/391] Loss_D: 1.6558 Loss_G: 1.0034 D(x): 0.4698 D(G(z)): 0.4340 / 0.3973\n",
      "[1/10][304/391] Loss_D: 1.3225 Loss_G: 2.4319 D(x): 0.7256 D(G(z)): 0.5534 / 0.1363\n",
      "[1/10][305/391] Loss_D: 1.7882 Loss_G: 0.4321 D(x): 0.2785 D(G(z)): 0.1800 / 0.6640\n",
      "[1/10][306/391] Loss_D: 1.8662 Loss_G: 1.8601 D(x): 0.8078 D(G(z)): 0.7741 / 0.1899\n",
      "[1/10][307/391] Loss_D: 1.4039 Loss_G: 1.5875 D(x): 0.4305 D(G(z)): 0.3574 / 0.2375\n",
      "[1/10][308/391] Loss_D: 1.4190 Loss_G: 0.9561 D(x): 0.4337 D(G(z)): 0.3557 / 0.4106\n",
      "[1/10][309/391] Loss_D: 1.5763 Loss_G: 0.9849 D(x): 0.5685 D(G(z)): 0.5781 / 0.4036\n",
      "[1/10][310/391] Loss_D: 1.4367 Loss_G: 1.1000 D(x): 0.5035 D(G(z)): 0.4586 / 0.3858\n",
      "[1/10][311/391] Loss_D: 1.3528 Loss_G: 1.3952 D(x): 0.5407 D(G(z)): 0.4340 / 0.3171\n",
      "[1/10][312/391] Loss_D: 1.2196 Loss_G: 1.0885 D(x): 0.5678 D(G(z)): 0.3875 / 0.3647\n",
      "[1/10][313/391] Loss_D: 1.1501 Loss_G: 1.1836 D(x): 0.6304 D(G(z)): 0.4532 / 0.3340\n",
      "[1/10][314/391] Loss_D: 1.1557 Loss_G: 1.2095 D(x): 0.6159 D(G(z)): 0.4423 / 0.3210\n",
      "[1/10][315/391] Loss_D: 1.2309 Loss_G: 1.0939 D(x): 0.5521 D(G(z)): 0.4166 / 0.3596\n",
      "[1/10][316/391] Loss_D: 1.1096 Loss_G: 1.3360 D(x): 0.6105 D(G(z)): 0.4203 / 0.2892\n",
      "[1/10][317/391] Loss_D: 1.3375 Loss_G: 1.3066 D(x): 0.5575 D(G(z)): 0.4710 / 0.3006\n",
      "[1/10][318/391] Loss_D: 1.1622 Loss_G: 0.8167 D(x): 0.5078 D(G(z)): 0.3323 / 0.4609\n",
      "[1/10][319/391] Loss_D: 1.3833 Loss_G: 1.9613 D(x): 0.6875 D(G(z)): 0.5906 / 0.1927\n",
      "[1/10][320/391] Loss_D: 1.4699 Loss_G: 0.6467 D(x): 0.3836 D(G(z)): 0.2508 / 0.5597\n",
      "[1/10][321/391] Loss_D: 1.6228 Loss_G: 1.6660 D(x): 0.7740 D(G(z)): 0.6850 / 0.2267\n",
      "[1/10][322/391] Loss_D: 1.3293 Loss_G: 1.4439 D(x): 0.4318 D(G(z)): 0.2850 / 0.2840\n",
      "[1/10][323/391] Loss_D: 1.0878 Loss_G: 1.3335 D(x): 0.6059 D(G(z)): 0.3770 / 0.2918\n",
      "[1/10][324/391] Loss_D: 1.0214 Loss_G: 1.6615 D(x): 0.6699 D(G(z)): 0.4300 / 0.2266\n",
      "[1/10][325/391] Loss_D: 1.3289 Loss_G: 1.2382 D(x): 0.5358 D(G(z)): 0.4158 / 0.3515\n",
      "[1/10][326/391] Loss_D: 1.4041 Loss_G: 1.5755 D(x): 0.5492 D(G(z)): 0.4781 / 0.3162\n",
      "[1/10][327/391] Loss_D: 1.3747 Loss_G: 1.4287 D(x): 0.5360 D(G(z)): 0.4053 / 0.3220\n",
      "[1/10][328/391] Loss_D: 1.3148 Loss_G: 1.8135 D(x): 0.5778 D(G(z)): 0.4249 / 0.2489\n",
      "[1/10][329/391] Loss_D: 0.9699 Loss_G: 1.6437 D(x): 0.6059 D(G(z)): 0.3077 / 0.2490\n",
      "[1/10][330/391] Loss_D: 0.9022 Loss_G: 2.0434 D(x): 0.7017 D(G(z)): 0.3801 / 0.1608\n",
      "[1/10][331/391] Loss_D: 0.9460 Loss_G: 1.2309 D(x): 0.5943 D(G(z)): 0.2806 / 0.3600\n",
      "[1/10][332/391] Loss_D: 1.2531 Loss_G: 2.4905 D(x): 0.7139 D(G(z)): 0.5226 / 0.1307\n",
      "[1/10][333/391] Loss_D: 1.4056 Loss_G: 0.7211 D(x): 0.3865 D(G(z)): 0.1796 / 0.5202\n",
      "[1/10][334/391] Loss_D: 1.3378 Loss_G: 2.3003 D(x): 0.8181 D(G(z)): 0.6309 / 0.1444\n",
      "[1/10][335/391] Loss_D: 1.1930 Loss_G: 1.0493 D(x): 0.4665 D(G(z)): 0.2349 / 0.4436\n",
      "[1/10][336/391] Loss_D: 1.4308 Loss_G: 1.9113 D(x): 0.7139 D(G(z)): 0.5528 / 0.2322\n",
      "[1/10][337/391] Loss_D: 1.1699 Loss_G: 1.7727 D(x): 0.5795 D(G(z)): 0.3138 / 0.2093\n",
      "[1/10][338/391] Loss_D: 1.2522 Loss_G: 1.3782 D(x): 0.4758 D(G(z)): 0.3323 / 0.2798\n",
      "[1/10][339/391] Loss_D: 0.9176 Loss_G: 2.2577 D(x): 0.6668 D(G(z)): 0.3761 / 0.1206\n",
      "[1/10][340/391] Loss_D: 1.1068 Loss_G: 2.7410 D(x): 0.6261 D(G(z)): 0.4228 / 0.0805\n",
      "[1/10][341/391] Loss_D: 1.1125 Loss_G: 1.7171 D(x): 0.5295 D(G(z)): 0.3105 / 0.2089\n",
      "[1/10][342/391] Loss_D: 1.4280 Loss_G: 2.3127 D(x): 0.6513 D(G(z)): 0.5514 / 0.1349\n",
      "[1/10][343/391] Loss_D: 1.4942 Loss_G: 1.0707 D(x): 0.4374 D(G(z)): 0.3650 / 0.3936\n",
      "[1/10][344/391] Loss_D: 1.2138 Loss_G: 2.6331 D(x): 0.7554 D(G(z)): 0.5417 / 0.0985\n",
      "[1/10][345/391] Loss_D: 0.9973 Loss_G: 1.8714 D(x): 0.5392 D(G(z)): 0.2073 / 0.1876\n",
      "[1/10][346/391] Loss_D: 0.9632 Loss_G: 1.9484 D(x): 0.7272 D(G(z)): 0.4233 / 0.1807\n",
      "[1/10][347/391] Loss_D: 0.9107 Loss_G: 2.2865 D(x): 0.7141 D(G(z)): 0.3815 / 0.1389\n",
      "[1/10][348/391] Loss_D: 0.8615 Loss_G: 2.5784 D(x): 0.6908 D(G(z)): 0.3220 / 0.0963\n",
      "[1/10][349/391] Loss_D: 1.0070 Loss_G: 1.4779 D(x): 0.5744 D(G(z)): 0.2643 / 0.2560\n",
      "[1/10][350/391] Loss_D: 1.3543 Loss_G: 3.9983 D(x): 0.8161 D(G(z)): 0.6231 / 0.0314\n",
      "[1/10][351/391] Loss_D: 2.4511 Loss_G: 0.8021 D(x): 0.1546 D(G(z)): 0.0710 / 0.5359\n",
      "[1/10][352/391] Loss_D: 1.2686 Loss_G: 1.8881 D(x): 0.9220 D(G(z)): 0.6080 / 0.1865\n",
      "[1/10][353/391] Loss_D: 0.7256 Loss_G: 2.8674 D(x): 0.7607 D(G(z)): 0.3085 / 0.0758\n",
      "[1/10][354/391] Loss_D: 0.7675 Loss_G: 2.0528 D(x): 0.6250 D(G(z)): 0.1566 / 0.1713\n",
      "[1/10][355/391] Loss_D: 0.6714 Loss_G: 2.6315 D(x): 0.8756 D(G(z)): 0.3667 / 0.0949\n",
      "[1/10][356/391] Loss_D: 0.6637 Loss_G: 2.9750 D(x): 0.7906 D(G(z)): 0.2973 / 0.0651\n",
      "[1/10][357/391] Loss_D: 0.7232 Loss_G: 3.0502 D(x): 0.7356 D(G(z)): 0.2499 / 0.0815\n",
      "[1/10][358/391] Loss_D: 0.8929 Loss_G: 4.5248 D(x): 0.8404 D(G(z)): 0.4551 / 0.0158\n",
      "[1/10][359/391] Loss_D: 2.6298 Loss_G: 0.6188 D(x): 0.1038 D(G(z)): 0.0522 / 0.6062\n",
      "[1/10][360/391] Loss_D: 2.5364 Loss_G: 2.3109 D(x): 0.8233 D(G(z)): 0.8299 / 0.1601\n",
      "[1/10][361/391] Loss_D: 1.3922 Loss_G: 2.1021 D(x): 0.4500 D(G(z)): 0.2728 / 0.1747\n",
      "[1/10][362/391] Loss_D: 1.4848 Loss_G: 0.9993 D(x): 0.4547 D(G(z)): 0.3260 / 0.4215\n",
      "[1/10][363/391] Loss_D: 1.8758 Loss_G: 0.9979 D(x): 0.5786 D(G(z)): 0.6276 / 0.4183\n",
      "[1/10][364/391] Loss_D: 1.7323 Loss_G: 1.0132 D(x): 0.4421 D(G(z)): 0.5093 / 0.3899\n",
      "[1/10][365/391] Loss_D: 1.5761 Loss_G: 0.8786 D(x): 0.4428 D(G(z)): 0.4923 / 0.4317\n",
      "[1/10][366/391] Loss_D: 1.3938 Loss_G: 0.9316 D(x): 0.5072 D(G(z)): 0.4746 / 0.4172\n",
      "[1/10][367/391] Loss_D: 1.1901 Loss_G: 0.9662 D(x): 0.5712 D(G(z)): 0.4417 / 0.3962\n",
      "[1/10][368/391] Loss_D: 1.1976 Loss_G: 1.2111 D(x): 0.6072 D(G(z)): 0.4657 / 0.3266\n",
      "[1/10][369/391] Loss_D: 1.2287 Loss_G: 1.0165 D(x): 0.5251 D(G(z)): 0.3898 / 0.3809\n",
      "[1/10][370/391] Loss_D: 1.2010 Loss_G: 1.1492 D(x): 0.5815 D(G(z)): 0.4485 / 0.3464\n",
      "[1/10][371/391] Loss_D: 1.2815 Loss_G: 1.0604 D(x): 0.5348 D(G(z)): 0.4377 / 0.3763\n",
      "[1/10][372/391] Loss_D: 1.3873 Loss_G: 1.1869 D(x): 0.5103 D(G(z)): 0.4517 / 0.3357\n",
      "[1/10][373/391] Loss_D: 1.1792 Loss_G: 1.0231 D(x): 0.5208 D(G(z)): 0.3638 / 0.3787\n",
      "[1/10][374/391] Loss_D: 1.1825 Loss_G: 1.3866 D(x): 0.6051 D(G(z)): 0.4596 / 0.2947\n",
      "[1/10][375/391] Loss_D: 1.0100 Loss_G: 1.2749 D(x): 0.5596 D(G(z)): 0.3150 / 0.3092\n",
      "[1/10][376/391] Loss_D: 1.0098 Loss_G: 1.2737 D(x): 0.6449 D(G(z)): 0.4051 / 0.3115\n",
      "[1/10][377/391] Loss_D: 1.0739 Loss_G: 1.2816 D(x): 0.6518 D(G(z)): 0.4431 / 0.3027\n",
      "[1/10][378/391] Loss_D: 1.0085 Loss_G: 1.2369 D(x): 0.5852 D(G(z)): 0.3413 / 0.3127\n",
      "[1/10][379/391] Loss_D: 0.9165 Loss_G: 1.7061 D(x): 0.7046 D(G(z)): 0.3985 / 0.2510\n",
      "[1/10][380/391] Loss_D: 0.9786 Loss_G: 1.4965 D(x): 0.6345 D(G(z)): 0.3516 / 0.2650\n",
      "[1/10][381/391] Loss_D: 1.1025 Loss_G: 1.4079 D(x): 0.6037 D(G(z)): 0.3859 / 0.2982\n",
      "[1/10][382/391] Loss_D: 1.1754 Loss_G: 1.8229 D(x): 0.6599 D(G(z)): 0.4814 / 0.1915\n",
      "[1/10][383/391] Loss_D: 1.1669 Loss_G: 1.7875 D(x): 0.5773 D(G(z)): 0.4061 / 0.1921\n",
      "[1/10][384/391] Loss_D: 1.2529 Loss_G: 1.1607 D(x): 0.5067 D(G(z)): 0.3777 / 0.3540\n",
      "[1/10][385/391] Loss_D: 1.5425 Loss_G: 2.9494 D(x): 0.6310 D(G(z)): 0.5918 / 0.0711\n",
      "[1/10][386/391] Loss_D: 1.5120 Loss_G: 0.5623 D(x): 0.3154 D(G(z)): 0.1353 / 0.6121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][387/391] Loss_D: 1.7104 Loss_G: 2.3562 D(x): 0.8574 D(G(z)): 0.7387 / 0.1259\n",
      "[1/10][388/391] Loss_D: 1.2050 Loss_G: 1.3261 D(x): 0.4397 D(G(z)): 0.2247 / 0.2898\n",
      "[1/10][389/391] Loss_D: 1.0210 Loss_G: 1.2646 D(x): 0.6562 D(G(z)): 0.4110 / 0.3114\n",
      "[1/10][390/391] Loss_D: 1.1534 Loss_G: 1.6747 D(x): 0.6411 D(G(z)): 0.4716 / 0.2134\n",
      "[2/10][0/391] Loss_D: 1.2784 Loss_G: 1.4039 D(x): 0.5165 D(G(z)): 0.3959 / 0.2872\n",
      "[2/10][1/391] Loss_D: 1.3249 Loss_G: 1.6908 D(x): 0.5857 D(G(z)): 0.4912 / 0.2134\n",
      "[2/10][2/391] Loss_D: 1.1595 Loss_G: 0.9943 D(x): 0.4848 D(G(z)): 0.2870 / 0.3995\n",
      "[2/10][3/391] Loss_D: 1.2350 Loss_G: 2.6287 D(x): 0.7980 D(G(z)): 0.6035 / 0.0909\n",
      "[2/10][4/391] Loss_D: 1.1460 Loss_G: 1.0289 D(x): 0.4436 D(G(z)): 0.1773 / 0.3934\n",
      "[2/10][5/391] Loss_D: 1.1316 Loss_G: 2.6361 D(x): 0.7604 D(G(z)): 0.5322 / 0.0906\n",
      "[2/10][6/391] Loss_D: 0.9832 Loss_G: 1.4130 D(x): 0.5313 D(G(z)): 0.2342 / 0.2844\n",
      "[2/10][7/391] Loss_D: 0.9347 Loss_G: 1.9597 D(x): 0.7220 D(G(z)): 0.4073 / 0.1661\n",
      "[2/10][8/391] Loss_D: 0.8150 Loss_G: 2.1065 D(x): 0.6652 D(G(z)): 0.2823 / 0.1495\n",
      "[2/10][9/391] Loss_D: 0.9747 Loss_G: 3.0714 D(x): 0.7291 D(G(z)): 0.4264 / 0.0565\n",
      "[2/10][10/391] Loss_D: 1.1109 Loss_G: 0.6989 D(x): 0.4069 D(G(z)): 0.0894 / 0.5192\n",
      "[2/10][11/391] Loss_D: 1.2198 Loss_G: 3.6381 D(x): 0.9062 D(G(z)): 0.6355 / 0.0380\n",
      "[2/10][12/391] Loss_D: 0.6419 Loss_G: 2.1184 D(x): 0.6168 D(G(z)): 0.1040 / 0.1443\n",
      "[2/10][13/391] Loss_D: 0.7093 Loss_G: 3.1779 D(x): 0.7844 D(G(z)): 0.3410 / 0.0570\n",
      "[2/10][14/391] Loss_D: 0.7925 Loss_G: 2.8172 D(x): 0.7096 D(G(z)): 0.3093 / 0.0808\n",
      "[2/10][15/391] Loss_D: 0.8516 Loss_G: 1.9402 D(x): 0.4928 D(G(z)): 0.0555 / 0.2031\n",
      "[2/10][16/391] Loss_D: 2.2988 Loss_G: 4.8805 D(x): 0.9330 D(G(z)): 0.8459 / 0.0151\n",
      "[2/10][17/391] Loss_D: 3.3403 Loss_G: 0.6181 D(x): 0.0618 D(G(z)): 0.0808 / 0.5801\n",
      "[2/10][18/391] Loss_D: 1.4437 Loss_G: 1.4100 D(x): 0.8637 D(G(z)): 0.6650 / 0.3162\n",
      "[2/10][19/391] Loss_D: 0.8813 Loss_G: 2.0751 D(x): 0.7503 D(G(z)): 0.4088 / 0.1676\n",
      "[2/10][20/391] Loss_D: 1.0133 Loss_G: 1.3920 D(x): 0.5606 D(G(z)): 0.2757 / 0.2879\n",
      "[2/10][21/391] Loss_D: 0.7955 Loss_G: 1.8935 D(x): 0.7497 D(G(z)): 0.3543 / 0.2064\n",
      "[2/10][22/391] Loss_D: 0.7513 Loss_G: 1.7964 D(x): 0.7620 D(G(z)): 0.3352 / 0.2099\n",
      "[2/10][23/391] Loss_D: 0.7359 Loss_G: 1.7328 D(x): 0.7389 D(G(z)): 0.3129 / 0.2033\n",
      "[2/10][24/391] Loss_D: 0.6534 Loss_G: 2.4119 D(x): 0.8297 D(G(z)): 0.3404 / 0.1146\n",
      "[2/10][25/391] Loss_D: 0.7125 Loss_G: 1.7472 D(x): 0.6816 D(G(z)): 0.2470 / 0.1941\n",
      "[2/10][26/391] Loss_D: 0.6253 Loss_G: 3.8113 D(x): 0.8742 D(G(z)): 0.3662 / 0.0309\n",
      "[2/10][27/391] Loss_D: 1.1134 Loss_G: 1.1697 D(x): 0.4356 D(G(z)): 0.1158 / 0.3977\n",
      "[2/10][28/391] Loss_D: 1.3044 Loss_G: 4.8648 D(x): 0.9500 D(G(z)): 0.6618 / 0.0132\n",
      "[2/10][29/391] Loss_D: 1.5540 Loss_G: 1.6082 D(x): 0.2859 D(G(z)): 0.0396 / 0.2632\n",
      "[2/10][30/391] Loss_D: 0.6747 Loss_G: 2.6543 D(x): 0.8827 D(G(z)): 0.3632 / 0.1076\n",
      "[2/10][31/391] Loss_D: 0.6699 Loss_G: 3.5558 D(x): 0.8162 D(G(z)): 0.3287 / 0.0403\n",
      "[2/10][32/391] Loss_D: 1.3158 Loss_G: 3.5667 D(x): 0.5904 D(G(z)): 0.4349 / 0.0400\n",
      "[2/10][33/391] Loss_D: 0.8159 Loss_G: 2.5178 D(x): 0.5923 D(G(z)): 0.1951 / 0.1148\n",
      "[2/10][34/391] Loss_D: 0.4256 Loss_G: 3.4785 D(x): 0.8775 D(G(z)): 0.2164 / 0.0488\n",
      "[2/10][35/391] Loss_D: 1.7025 Loss_G: 9.3018 D(x): 0.8729 D(G(z)): 0.7269 / 0.0003\n",
      "[2/10][36/391] Loss_D: 6.4342 Loss_G: 3.3809 D(x): 0.0056 D(G(z)): 0.0020 / 0.0638\n",
      "[2/10][37/391] Loss_D: 2.5267 Loss_G: 0.4883 D(x): 0.1590 D(G(z)): 0.1276 / 0.6668\n",
      "[2/10][38/391] Loss_D: 1.6902 Loss_G: 0.9820 D(x): 0.8704 D(G(z)): 0.6974 / 0.4401\n",
      "[2/10][39/391] Loss_D: 1.4501 Loss_G: 1.8591 D(x): 0.7574 D(G(z)): 0.6214 / 0.1921\n",
      "[2/10][40/391] Loss_D: 1.4127 Loss_G: 1.5800 D(x): 0.4361 D(G(z)): 0.3192 / 0.2655\n",
      "[2/10][41/391] Loss_D: 1.3069 Loss_G: 1.3841 D(x): 0.4972 D(G(z)): 0.3775 / 0.2861\n",
      "[2/10][42/391] Loss_D: 1.1399 Loss_G: 1.1927 D(x): 0.5882 D(G(z)): 0.3938 / 0.3390\n",
      "[2/10][43/391] Loss_D: 1.1292 Loss_G: 1.2752 D(x): 0.6095 D(G(z)): 0.4266 / 0.3035\n",
      "[2/10][44/391] Loss_D: 1.0297 Loss_G: 1.2088 D(x): 0.6227 D(G(z)): 0.3979 / 0.3175\n",
      "[2/10][45/391] Loss_D: 1.2264 Loss_G: 1.0519 D(x): 0.5385 D(G(z)): 0.4100 / 0.3658\n",
      "[2/10][46/391] Loss_D: 1.1987 Loss_G: 1.0531 D(x): 0.5585 D(G(z)): 0.4171 / 0.3741\n",
      "[2/10][47/391] Loss_D: 1.0415 Loss_G: 1.1275 D(x): 0.6523 D(G(z)): 0.4262 / 0.3481\n",
      "[2/10][48/391] Loss_D: 1.0865 Loss_G: 1.2133 D(x): 0.6462 D(G(z)): 0.4487 / 0.3201\n",
      "[2/10][49/391] Loss_D: 0.9503 Loss_G: 1.2309 D(x): 0.6496 D(G(z)): 0.3791 / 0.3073\n",
      "[2/10][50/391] Loss_D: 0.9052 Loss_G: 1.4437 D(x): 0.6597 D(G(z)): 0.3591 / 0.2570\n",
      "[2/10][51/391] Loss_D: 0.7935 Loss_G: 1.8513 D(x): 0.7295 D(G(z)): 0.3569 / 0.1893\n",
      "[2/10][52/391] Loss_D: 0.6944 Loss_G: 1.9036 D(x): 0.7565 D(G(z)): 0.3118 / 0.1748\n",
      "[2/10][53/391] Loss_D: 0.6523 Loss_G: 1.9585 D(x): 0.7550 D(G(z)): 0.2654 / 0.1646\n",
      "[2/10][54/391] Loss_D: 0.4410 Loss_G: 2.5077 D(x): 0.8606 D(G(z)): 0.2379 / 0.1017\n",
      "[2/10][55/391] Loss_D: 0.4843 Loss_G: 3.1627 D(x): 0.8550 D(G(z)): 0.2575 / 0.0533\n",
      "[2/10][56/391] Loss_D: 0.4261 Loss_G: 2.9235 D(x): 0.7861 D(G(z)): 0.1503 / 0.0681\n",
      "[2/10][57/391] Loss_D: 0.6155 Loss_G: 1.7138 D(x): 0.7471 D(G(z)): 0.2402 / 0.2293\n",
      "[2/10][58/391] Loss_D: 0.2829 Loss_G: 4.0594 D(x): 0.9337 D(G(z)): 0.1813 / 0.0220\n",
      "[2/10][59/391] Loss_D: 0.4814 Loss_G: 0.2223 D(x): 0.6869 D(G(z)): 0.0590 / 0.8136\n",
      "[2/10][60/391] Loss_D: 2.1151 Loss_G: 6.9221 D(x): 0.8632 D(G(z)): 0.8126 / 0.0017\n",
      "[2/10][61/391] Loss_D: 5.2656 Loss_G: 2.3315 D(x): 0.0088 D(G(z)): 0.0033 / 0.1101\n",
      "[2/10][62/391] Loss_D: 1.6252 Loss_G: 0.5120 D(x): 0.2658 D(G(z)): 0.1327 / 0.6335\n",
      "[2/10][63/391] Loss_D: 1.5912 Loss_G: 0.6636 D(x): 0.7905 D(G(z)): 0.6656 / 0.5801\n",
      "[2/10][64/391] Loss_D: 1.2615 Loss_G: 1.2082 D(x): 0.7755 D(G(z)): 0.5782 / 0.3629\n",
      "[2/10][65/391] Loss_D: 1.1047 Loss_G: 1.2186 D(x): 0.6190 D(G(z)): 0.4020 / 0.3126\n",
      "[2/10][66/391] Loss_D: 1.0772 Loss_G: 1.2610 D(x): 0.5716 D(G(z)): 0.3541 / 0.2993\n",
      "[2/10][67/391] Loss_D: 1.0045 Loss_G: 1.2748 D(x): 0.6108 D(G(z)): 0.3625 / 0.3130\n",
      "[2/10][68/391] Loss_D: 1.0840 Loss_G: 1.0115 D(x): 0.6131 D(G(z)): 0.3976 / 0.3816\n",
      "[2/10][69/391] Loss_D: 1.2091 Loss_G: 1.1375 D(x): 0.5865 D(G(z)): 0.4603 / 0.3400\n",
      "[2/10][70/391] Loss_D: 1.1336 Loss_G: 1.2125 D(x): 0.6387 D(G(z)): 0.4588 / 0.3156\n",
      "[2/10][71/391] Loss_D: 1.2880 Loss_G: 1.0294 D(x): 0.5138 D(G(z)): 0.4154 / 0.3742\n",
      "[2/10][72/391] Loss_D: 1.2626 Loss_G: 1.2290 D(x): 0.5684 D(G(z)): 0.4623 / 0.3111\n",
      "[2/10][73/391] Loss_D: 1.1335 Loss_G: 1.3350 D(x): 0.5745 D(G(z)): 0.3940 / 0.2818\n",
      "[2/10][74/391] Loss_D: 1.1424 Loss_G: 1.2135 D(x): 0.5608 D(G(z)): 0.3878 / 0.3141\n",
      "[2/10][75/391] Loss_D: 1.2779 Loss_G: 1.3651 D(x): 0.6077 D(G(z)): 0.4986 / 0.2761\n",
      "[2/10][76/391] Loss_D: 1.2508 Loss_G: 1.5338 D(x): 0.4958 D(G(z)): 0.3765 / 0.2372\n",
      "[2/10][77/391] Loss_D: 1.3226 Loss_G: 1.9821 D(x): 0.5479 D(G(z)): 0.4708 / 0.1671\n",
      "[2/10][78/391] Loss_D: 1.2992 Loss_G: 1.3412 D(x): 0.4405 D(G(z)): 0.2948 / 0.2813\n",
      "[2/10][79/391] Loss_D: 1.3106 Loss_G: 1.9214 D(x): 0.6018 D(G(z)): 0.5160 / 0.1743\n",
      "[2/10][80/391] Loss_D: 1.2461 Loss_G: 1.3928 D(x): 0.5088 D(G(z)): 0.3782 / 0.2782\n",
      "[2/10][81/391] Loss_D: 1.3145 Loss_G: 1.5389 D(x): 0.6003 D(G(z)): 0.5056 / 0.2481\n",
      "[2/10][82/391] Loss_D: 1.0948 Loss_G: 1.3954 D(x): 0.5626 D(G(z)): 0.3447 / 0.2803\n",
      "[2/10][83/391] Loss_D: 1.1089 Loss_G: 1.5278 D(x): 0.6157 D(G(z)): 0.4136 / 0.2491\n",
      "[2/10][84/391] Loss_D: 0.9939 Loss_G: 1.7588 D(x): 0.6837 D(G(z)): 0.4101 / 0.2009\n",
      "[2/10][85/391] Loss_D: 0.9646 Loss_G: 1.2126 D(x): 0.6228 D(G(z)): 0.3445 / 0.3351\n",
      "[2/10][86/391] Loss_D: 1.0882 Loss_G: 2.3433 D(x): 0.7671 D(G(z)): 0.5176 / 0.1320\n",
      "[2/10][87/391] Loss_D: 1.4467 Loss_G: 0.9360 D(x): 0.4163 D(G(z)): 0.2829 / 0.4299\n",
      "[2/10][88/391] Loss_D: 1.3126 Loss_G: 1.1528 D(x): 0.7151 D(G(z)): 0.5519 / 0.3347\n",
      "[2/10][89/391] Loss_D: 1.1599 Loss_G: 1.5099 D(x): 0.6479 D(G(z)): 0.4680 / 0.2427\n",
      "[2/10][90/391] Loss_D: 0.9924 Loss_G: 1.2285 D(x): 0.6010 D(G(z)): 0.3270 / 0.3235\n",
      "[2/10][91/391] Loss_D: 1.1071 Loss_G: 1.5145 D(x): 0.7014 D(G(z)): 0.4607 / 0.2469\n",
      "[2/10][92/391] Loss_D: 1.2600 Loss_G: 1.3057 D(x): 0.5785 D(G(z)): 0.4496 / 0.3061\n",
      "[2/10][93/391] Loss_D: 1.3005 Loss_G: 1.1419 D(x): 0.5181 D(G(z)): 0.4099 / 0.3560\n",
      "[2/10][94/391] Loss_D: 1.4688 Loss_G: 2.6503 D(x): 0.6935 D(G(z)): 0.6213 / 0.0876\n",
      "[2/10][95/391] Loss_D: 1.7304 Loss_G: 0.7800 D(x): 0.2624 D(G(z)): 0.1703 / 0.4954\n",
      "[2/10][96/391] Loss_D: 1.3404 Loss_G: 1.6398 D(x): 0.7326 D(G(z)): 0.5850 / 0.2199\n",
      "[2/10][97/391] Loss_D: 1.1514 Loss_G: 2.0417 D(x): 0.5471 D(G(z)): 0.3518 / 0.1558\n",
      "[2/10][98/391] Loss_D: 1.0856 Loss_G: 1.7012 D(x): 0.5374 D(G(z)): 0.3052 / 0.2273\n",
      "[2/10][99/391] Loss_D: 1.2354 Loss_G: 2.5495 D(x): 0.6388 D(G(z)): 0.4903 / 0.0927\n",
      "[2/10][100/391] Loss_D: 1.4597 Loss_G: 1.5719 D(x): 0.4198 D(G(z)): 0.3282 / 0.2445\n",
      "[2/10][101/391] Loss_D: 1.4470 Loss_G: 2.1907 D(x): 0.5387 D(G(z)): 0.4903 / 0.1553\n",
      "[2/10][102/391] Loss_D: 1.2762 Loss_G: 1.4384 D(x): 0.4715 D(G(z)): 0.3110 / 0.2810\n",
      "[2/10][103/391] Loss_D: 1.2912 Loss_G: 1.7076 D(x): 0.6153 D(G(z)): 0.4933 / 0.2155\n",
      "[2/10][104/391] Loss_D: 1.4303 Loss_G: 1.0079 D(x): 0.4612 D(G(z)): 0.3909 / 0.4017\n",
      "[2/10][105/391] Loss_D: 1.4066 Loss_G: 1.3707 D(x): 0.6237 D(G(z)): 0.5503 / 0.2972\n",
      "[2/10][106/391] Loss_D: 1.3923 Loss_G: 1.2139 D(x): 0.5354 D(G(z)): 0.4655 / 0.3284\n",
      "[2/10][107/391] Loss_D: 1.1654 Loss_G: 1.0611 D(x): 0.5627 D(G(z)): 0.3945 / 0.3678\n",
      "[2/10][108/391] Loss_D: 1.2056 Loss_G: 1.6374 D(x): 0.6634 D(G(z)): 0.5133 / 0.2335\n",
      "[2/10][109/391] Loss_D: 1.0374 Loss_G: 1.6985 D(x): 0.5889 D(G(z)): 0.3444 / 0.2122\n",
      "[2/10][110/391] Loss_D: 1.1096 Loss_G: 1.3754 D(x): 0.5689 D(G(z)): 0.3756 / 0.2749\n",
      "[2/10][111/391] Loss_D: 1.2235 Loss_G: 1.6341 D(x): 0.5901 D(G(z)): 0.4626 / 0.2194\n",
      "[2/10][112/391] Loss_D: 1.0220 Loss_G: 1.5613 D(x): 0.5893 D(G(z)): 0.3522 / 0.2291\n",
      "[2/10][113/391] Loss_D: 1.1847 Loss_G: 1.0343 D(x): 0.5509 D(G(z)): 0.4069 / 0.3746\n",
      "[2/10][114/391] Loss_D: 1.3992 Loss_G: 2.7860 D(x): 0.7143 D(G(z)): 0.6222 / 0.0800\n",
      "[2/10][115/391] Loss_D: 1.6217 Loss_G: 0.9432 D(x): 0.3078 D(G(z)): 0.1880 / 0.4340\n",
      "[2/10][116/391] Loss_D: 1.1632 Loss_G: 1.4268 D(x): 0.7319 D(G(z)): 0.5085 / 0.2812\n",
      "[2/10][117/391] Loss_D: 1.1610 Loss_G: 1.6267 D(x): 0.6498 D(G(z)): 0.4589 / 0.2167\n",
      "[2/10][118/391] Loss_D: 1.3013 Loss_G: 1.3588 D(x): 0.5370 D(G(z)): 0.4391 / 0.2974\n",
      "[2/10][119/391] Loss_D: 1.3050 Loss_G: 1.5255 D(x): 0.6043 D(G(z)): 0.4897 / 0.2521\n",
      "[2/10][120/391] Loss_D: 1.6338 Loss_G: 1.1967 D(x): 0.4773 D(G(z)): 0.4782 / 0.3580\n",
      "[2/10][121/391] Loss_D: 1.4594 Loss_G: 1.7354 D(x): 0.5816 D(G(z)): 0.5391 / 0.2026\n",
      "[2/10][122/391] Loss_D: 1.1304 Loss_G: 1.5391 D(x): 0.5162 D(G(z)): 0.3212 / 0.2466\n",
      "[2/10][123/391] Loss_D: 1.2905 Loss_G: 1.9525 D(x): 0.6031 D(G(z)): 0.4870 / 0.1706\n",
      "[2/10][124/391] Loss_D: 1.1418 Loss_G: 1.7855 D(x): 0.5314 D(G(z)): 0.3145 / 0.2060\n",
      "[2/10][125/391] Loss_D: 1.2250 Loss_G: 1.4614 D(x): 0.5911 D(G(z)): 0.4261 / 0.2640\n",
      "[2/10][126/391] Loss_D: 1.3622 Loss_G: 1.4634 D(x): 0.5732 D(G(z)): 0.4780 / 0.2591\n",
      "[2/10][127/391] Loss_D: 1.3381 Loss_G: 2.4322 D(x): 0.6232 D(G(z)): 0.5141 / 0.1195\n",
      "[2/10][128/391] Loss_D: 1.2128 Loss_G: 1.0113 D(x): 0.4591 D(G(z)): 0.2657 / 0.3956\n",
      "[2/10][129/391] Loss_D: 1.1698 Loss_G: 2.4368 D(x): 0.7858 D(G(z)): 0.5510 / 0.1074\n",
      "[2/10][130/391] Loss_D: 1.5603 Loss_G: 0.7597 D(x): 0.3330 D(G(z)): 0.2656 / 0.4920\n",
      "[2/10][131/391] Loss_D: 1.2770 Loss_G: 1.9915 D(x): 0.7577 D(G(z)): 0.5855 / 0.1566\n",
      "[2/10][132/391] Loss_D: 0.9360 Loss_G: 1.7818 D(x): 0.5589 D(G(z)): 0.2492 / 0.1914\n",
      "[2/10][133/391] Loss_D: 1.1053 Loss_G: 1.5688 D(x): 0.6142 D(G(z)): 0.4078 / 0.2361\n",
      "[2/10][134/391] Loss_D: 1.1222 Loss_G: 1.9563 D(x): 0.6460 D(G(z)): 0.4527 / 0.1650\n",
      "[2/10][135/391] Loss_D: 0.9672 Loss_G: 1.5830 D(x): 0.5598 D(G(z)): 0.2816 / 0.2262\n",
      "[2/10][136/391] Loss_D: 0.9985 Loss_G: 2.1071 D(x): 0.6964 D(G(z)): 0.4369 / 0.1425\n",
      "[2/10][137/391] Loss_D: 0.9684 Loss_G: 1.7903 D(x): 0.5989 D(G(z)): 0.3179 / 0.2032\n",
      "[2/10][138/391] Loss_D: 1.3817 Loss_G: 2.9900 D(x): 0.7174 D(G(z)): 0.6037 / 0.0678\n",
      "[2/10][139/391] Loss_D: 1.8895 Loss_G: 0.7392 D(x): 0.2172 D(G(z)): 0.1221 / 0.5019\n",
      "[2/10][140/391] Loss_D: 1.4751 Loss_G: 1.4850 D(x): 0.7226 D(G(z)): 0.6413 / 0.2952\n",
      "[2/10][141/391] Loss_D: 1.1102 Loss_G: 1.5660 D(x): 0.6107 D(G(z)): 0.3828 / 0.2458\n",
      "[2/10][142/391] Loss_D: 1.4670 Loss_G: 1.4202 D(x): 0.5208 D(G(z)): 0.4571 / 0.2691\n",
      "[2/10][143/391] Loss_D: 1.2908 Loss_G: 1.5399 D(x): 0.5763 D(G(z)): 0.4564 / 0.2493\n",
      "[2/10][144/391] Loss_D: 1.0693 Loss_G: 1.8914 D(x): 0.6204 D(G(z)): 0.3877 / 0.1858\n",
      "[2/10][145/391] Loss_D: 1.1275 Loss_G: 1.6852 D(x): 0.6101 D(G(z)): 0.4131 / 0.2130\n",
      "[2/10][146/391] Loss_D: 1.3719 Loss_G: 1.6553 D(x): 0.5646 D(G(z)): 0.4768 / 0.2260\n",
      "[2/10][147/391] Loss_D: 1.0913 Loss_G: 2.0619 D(x): 0.6517 D(G(z)): 0.4349 / 0.1493\n",
      "[2/10][148/391] Loss_D: 0.9209 Loss_G: 1.7326 D(x): 0.5709 D(G(z)): 0.2616 / 0.2157\n",
      "[2/10][149/391] Loss_D: 0.8822 Loss_G: 2.2494 D(x): 0.7456 D(G(z)): 0.3935 / 0.1424\n",
      "[2/10][150/391] Loss_D: 0.8493 Loss_G: 1.7281 D(x): 0.6402 D(G(z)): 0.2659 / 0.2039\n",
      "[2/10][151/391] Loss_D: 0.7644 Loss_G: 2.8177 D(x): 0.7880 D(G(z)): 0.3720 / 0.0815\n",
      "[2/10][152/391] Loss_D: 0.7396 Loss_G: 1.7976 D(x): 0.6471 D(G(z)): 0.2146 / 0.2001\n",
      "[2/10][153/391] Loss_D: 0.8572 Loss_G: 3.3831 D(x): 0.8583 D(G(z)): 0.4613 / 0.0449\n",
      "[2/10][154/391] Loss_D: 1.2526 Loss_G: 0.8198 D(x): 0.3925 D(G(z)): 0.1170 / 0.4737\n",
      "[2/10][155/391] Loss_D: 1.3308 Loss_G: 4.2700 D(x): 0.9444 D(G(z)): 0.6808 / 0.0195\n",
      "[2/10][156/391] Loss_D: 1.5837 Loss_G: 0.5209 D(x): 0.2846 D(G(z)): 0.0863 / 0.6364\n",
      "[2/10][157/391] Loss_D: 2.3549 Loss_G: 3.1171 D(x): 0.8987 D(G(z)): 0.8507 / 0.0635\n",
      "[2/10][158/391] Loss_D: 1.4993 Loss_G: 1.7969 D(x): 0.3301 D(G(z)): 0.1472 / 0.2411\n",
      "[2/10][159/391] Loss_D: 1.3691 Loss_G: 1.4174 D(x): 0.5951 D(G(z)): 0.4545 / 0.3068\n",
      "[2/10][160/391] Loss_D: 1.3424 Loss_G: 2.4157 D(x): 0.6345 D(G(z)): 0.5007 / 0.1245\n",
      "[2/10][161/391] Loss_D: 1.3021 Loss_G: 1.8568 D(x): 0.4675 D(G(z)): 0.3291 / 0.1913\n",
      "[2/10][162/391] Loss_D: 1.1815 Loss_G: 1.5264 D(x): 0.5241 D(G(z)): 0.3354 / 0.2546\n",
      "[2/10][163/391] Loss_D: 1.1130 Loss_G: 2.0271 D(x): 0.6845 D(G(z)): 0.4698 / 0.1570\n",
      "[2/10][164/391] Loss_D: 1.0625 Loss_G: 2.3070 D(x): 0.5658 D(G(z)): 0.3261 / 0.1307\n",
      "[2/10][165/391] Loss_D: 1.0438 Loss_G: 2.3540 D(x): 0.6117 D(G(z)): 0.3522 / 0.1286\n",
      "[2/10][166/391] Loss_D: 1.0677 Loss_G: 2.2705 D(x): 0.6838 D(G(z)): 0.4551 / 0.1382\n",
      "[2/10][167/391] Loss_D: 1.1243 Loss_G: 1.7361 D(x): 0.5569 D(G(z)): 0.3099 / 0.2122\n",
      "[2/10][168/391] Loss_D: 0.7160 Loss_G: 1.9756 D(x): 0.8257 D(G(z)): 0.3812 / 0.1661\n",
      "[2/10][169/391] Loss_D: 0.6816 Loss_G: 1.8425 D(x): 0.7281 D(G(z)): 0.2753 / 0.1812\n",
      "[2/10][170/391] Loss_D: 0.7513 Loss_G: 2.7457 D(x): 0.7835 D(G(z)): 0.3725 / 0.0827\n",
      "[2/10][171/391] Loss_D: 0.5883 Loss_G: 2.6311 D(x): 0.7457 D(G(z)): 0.2223 / 0.0872\n",
      "[2/10][172/391] Loss_D: 0.7140 Loss_G: 1.7204 D(x): 0.7107 D(G(z)): 0.2707 / 0.2053\n",
      "[2/10][173/391] Loss_D: 0.6171 Loss_G: 2.9537 D(x): 0.9200 D(G(z)): 0.3951 / 0.0647\n",
      "[2/10][174/391] Loss_D: 0.6569 Loss_G: 1.6500 D(x): 0.6853 D(G(z)): 0.1911 / 0.2307\n",
      "[2/10][175/391] Loss_D: 0.5356 Loss_G: 3.5579 D(x): 0.9075 D(G(z)): 0.3353 / 0.0353\n",
      "[2/10][176/391] Loss_D: 1.0708 Loss_G: 2.0633 D(x): 0.5172 D(G(z)): 0.2249 / 0.1593\n",
      "[2/10][177/391] Loss_D: 1.5385 Loss_G: 1.0301 D(x): 0.5094 D(G(z)): 0.4874 / 0.3960\n",
      "[2/10][178/391] Loss_D: 2.7113 Loss_G: 7.4162 D(x): 0.9093 D(G(z)): 0.8896 / 0.0010\n",
      "[2/10][179/391] Loss_D: 5.5235 Loss_G: 3.5457 D(x): 0.0091 D(G(z)): 0.0025 / 0.0446\n",
      "[2/10][180/391] Loss_D: 2.6160 Loss_G: 0.7849 D(x): 0.1142 D(G(z)): 0.0661 / 0.5198\n",
      "[2/10][181/391] Loss_D: 1.4565 Loss_G: 0.5496 D(x): 0.7409 D(G(z)): 0.5497 / 0.6273\n",
      "[2/10][182/391] Loss_D: 1.6274 Loss_G: 1.0420 D(x): 0.8101 D(G(z)): 0.6936 / 0.4272\n",
      "[2/10][183/391] Loss_D: 1.2442 Loss_G: 1.4477 D(x): 0.6457 D(G(z)): 0.4698 / 0.2754\n",
      "[2/10][184/391] Loss_D: 1.0547 Loss_G: 1.3301 D(x): 0.5620 D(G(z)): 0.3257 / 0.2924\n",
      "[2/10][185/391] Loss_D: 1.0716 Loss_G: 1.2201 D(x): 0.5668 D(G(z)): 0.3374 / 0.3150\n",
      "[2/10][186/391] Loss_D: 1.0322 Loss_G: 1.1944 D(x): 0.5984 D(G(z)): 0.3751 / 0.3268\n",
      "[2/10][187/391] Loss_D: 1.1703 Loss_G: 1.1861 D(x): 0.6436 D(G(z)): 0.4824 / 0.3253\n",
      "[2/10][188/391] Loss_D: 1.0660 Loss_G: 1.4220 D(x): 0.6327 D(G(z)): 0.4106 / 0.2634\n",
      "[2/10][189/391] Loss_D: 1.1202 Loss_G: 1.3665 D(x): 0.5890 D(G(z)): 0.3903 / 0.2852\n",
      "[2/10][190/391] Loss_D: 1.0757 Loss_G: 1.3627 D(x): 0.6199 D(G(z)): 0.3982 / 0.2839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][191/391] Loss_D: 1.0910 Loss_G: 1.4158 D(x): 0.6087 D(G(z)): 0.4008 / 0.2658\n",
      "[2/10][192/391] Loss_D: 1.2676 Loss_G: 1.1214 D(x): 0.4846 D(G(z)): 0.3767 / 0.3473\n",
      "[2/10][193/391] Loss_D: 1.3218 Loss_G: 1.2491 D(x): 0.5516 D(G(z)): 0.4640 / 0.3136\n",
      "[2/10][194/391] Loss_D: 1.1591 Loss_G: 1.3023 D(x): 0.5797 D(G(z)): 0.4181 / 0.2893\n",
      "[2/10][195/391] Loss_D: 1.0566 Loss_G: 1.5696 D(x): 0.6214 D(G(z)): 0.4049 / 0.2277\n",
      "[2/10][196/391] Loss_D: 0.9919 Loss_G: 1.5327 D(x): 0.6040 D(G(z)): 0.3488 / 0.2421\n",
      "[2/10][197/391] Loss_D: 1.0406 Loss_G: 1.6369 D(x): 0.6235 D(G(z)): 0.3891 / 0.2184\n",
      "[2/10][198/391] Loss_D: 1.1660 Loss_G: 2.0131 D(x): 0.6289 D(G(z)): 0.4537 / 0.1620\n",
      "[2/10][199/391] Loss_D: 1.1111 Loss_G: 1.7649 D(x): 0.5349 D(G(z)): 0.3140 / 0.2065\n",
      "[2/10][200/391] Loss_D: 1.1677 Loss_G: 1.7070 D(x): 0.5917 D(G(z)): 0.4043 / 0.2080\n",
      "[2/10][201/391] Loss_D: 1.1076 Loss_G: 2.0160 D(x): 0.6322 D(G(z)): 0.4231 / 0.1608\n",
      "[2/10][202/391] Loss_D: 0.9608 Loss_G: 1.8107 D(x): 0.6425 D(G(z)): 0.3478 / 0.2103\n",
      "[2/10][203/391] Loss_D: 1.0463 Loss_G: 2.9492 D(x): 0.7518 D(G(z)): 0.4548 / 0.0912\n",
      "[2/10][204/391] Loss_D: 1.4367 Loss_G: 1.0591 D(x): 0.3879 D(G(z)): 0.1909 / 0.3783\n",
      "[2/10][205/391] Loss_D: 1.2607 Loss_G: 2.4377 D(x): 0.8429 D(G(z)): 0.6242 / 0.1057\n",
      "[2/10][206/391] Loss_D: 0.9094 Loss_G: 1.8088 D(x): 0.5358 D(G(z)): 0.1853 / 0.1871\n",
      "[2/10][207/391] Loss_D: 0.7751 Loss_G: 2.3102 D(x): 0.7839 D(G(z)): 0.3754 / 0.1252\n",
      "[2/10][208/391] Loss_D: 0.8442 Loss_G: 3.3400 D(x): 0.7724 D(G(z)): 0.4001 / 0.0660\n",
      "[2/10][209/391] Loss_D: 0.9524 Loss_G: 1.9207 D(x): 0.5292 D(G(z)): 0.1657 / 0.2036\n",
      "[2/10][210/391] Loss_D: 0.7898 Loss_G: 3.4740 D(x): 0.8552 D(G(z)): 0.4189 / 0.0387\n",
      "[2/10][211/391] Loss_D: 0.4734 Loss_G: 2.9108 D(x): 0.7213 D(G(z)): 0.1079 / 0.0648\n",
      "[2/10][212/391] Loss_D: 0.5407 Loss_G: 3.1603 D(x): 0.8368 D(G(z)): 0.2834 / 0.0519\n",
      "[2/10][213/391] Loss_D: 0.3644 Loss_G: 3.1108 D(x): 0.8214 D(G(z)): 0.1200 / 0.0532\n",
      "[2/10][214/391] Loss_D: 0.6978 Loss_G: 5.3661 D(x): 0.9203 D(G(z)): 0.4320 / 0.0059\n",
      "[2/10][215/391] Loss_D: 1.8564 Loss_G: 1.4749 D(x): 0.2446 D(G(z)): 0.0671 / 0.2722\n",
      "[2/10][216/391] Loss_D: 1.7102 Loss_G: 4.5086 D(x): 0.7768 D(G(z)): 0.7204 / 0.0224\n",
      "[2/10][217/391] Loss_D: 2.0139 Loss_G: 1.5029 D(x): 0.2187 D(G(z)): 0.0731 / 0.2604\n",
      "[2/10][218/391] Loss_D: 0.8177 Loss_G: 1.7101 D(x): 0.8573 D(G(z)): 0.4290 / 0.2148\n",
      "[2/10][219/391] Loss_D: 0.6516 Loss_G: 2.6727 D(x): 0.8768 D(G(z)): 0.3688 / 0.0857\n",
      "[2/10][220/391] Loss_D: 0.6307 Loss_G: 2.3803 D(x): 0.7416 D(G(z)): 0.2436 / 0.1245\n",
      "[2/10][221/391] Loss_D: 0.7823 Loss_G: 2.2988 D(x): 0.7755 D(G(z)): 0.3491 / 0.1380\n",
      "[2/10][222/391] Loss_D: 0.8457 Loss_G: 1.9854 D(x): 0.6956 D(G(z)): 0.3211 / 0.1817\n",
      "[2/10][223/391] Loss_D: 1.0216 Loss_G: 2.8302 D(x): 0.7814 D(G(z)): 0.4834 / 0.0767\n",
      "[2/10][224/391] Loss_D: 0.9020 Loss_G: 1.8959 D(x): 0.5824 D(G(z)): 0.2311 / 0.1760\n",
      "[2/10][225/391] Loss_D: 0.9515 Loss_G: 3.7566 D(x): 0.8097 D(G(z)): 0.4800 / 0.0332\n",
      "[2/10][226/391] Loss_D: 1.1831 Loss_G: 1.2032 D(x): 0.3895 D(G(z)): 0.1287 / 0.3415\n",
      "[2/10][227/391] Loss_D: 2.0069 Loss_G: 5.0296 D(x): 0.8926 D(G(z)): 0.7920 / 0.0156\n",
      "[2/10][228/391] Loss_D: 2.1586 Loss_G: 1.1730 D(x): 0.1863 D(G(z)): 0.1207 / 0.3502\n",
      "[2/10][229/391] Loss_D: 1.3895 Loss_G: 1.5846 D(x): 0.6902 D(G(z)): 0.5809 / 0.2401\n",
      "[2/10][230/391] Loss_D: 1.1013 Loss_G: 1.8782 D(x): 0.6130 D(G(z)): 0.3885 / 0.1769\n",
      "[2/10][231/391] Loss_D: 0.8910 Loss_G: 1.9101 D(x): 0.6785 D(G(z)): 0.3501 / 0.1766\n",
      "[2/10][232/391] Loss_D: 0.7925 Loss_G: 2.2550 D(x): 0.7138 D(G(z)): 0.3199 / 0.1317\n",
      "[2/10][233/391] Loss_D: 0.9353 Loss_G: 2.4149 D(x): 0.7039 D(G(z)): 0.3911 / 0.1092\n",
      "[2/10][234/391] Loss_D: 1.0069 Loss_G: 1.9715 D(x): 0.6123 D(G(z)): 0.3479 / 0.1743\n",
      "[2/10][235/391] Loss_D: 1.7899 Loss_G: 5.1989 D(x): 0.6025 D(G(z)): 0.6490 / 0.0081\n",
      "[2/10][236/391] Loss_D: 2.5922 Loss_G: 0.4285 D(x): 0.1132 D(G(z)): 0.0397 / 0.6777\n",
      "[2/10][237/391] Loss_D: 1.8420 Loss_G: 2.7331 D(x): 0.8491 D(G(z)): 0.7611 / 0.1136\n",
      "[2/10][238/391] Loss_D: 1.1928 Loss_G: 1.9775 D(x): 0.5193 D(G(z)): 0.2886 / 0.1789\n",
      "[2/10][239/391] Loss_D: 1.4594 Loss_G: 1.3574 D(x): 0.5348 D(G(z)): 0.4499 / 0.3093\n",
      "[2/10][240/391] Loss_D: 1.3000 Loss_G: 1.0974 D(x): 0.5476 D(G(z)): 0.4099 / 0.3550\n",
      "[2/10][241/391] Loss_D: 1.3181 Loss_G: 1.3490 D(x): 0.6351 D(G(z)): 0.5248 / 0.2871\n",
      "[2/10][242/391] Loss_D: 1.1371 Loss_G: 1.6087 D(x): 0.6035 D(G(z)): 0.4143 / 0.2333\n",
      "[2/10][243/391] Loss_D: 0.9857 Loss_G: 1.4645 D(x): 0.5836 D(G(z)): 0.2933 / 0.2607\n",
      "[2/10][244/391] Loss_D: 0.8564 Loss_G: 1.7470 D(x): 0.7388 D(G(z)): 0.3836 / 0.1993\n",
      "[2/10][245/391] Loss_D: 0.7450 Loss_G: 1.9795 D(x): 0.6939 D(G(z)): 0.2773 / 0.1713\n",
      "[2/10][246/391] Loss_D: 0.6663 Loss_G: 1.9654 D(x): 0.7484 D(G(z)): 0.2802 / 0.1758\n",
      "[2/10][247/391] Loss_D: 0.8636 Loss_G: 2.0267 D(x): 0.7000 D(G(z)): 0.3530 / 0.1598\n",
      "[2/10][248/391] Loss_D: 0.6325 Loss_G: 1.5799 D(x): 0.6810 D(G(z)): 0.1636 / 0.2380\n",
      "[2/10][249/391] Loss_D: 1.3215 Loss_G: 3.5963 D(x): 0.9103 D(G(z)): 0.6528 / 0.0467\n",
      "[2/10][250/391] Loss_D: 1.5344 Loss_G: 1.5911 D(x): 0.3254 D(G(z)): 0.0857 / 0.2410\n",
      "[2/10][251/391] Loss_D: 1.3154 Loss_G: 2.1092 D(x): 0.8005 D(G(z)): 0.5975 / 0.1545\n",
      "[2/10][252/391] Loss_D: 1.1624 Loss_G: 1.8246 D(x): 0.5523 D(G(z)): 0.3659 / 0.1900\n",
      "[2/10][253/391] Loss_D: 1.0406 Loss_G: 2.1057 D(x): 0.6248 D(G(z)): 0.3706 / 0.1519\n",
      "[2/10][254/391] Loss_D: 1.6273 Loss_G: 0.4409 D(x): 0.3648 D(G(z)): 0.3371 / 0.6634\n",
      "[2/10][255/391] Loss_D: 2.1207 Loss_G: 4.7615 D(x): 0.9368 D(G(z)): 0.8396 / 0.0178\n",
      "[2/10][256/391] Loss_D: 2.7817 Loss_G: 2.0798 D(x): 0.1049 D(G(z)): 0.0398 / 0.1658\n",
      "[2/10][257/391] Loss_D: 1.3339 Loss_G: 0.5454 D(x): 0.4502 D(G(z)): 0.3276 / 0.6128\n",
      "[2/10][258/391] Loss_D: 1.6533 Loss_G: 2.2383 D(x): 0.8389 D(G(z)): 0.7228 / 0.1367\n",
      "[2/10][259/391] Loss_D: 1.3119 Loss_G: 1.5724 D(x): 0.4318 D(G(z)): 0.2584 / 0.2375\n",
      "[2/10][260/391] Loss_D: 1.2180 Loss_G: 0.9763 D(x): 0.5419 D(G(z)): 0.3962 / 0.4047\n",
      "[2/10][261/391] Loss_D: 1.0532 Loss_G: 1.3362 D(x): 0.6924 D(G(z)): 0.4548 / 0.3072\n",
      "[2/10][262/391] Loss_D: 1.0059 Loss_G: 1.5620 D(x): 0.6856 D(G(z)): 0.4227 / 0.2385\n",
      "[2/10][263/391] Loss_D: 1.0267 Loss_G: 1.4743 D(x): 0.5832 D(G(z)): 0.3511 / 0.2584\n",
      "[2/10][264/391] Loss_D: 1.0746 Loss_G: 1.3590 D(x): 0.5638 D(G(z)): 0.3459 / 0.2763\n",
      "[2/10][265/391] Loss_D: 0.8165 Loss_G: 1.6308 D(x): 0.7078 D(G(z)): 0.3458 / 0.2301\n",
      "[2/10][266/391] Loss_D: 1.2394 Loss_G: 2.3216 D(x): 0.7550 D(G(z)): 0.5795 / 0.1306\n",
      "[2/10][267/391] Loss_D: 1.3487 Loss_G: 1.6829 D(x): 0.3884 D(G(z)): 0.2052 / 0.2285\n",
      "[2/10][268/391] Loss_D: 1.1421 Loss_G: 1.1331 D(x): 0.6191 D(G(z)): 0.4324 / 0.3502\n",
      "[2/10][269/391] Loss_D: 1.3602 Loss_G: 2.1298 D(x): 0.7270 D(G(z)): 0.5890 / 0.1688\n",
      "[2/10][270/391] Loss_D: 1.0383 Loss_G: 1.6704 D(x): 0.5354 D(G(z)): 0.2170 / 0.2122\n",
      "[2/10][271/391] Loss_D: 0.7165 Loss_G: 1.9156 D(x): 0.8095 D(G(z)): 0.3705 / 0.1705\n",
      "[2/10][272/391] Loss_D: 0.8636 Loss_G: 1.7509 D(x): 0.6956 D(G(z)): 0.3481 / 0.1987\n",
      "[2/10][273/391] Loss_D: 0.7361 Loss_G: 2.2229 D(x): 0.7436 D(G(z)): 0.3243 / 0.1410\n",
      "[2/10][274/391] Loss_D: 0.6443 Loss_G: 2.4163 D(x): 0.8051 D(G(z)): 0.3208 / 0.1151\n",
      "[2/10][275/391] Loss_D: 0.6295 Loss_G: 2.4180 D(x): 0.7001 D(G(z)): 0.2037 / 0.1174\n",
      "[2/10][276/391] Loss_D: 0.7845 Loss_G: 1.7874 D(x): 0.6922 D(G(z)): 0.3003 / 0.1979\n",
      "[2/10][277/391] Loss_D: 1.2238 Loss_G: 5.3047 D(x): 0.8630 D(G(z)): 0.6059 / 0.0082\n",
      "[2/10][278/391] Loss_D: 2.6581 Loss_G: 1.2575 D(x): 0.1171 D(G(z)): 0.0167 / 0.3389\n",
      "[2/10][279/391] Loss_D: 0.6701 Loss_G: 1.7828 D(x): 0.8245 D(G(z)): 0.3246 / 0.2064\n",
      "[2/10][280/391] Loss_D: 0.9013 Loss_G: 2.5200 D(x): 0.7781 D(G(z)): 0.4206 / 0.1097\n",
      "[2/10][281/391] Loss_D: 2.0648 Loss_G: 2.3098 D(x): 0.3998 D(G(z)): 0.5825 / 0.1354\n",
      "[2/10][282/391] Loss_D: 1.5359 Loss_G: 0.7208 D(x): 0.3530 D(G(z)): 0.2410 / 0.5380\n",
      "[2/10][283/391] Loss_D: 1.4617 Loss_G: 1.5978 D(x): 0.8609 D(G(z)): 0.6625 / 0.2463\n",
      "[2/10][284/391] Loss_D: 1.1220 Loss_G: 1.7750 D(x): 0.5735 D(G(z)): 0.3415 / 0.2040\n",
      "[2/10][285/391] Loss_D: 1.2446 Loss_G: 1.1675 D(x): 0.5283 D(G(z)): 0.3762 / 0.3530\n",
      "[2/10][286/391] Loss_D: 1.1688 Loss_G: 1.7458 D(x): 0.6852 D(G(z)): 0.4856 / 0.2044\n",
      "[2/10][287/391] Loss_D: 1.0261 Loss_G: 1.7081 D(x): 0.5847 D(G(z)): 0.3394 / 0.2065\n",
      "[2/10][288/391] Loss_D: 0.9797 Loss_G: 1.6677 D(x): 0.5496 D(G(z)): 0.2575 / 0.2196\n",
      "[2/10][289/391] Loss_D: 1.1262 Loss_G: 2.7416 D(x): 0.7978 D(G(z)): 0.5296 / 0.0819\n",
      "[2/10][290/391] Loss_D: 0.9652 Loss_G: 2.0673 D(x): 0.4628 D(G(z)): 0.0834 / 0.1616\n",
      "[2/10][291/391] Loss_D: 0.8772 Loss_G: 1.0653 D(x): 0.6232 D(G(z)): 0.2611 / 0.3997\n",
      "[2/10][292/391] Loss_D: 1.0311 Loss_G: 2.3240 D(x): 0.8666 D(G(z)): 0.5273 / 0.1338\n",
      "[2/10][293/391] Loss_D: 0.8258 Loss_G: 2.7685 D(x): 0.7941 D(G(z)): 0.3843 / 0.1037\n",
      "[2/10][294/391] Loss_D: 0.8764 Loss_G: 3.5814 D(x): 0.8499 D(G(z)): 0.4486 / 0.0448\n",
      "[2/10][295/391] Loss_D: 0.8732 Loss_G: 3.4909 D(x): 0.4928 D(G(z)): 0.0445 / 0.0415\n",
      "[2/10][296/391] Loss_D: 1.1936 Loss_G: 0.2096 D(x): 0.3986 D(G(z)): 0.1027 / 0.8203\n",
      "[2/10][297/391] Loss_D: 3.2005 Loss_G: 3.2417 D(x): 0.9331 D(G(z)): 0.9283 / 0.0565\n",
      "[2/10][298/391] Loss_D: 2.5680 Loss_G: 1.2724 D(x): 0.1233 D(G(z)): 0.1222 / 0.3145\n",
      "[2/10][299/391] Loss_D: 1.3322 Loss_G: 0.6620 D(x): 0.4661 D(G(z)): 0.3703 / 0.5296\n",
      "[2/10][300/391] Loss_D: 1.2518 Loss_G: 0.9092 D(x): 0.6981 D(G(z)): 0.5644 / 0.4529\n",
      "[2/10][301/391] Loss_D: 1.1645 Loss_G: 1.0720 D(x): 0.6747 D(G(z)): 0.4827 / 0.3648\n",
      "[2/10][302/391] Loss_D: 1.0154 Loss_G: 1.1884 D(x): 0.6616 D(G(z)): 0.4303 / 0.3301\n",
      "[2/10][303/391] Loss_D: 1.0653 Loss_G: 1.2323 D(x): 0.6132 D(G(z)): 0.3928 / 0.3218\n",
      "[2/10][304/391] Loss_D: 1.0023 Loss_G: 1.1355 D(x): 0.6049 D(G(z)): 0.3566 / 0.3451\n",
      "[2/10][305/391] Loss_D: 1.1163 Loss_G: 1.2298 D(x): 0.6994 D(G(z)): 0.5000 / 0.3137\n",
      "[2/10][306/391] Loss_D: 1.2112 Loss_G: 1.1976 D(x): 0.5550 D(G(z)): 0.4137 / 0.3259\n",
      "[2/10][307/391] Loss_D: 1.2528 Loss_G: 1.0782 D(x): 0.5435 D(G(z)): 0.4236 / 0.3610\n",
      "[2/10][308/391] Loss_D: 1.3107 Loss_G: 1.2862 D(x): 0.6024 D(G(z)): 0.5064 / 0.3030\n",
      "[2/10][309/391] Loss_D: 1.2772 Loss_G: 1.1572 D(x): 0.5031 D(G(z)): 0.4000 / 0.3423\n",
      "[2/10][310/391] Loss_D: 1.2689 Loss_G: 1.4857 D(x): 0.5940 D(G(z)): 0.4684 / 0.2485\n",
      "[2/10][311/391] Loss_D: 1.1603 Loss_G: 1.0342 D(x): 0.4855 D(G(z)): 0.3137 / 0.3755\n",
      "[2/10][312/391] Loss_D: 1.0463 Loss_G: 1.8761 D(x): 0.6999 D(G(z)): 0.4555 / 0.1839\n",
      "[2/10][313/391] Loss_D: 0.9959 Loss_G: 1.5510 D(x): 0.5501 D(G(z)): 0.2737 / 0.2430\n",
      "[2/10][314/391] Loss_D: 0.7908 Loss_G: 2.0124 D(x): 0.7058 D(G(z)): 0.3132 / 0.1648\n",
      "[2/10][315/391] Loss_D: 0.7411 Loss_G: 2.0210 D(x): 0.7244 D(G(z)): 0.3065 / 0.1719\n",
      "[2/10][316/391] Loss_D: 0.8423 Loss_G: 1.9574 D(x): 0.6792 D(G(z)): 0.3110 / 0.1715\n",
      "[2/10][317/391] Loss_D: 0.9563 Loss_G: 1.9258 D(x): 0.7171 D(G(z)): 0.4154 / 0.1697\n",
      "[2/10][318/391] Loss_D: 1.2315 Loss_G: 1.4994 D(x): 0.5938 D(G(z)): 0.4446 / 0.2455\n",
      "[2/10][319/391] Loss_D: 1.2101 Loss_G: 1.2168 D(x): 0.5489 D(G(z)): 0.4100 / 0.3422\n",
      "[2/10][320/391] Loss_D: 1.1571 Loss_G: 2.3757 D(x): 0.7305 D(G(z)): 0.5197 / 0.1122\n",
      "[2/10][321/391] Loss_D: 1.1775 Loss_G: 1.0321 D(x): 0.4315 D(G(z)): 0.2166 / 0.3893\n",
      "[2/10][322/391] Loss_D: 1.3443 Loss_G: 3.3411 D(x): 0.8024 D(G(z)): 0.6323 / 0.0568\n",
      "[2/10][323/391] Loss_D: 1.5669 Loss_G: 1.0570 D(x): 0.3178 D(G(z)): 0.1493 / 0.3838\n",
      "[2/10][324/391] Loss_D: 1.5236 Loss_G: 3.0603 D(x): 0.7306 D(G(z)): 0.6556 / 0.0618\n",
      "[2/10][325/391] Loss_D: 1.6482 Loss_G: 1.1557 D(x): 0.3033 D(G(z)): 0.2171 / 0.3658\n",
      "[2/10][326/391] Loss_D: 1.6266 Loss_G: 2.2067 D(x): 0.6123 D(G(z)): 0.5925 / 0.1325\n",
      "[2/10][327/391] Loss_D: 1.2105 Loss_G: 1.4173 D(x): 0.4543 D(G(z)): 0.2628 / 0.2670\n",
      "[2/10][328/391] Loss_D: 0.9769 Loss_G: 1.4516 D(x): 0.6543 D(G(z)): 0.3901 / 0.2560\n",
      "[2/10][329/391] Loss_D: 0.9822 Loss_G: 2.2469 D(x): 0.7063 D(G(z)): 0.4373 / 0.1226\n",
      "[2/10][330/391] Loss_D: 1.0691 Loss_G: 1.3493 D(x): 0.5171 D(G(z)): 0.2789 / 0.2867\n",
      "[2/10][331/391] Loss_D: 1.1018 Loss_G: 2.2100 D(x): 0.6853 D(G(z)): 0.4781 / 0.1295\n",
      "[2/10][332/391] Loss_D: 1.1972 Loss_G: 1.2439 D(x): 0.5129 D(G(z)): 0.3395 / 0.3256\n",
      "[2/10][333/391] Loss_D: 1.1223 Loss_G: 2.3466 D(x): 0.7462 D(G(z)): 0.5245 / 0.1215\n",
      "[2/10][334/391] Loss_D: 0.9486 Loss_G: 1.6147 D(x): 0.6076 D(G(z)): 0.2998 / 0.2275\n",
      "[2/10][335/391] Loss_D: 1.0309 Loss_G: 1.4557 D(x): 0.6024 D(G(z)): 0.3618 / 0.2613\n",
      "[2/10][336/391] Loss_D: 0.9338 Loss_G: 2.5338 D(x): 0.7911 D(G(z)): 0.4688 / 0.1185\n",
      "[2/10][337/391] Loss_D: 0.9299 Loss_G: 1.4920 D(x): 0.5603 D(G(z)): 0.1747 / 0.2934\n",
      "[2/10][338/391] Loss_D: 0.7415 Loss_G: 2.2648 D(x): 0.8292 D(G(z)): 0.3762 / 0.1328\n",
      "[2/10][339/391] Loss_D: 0.7635 Loss_G: 1.3122 D(x): 0.6381 D(G(z)): 0.2304 / 0.3095\n",
      "[2/10][340/391] Loss_D: 1.4209 Loss_G: 3.5091 D(x): 0.9303 D(G(z)): 0.7057 / 0.0419\n",
      "[2/10][341/391] Loss_D: 1.6352 Loss_G: 1.3185 D(x): 0.2767 D(G(z)): 0.0612 / 0.3064\n",
      "[2/10][342/391] Loss_D: 1.0844 Loss_G: 1.9835 D(x): 0.8276 D(G(z)): 0.5071 / 0.1655\n",
      "[2/10][343/391] Loss_D: 1.0844 Loss_G: 2.0134 D(x): 0.6145 D(G(z)): 0.3905 / 0.1582\n",
      "[2/10][344/391] Loss_D: 1.0523 Loss_G: 1.9066 D(x): 0.5350 D(G(z)): 0.2913 / 0.1698\n",
      "[2/10][345/391] Loss_D: 0.9195 Loss_G: 2.4177 D(x): 0.7756 D(G(z)): 0.4553 / 0.1133\n",
      "[2/10][346/391] Loss_D: 0.9045 Loss_G: 1.9281 D(x): 0.6006 D(G(z)): 0.2646 / 0.1767\n",
      "[2/10][347/391] Loss_D: 0.9545 Loss_G: 3.2928 D(x): 0.8494 D(G(z)): 0.5009 / 0.0471\n",
      "[2/10][348/391] Loss_D: 1.9668 Loss_G: 0.9815 D(x): 0.1848 D(G(z)): 0.0967 / 0.4015\n",
      "[2/10][349/391] Loss_D: 1.1369 Loss_G: 2.1213 D(x): 0.8968 D(G(z)): 0.6105 / 0.1486\n",
      "[2/10][350/391] Loss_D: 1.2809 Loss_G: 1.6077 D(x): 0.4797 D(G(z)): 0.3158 / 0.2273\n",
      "[2/10][351/391] Loss_D: 0.5811 Loss_G: 1.9115 D(x): 0.8099 D(G(z)): 0.2789 / 0.1736\n",
      "[2/10][352/391] Loss_D: 0.6344 Loss_G: 2.2420 D(x): 0.7814 D(G(z)): 0.2812 / 0.1276\n",
      "[2/10][353/391] Loss_D: 0.5941 Loss_G: 1.9494 D(x): 0.7572 D(G(z)): 0.2319 / 0.1664\n",
      "[2/10][354/391] Loss_D: 0.6168 Loss_G: 2.3143 D(x): 0.7957 D(G(z)): 0.2842 / 0.1214\n",
      "[2/10][355/391] Loss_D: 0.9182 Loss_G: 3.6475 D(x): 0.8121 D(G(z)): 0.4508 / 0.0349\n",
      "[2/10][356/391] Loss_D: 1.0182 Loss_G: 1.0436 D(x): 0.4653 D(G(z)): 0.0851 / 0.3929\n",
      "[2/10][357/391] Loss_D: 0.8740 Loss_G: 3.0767 D(x): 0.8099 D(G(z)): 0.4373 / 0.0597\n",
      "[2/10][358/391] Loss_D: 0.8777 Loss_G: 5.9765 D(x): 0.7964 D(G(z)): 0.4181 / 0.0036\n",
      "[2/10][359/391] Loss_D: 2.5141 Loss_G: 0.8610 D(x): 0.1076 D(G(z)): 0.0258 / 0.4796\n",
      "[2/10][360/391] Loss_D: 1.7364 Loss_G: 4.5842 D(x): 0.8460 D(G(z)): 0.7018 / 0.0224\n",
      "[2/10][361/391] Loss_D: 1.6490 Loss_G: 1.9706 D(x): 0.2810 D(G(z)): 0.0632 / 0.2011\n",
      "[2/10][362/391] Loss_D: 1.0603 Loss_G: 1.6206 D(x): 0.6526 D(G(z)): 0.3893 / 0.2709\n",
      "[2/10][363/391] Loss_D: 1.1758 Loss_G: 2.8291 D(x): 0.8858 D(G(z)): 0.5934 / 0.0931\n",
      "[2/10][364/391] Loss_D: 0.8993 Loss_G: 2.3569 D(x): 0.6657 D(G(z)): 0.2980 / 0.1239\n",
      "[2/10][365/391] Loss_D: 0.8727 Loss_G: 2.1147 D(x): 0.6578 D(G(z)): 0.3075 / 0.1398\n",
      "[2/10][366/391] Loss_D: 0.6596 Loss_G: 2.4684 D(x): 0.7874 D(G(z)): 0.3118 / 0.1086\n",
      "[2/10][367/391] Loss_D: 0.5718 Loss_G: 3.0342 D(x): 0.8454 D(G(z)): 0.3052 / 0.0628\n",
      "[2/10][368/391] Loss_D: 0.6141 Loss_G: 2.4018 D(x): 0.6956 D(G(z)): 0.1846 / 0.1112\n",
      "[2/10][369/391] Loss_D: 0.6606 Loss_G: 3.1659 D(x): 0.7925 D(G(z)): 0.3197 / 0.0588\n",
      "[2/10][370/391] Loss_D: 0.3488 Loss_G: 3.8819 D(x): 0.8665 D(G(z)): 0.1627 / 0.0318\n",
      "[2/10][371/391] Loss_D: 0.4276 Loss_G: 3.3890 D(x): 0.8166 D(G(z)): 0.1712 / 0.0488\n",
      "[2/10][372/391] Loss_D: 0.2979 Loss_G: 5.0694 D(x): 0.9299 D(G(z)): 0.1871 / 0.0084\n",
      "[2/10][373/391] Loss_D: 0.8638 Loss_G: 2.3516 D(x): 0.5137 D(G(z)): 0.0783 / 0.1289\n",
      "[2/10][374/391] Loss_D: 1.2988 Loss_G: 0.6935 D(x): 0.4665 D(G(z)): 0.3293 / 0.5375\n",
      "[2/10][375/391] Loss_D: 2.1293 Loss_G: 4.3066 D(x): 0.9407 D(G(z)): 0.8197 / 0.0180\n",
      "[2/10][376/391] Loss_D: 1.9436 Loss_G: 1.4753 D(x): 0.2061 D(G(z)): 0.0542 / 0.2726\n",
      "[2/10][377/391] Loss_D: 0.7511 Loss_G: 1.7338 D(x): 0.9097 D(G(z)): 0.4226 / 0.2237\n",
      "[2/10][378/391] Loss_D: 1.3117 Loss_G: 3.2892 D(x): 0.8609 D(G(z)): 0.6355 / 0.0599\n",
      "[2/10][379/391] Loss_D: 1.3694 Loss_G: 1.8874 D(x): 0.3425 D(G(z)): 0.1076 / 0.1999\n",
      "[2/10][380/391] Loss_D: 0.9936 Loss_G: 0.9475 D(x): 0.6002 D(G(z)): 0.2963 / 0.4227\n",
      "[2/10][381/391] Loss_D: 1.5458 Loss_G: 1.9020 D(x): 0.8166 D(G(z)): 0.6719 / 0.1816\n",
      "[2/10][382/391] Loss_D: 1.2242 Loss_G: 1.7861 D(x): 0.5212 D(G(z)): 0.3754 / 0.2055\n",
      "[2/10][383/391] Loss_D: 1.3828 Loss_G: 1.4173 D(x): 0.4608 D(G(z)): 0.3843 / 0.2729\n",
      "[2/10][384/391] Loss_D: 1.2085 Loss_G: 1.7063 D(x): 0.6256 D(G(z)): 0.4792 / 0.2075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][385/391] Loss_D: 1.1066 Loss_G: 1.5898 D(x): 0.5462 D(G(z)): 0.3458 / 0.2323\n",
      "[2/10][386/391] Loss_D: 1.0942 Loss_G: 1.6525 D(x): 0.6188 D(G(z)): 0.4089 / 0.2363\n",
      "[2/10][387/391] Loss_D: 1.4152 Loss_G: 2.0112 D(x): 0.6086 D(G(z)): 0.5061 / 0.2224\n",
      "[2/10][388/391] Loss_D: 1.6508 Loss_G: 1.5344 D(x): 0.4163 D(G(z)): 0.3563 / 0.3125\n",
      "[2/10][389/391] Loss_D: 1.3832 Loss_G: 1.1224 D(x): 0.5825 D(G(z)): 0.4555 / 0.3598\n",
      "[2/10][390/391] Loss_D: 1.1840 Loss_G: 1.8182 D(x): 0.6489 D(G(z)): 0.4756 / 0.1916\n",
      "[3/10][0/391] Loss_D: 1.1565 Loss_G: 1.2616 D(x): 0.5396 D(G(z)): 0.3585 / 0.3256\n",
      "[3/10][1/391] Loss_D: 1.3494 Loss_G: 2.6705 D(x): 0.6448 D(G(z)): 0.5292 / 0.1098\n",
      "[3/10][2/391] Loss_D: 1.2479 Loss_G: 1.2419 D(x): 0.4528 D(G(z)): 0.2463 / 0.3705\n",
      "[3/10][3/391] Loss_D: 1.1322 Loss_G: 2.1204 D(x): 0.7359 D(G(z)): 0.4724 / 0.1398\n",
      "[3/10][4/391] Loss_D: 1.0883 Loss_G: 0.8180 D(x): 0.5011 D(G(z)): 0.2489 / 0.4660\n",
      "[3/10][5/391] Loss_D: 1.8200 Loss_G: 3.2792 D(x): 0.8092 D(G(z)): 0.7639 / 0.0563\n",
      "[3/10][6/391] Loss_D: 2.6518 Loss_G: 0.9220 D(x): 0.1049 D(G(z)): 0.0808 / 0.4472\n",
      "[3/10][7/391] Loss_D: 1.5158 Loss_G: 1.0739 D(x): 0.5574 D(G(z)): 0.4690 / 0.4656\n",
      "[3/10][8/391] Loss_D: 1.4649 Loss_G: 1.0187 D(x): 0.6365 D(G(z)): 0.5150 / 0.3945\n",
      "[3/10][9/391] Loss_D: 1.2013 Loss_G: 1.4196 D(x): 0.6339 D(G(z)): 0.4848 / 0.2884\n",
      "[3/10][10/391] Loss_D: 1.3040 Loss_G: 1.1376 D(x): 0.5048 D(G(z)): 0.3718 / 0.3440\n",
      "[3/10][11/391] Loss_D: 1.1573 Loss_G: 1.1562 D(x): 0.6074 D(G(z)): 0.4418 / 0.3524\n",
      "[3/10][12/391] Loss_D: 1.3693 Loss_G: 0.8790 D(x): 0.5157 D(G(z)): 0.4392 / 0.4473\n",
      "[3/10][13/391] Loss_D: 1.3253 Loss_G: 1.3859 D(x): 0.6101 D(G(z)): 0.5148 / 0.2889\n",
      "[3/10][14/391] Loss_D: 1.1173 Loss_G: 1.3767 D(x): 0.5561 D(G(z)): 0.3600 / 0.2901\n",
      "[3/10][15/391] Loss_D: 1.0574 Loss_G: 1.5460 D(x): 0.6185 D(G(z)): 0.3727 / 0.2630\n",
      "[3/10][16/391] Loss_D: 1.1420 Loss_G: 1.0679 D(x): 0.5795 D(G(z)): 0.3772 / 0.3713\n",
      "[3/10][17/391] Loss_D: 1.2617 Loss_G: 2.0944 D(x): 0.7095 D(G(z)): 0.5478 / 0.1882\n",
      "[3/10][18/391] Loss_D: 1.6120 Loss_G: 0.9120 D(x): 0.3919 D(G(z)): 0.3248 / 0.4302\n",
      "[3/10][19/391] Loss_D: 1.4173 Loss_G: 1.5572 D(x): 0.6758 D(G(z)): 0.5767 / 0.2653\n",
      "[3/10][20/391] Loss_D: 1.3801 Loss_G: 0.9119 D(x): 0.4885 D(G(z)): 0.4073 / 0.4216\n",
      "[3/10][21/391] Loss_D: 1.4384 Loss_G: 1.1650 D(x): 0.6002 D(G(z)): 0.5515 / 0.3554\n",
      "[3/10][22/391] Loss_D: 1.4162 Loss_G: 1.0287 D(x): 0.5102 D(G(z)): 0.4517 / 0.3772\n",
      "[3/10][23/391] Loss_D: 1.3755 Loss_G: 1.2338 D(x): 0.5930 D(G(z)): 0.5192 / 0.3109\n",
      "[3/10][24/391] Loss_D: 1.4787 Loss_G: 0.7341 D(x): 0.3937 D(G(z)): 0.3790 / 0.4954\n",
      "[3/10][25/391] Loss_D: 1.4862 Loss_G: 1.2962 D(x): 0.6516 D(G(z)): 0.6182 / 0.2941\n",
      "[3/10][26/391] Loss_D: 1.6781 Loss_G: 0.9163 D(x): 0.3848 D(G(z)): 0.4670 / 0.4348\n",
      "[3/10][27/391] Loss_D: 1.5920 Loss_G: 0.7987 D(x): 0.4375 D(G(z)): 0.4601 / 0.4674\n",
      "[3/10][28/391] Loss_D: 1.6766 Loss_G: 1.2304 D(x): 0.5014 D(G(z)): 0.5983 / 0.3098\n",
      "[3/10][29/391] Loss_D: 1.4732 Loss_G: 0.9988 D(x): 0.3874 D(G(z)): 0.3711 / 0.3908\n",
      "[3/10][30/391] Loss_D: 1.5000 Loss_G: 1.2076 D(x): 0.5133 D(G(z)): 0.5308 / 0.3258\n",
      "[3/10][31/391] Loss_D: 1.2657 Loss_G: 1.1150 D(x): 0.5009 D(G(z)): 0.4020 / 0.3517\n",
      "[3/10][32/391] Loss_D: 1.2885 Loss_G: 1.2121 D(x): 0.5630 D(G(z)): 0.4660 / 0.3418\n",
      "[3/10][33/391] Loss_D: 1.1566 Loss_G: 1.3998 D(x): 0.6093 D(G(z)): 0.4382 / 0.2848\n",
      "[3/10][34/391] Loss_D: 1.2201 Loss_G: 1.1698 D(x): 0.5620 D(G(z)): 0.4081 / 0.3342\n",
      "[3/10][35/391] Loss_D: 1.3029 Loss_G: 0.7950 D(x): 0.4950 D(G(z)): 0.4172 / 0.4876\n",
      "[3/10][36/391] Loss_D: 1.2485 Loss_G: 1.0778 D(x): 0.7050 D(G(z)): 0.5596 / 0.3536\n",
      "[3/10][37/391] Loss_D: 1.1323 Loss_G: 1.2067 D(x): 0.5639 D(G(z)): 0.4104 / 0.3387\n",
      "[3/10][38/391] Loss_D: 0.9473 Loss_G: 1.1229 D(x): 0.6520 D(G(z)): 0.3811 / 0.3410\n",
      "[3/10][39/391] Loss_D: 0.8522 Loss_G: 1.8691 D(x): 0.7791 D(G(z)): 0.4212 / 0.2063\n",
      "[3/10][40/391] Loss_D: 0.9899 Loss_G: 1.2092 D(x): 0.6125 D(G(z)): 0.3174 / 0.3406\n",
      "[3/10][41/391] Loss_D: 0.9844 Loss_G: 2.2266 D(x): 0.7321 D(G(z)): 0.4197 / 0.1649\n",
      "[3/10][42/391] Loss_D: 1.2410 Loss_G: 0.8970 D(x): 0.5054 D(G(z)): 0.2837 / 0.4517\n",
      "[3/10][43/391] Loss_D: 1.4388 Loss_G: 2.2528 D(x): 0.7320 D(G(z)): 0.6059 / 0.1505\n",
      "[3/10][44/391] Loss_D: 1.5600 Loss_G: 0.9250 D(x): 0.3550 D(G(z)): 0.2440 / 0.4407\n",
      "[3/10][45/391] Loss_D: 1.3501 Loss_G: 1.8861 D(x): 0.6787 D(G(z)): 0.5501 / 0.1692\n",
      "[3/10][46/391] Loss_D: 1.0846 Loss_G: 1.3476 D(x): 0.4835 D(G(z)): 0.2556 / 0.2954\n",
      "[3/10][47/391] Loss_D: 1.1085 Loss_G: 1.6979 D(x): 0.6174 D(G(z)): 0.4130 / 0.2140\n",
      "[3/10][48/391] Loss_D: 1.0886 Loss_G: 2.4583 D(x): 0.6459 D(G(z)): 0.4424 / 0.1009\n",
      "[3/10][49/391] Loss_D: 1.1713 Loss_G: 1.0499 D(x): 0.4426 D(G(z)): 0.2225 / 0.4017\n",
      "[3/10][50/391] Loss_D: 1.6182 Loss_G: 2.2498 D(x): 0.6698 D(G(z)): 0.6302 / 0.1234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d2cb8140af75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#detach the generated image from the graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0merrD_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0merrD_fake\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mD_G_z1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# mean of the descriminator output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0merrD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrD_real\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0merrD_fake\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "#         print(f'type data[0]: {type(data[0])}')\n",
    "#         print(f'shape data[0]: {data[0].shape}')\n",
    "#         print(f'len data: {len(data)}')\n",
    "#         print(f'content data[1]: {data[1]}')\n",
    "        real_input = data[0].to(device)\n",
    "        batch_size = real_input.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "        \n",
    "        output = netD(real_input)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(size=(batch_size, LATENT_DIM, 1, 1), device=device)\n",
    "        # generate fake input\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "\n",
    "        output = netD(fake.detach()) #detach the generated image from the graph\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item() # mean of the descriminator output\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item() # ???\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, EPOCHS, i, len(train_loader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_input,\n",
    "                    f'{SAMPLES_DIR}/real_samples.png', normalize=False)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),\n",
    "                    f'{SAMPLES_DIR}/fake_samples_epoch_%03d.png' % (epoch),\n",
    "                    normalize=False)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (SAVE_DIR, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (SAVE_DIR, epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
